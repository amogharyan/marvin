/**
 * @module Built-In
 * @version 5.13.0
 * For Snapchat Version: 13.55
*/
interface ComponentNameMap {
    "Animation": Animation;
    "AnimationMixer": AnimationMixer;
    "AnimationPlayer": AnimationPlayer;
    "AudioChainComponent": AudioEffectComponent;
    "AudioComponent": AudioComponent;
    "AudioEffectComponent": AudioEffectComponent;
    "AudioListenerComponent": AudioListenerComponent;
    "AudioPlayer": AudioComponent;
    "BaseMeshVisual": BaseMeshVisual;
    "BlendShapes": BlendShapes;
    "BlurNoiseEstimation": BlurNoiseEstimation;
    "Camera": Camera;
    "Canvas": Canvas;
    "ClearDepth": ClearDepth;
    "ClearScreen": ClearDepth;
    "ClothVisual": ClothVisual;
    "ColliderComponent": ColliderComponent;
    "ColocatedTrackingComponent": ColocatedTrackingComponent;
    "Component": Component;
    "Component.Animation": Animation;
    "Component.AnimationMixer": AnimationMixer;
    "Component.AnimationPlayer": AnimationPlayer;
    "Component.AudioChainComponent": AudioEffectComponent;
    "Component.AudioComponent": AudioComponent;
    "Component.AudioEffectComponent": AudioEffectComponent;
    "Component.AudioListenerComponent": AudioListenerComponent;
    "Component.AudioPlayer": AudioComponent;
    "Component.BaseMeshVisual": BaseMeshVisual;
    "Component.BlendShapes": BlendShapes;
    "Component.BlurNoiseEstimation": BlurNoiseEstimation;
    "Component.Camera": Camera;
    "Component.Canvas": Canvas;
    "Component.ClearDepth": ClearDepth;
    "Component.ClearScreen": ClearDepth;
    "Component.ClothVisual": ClothVisual;
    "Component.ColliderComponent": ColliderComponent;
    "Component.ColocatedTrackingComponent": ColocatedTrackingComponent;
    "Component.CustomLocationGroupComponent": CustomLocationGroupComponent;
    "Component.DepthSetter": DepthSetter;
    "Component.DeviceLocationTrackingComponent": DeviceLocationTrackingComponent;
    "Component.DeviceTracking": DeviceTracking;
    "Component.EyeColorVisual": EyeColorVisual;
    "Component.FaceInsetVisual": FaceInsetVisual;
    "Component.FaceMaskVisual": FaceMaskVisual;
    "Component.FaceStretchVisual": FaceStretchVisual;
    "Component.FaceSubVisual": FaceMaskVisual;
    "Component.GaussianSplattingVisual": GaussianSplattingVisual;
    "Component.Gyroscope": Gyroscope;
    "Component.HairSimulationColliderComponent": ColliderComponent;
    "Component.HairVisual": HairVisual;
    "Component.Head": Head;
    "Component.Hints": HintsComponent;
    "Component.HintsComponent": HintsComponent;
    "Component.Image": Image;
    "Component.InteractionComponent": InteractionComponent;
    "Component.Label": Label;
    "Component.LightSource": LightSource;
    "Component.LiquifyVisual": LiquifyVisual;
    "Component.LocatedAtComponent": LocatedAtComponent;
    "Component.LookAtComponent": LookAtComponent;
    "Component.MLComponent": MLComponent;
    "Component.ManipulateComponent": ManipulateComponent;
    "Component.MarkerTrackingComponent": MarkerTrackingComponent;
    "Component.MaskingComponent": MaskingComponent;
    "Component.MaterialMeshVisual": MaterialMeshVisual;
    "Component.MeshVisual": RenderMeshVisual;
    "Component.ObjectTracker2D": ObjectTracking;
    "Component.ObjectTracking": ObjectTracking;
    "Component.ObjectTracking3D": ObjectTracking3D;
    "Component.PinToMeshComponent": PinToMeshComponent;
    "Component.PostEffectVisual": PostEffectVisual;
    "Component.RectangleSetter": RectangleSetter;
    "Component.RenderMeshVisual": RenderMeshVisual;
    "Component.RetouchVisual": RetouchVisual;
    "Component.ScreenRegionComponent": ScreenRegionComponent;
    "Component.ScreenTransform": ScreenTransform;
    "Component.Script": ScriptComponent;
    "Component.ScriptComponent": ScriptComponent;
    "Component.Skin": Skin;
    "Component.SplineComponent": SplineComponent;
    "Component.SpriteAligner": SpriteAligner;
    "Component.SpriteVisual": SpriteVisual;
    "Component.SpriteVisualV2": Image;
    "Component.Text": Text;
    "Component.Text3D": Text3D;
    "Component.TextVisual": Label;
    "Component.Touch": InteractionComponent;
    "Component.TouchComponent": InteractionComponent;
    "Component.TrackedPointComponent": TrackedPointComponent;
    "Component.VFXComponent": VFXComponent;
    "Component.VertexCache": VertexCache;
    "Component.Visual": Visual;
    "Component.WorldTracking": DeviceTracking;
    "CustomLocationGroupComponent": CustomLocationGroupComponent;
    "DepthSetter": DepthSetter;
    "DeviceLocationTrackingComponent": DeviceLocationTrackingComponent;
    "DeviceTracking": DeviceTracking;
    "EyeColorVisual": EyeColorVisual;
    "FaceInsetVisual": FaceInsetVisual;
    "FaceMaskVisual": FaceMaskVisual;
    "FaceStretchVisual": FaceStretchVisual;
    "FaceSubVisual": FaceMaskVisual;
    "GaussianSplattingVisual": GaussianSplattingVisual;
    "Gyroscope": Gyroscope;
    "HairSimulationColliderComponent": ColliderComponent;
    "HairVisual": HairVisual;
    "Head": Head;
    "Hints": HintsComponent;
    "HintsComponent": HintsComponent;
    "Image": Image;
    "InteractionComponent": InteractionComponent;
    "Label": Label;
    "LightSource": LightSource;
    "LiquifyVisual": LiquifyVisual;
    "LocatedAtComponent": LocatedAtComponent;
    "LookAtComponent": LookAtComponent;
    "MLComponent": MLComponent;
    "ManipulateComponent": ManipulateComponent;
    "MarkerTrackingComponent": MarkerTrackingComponent;
    "MaskingComponent": MaskingComponent;
    "MaterialMeshVisual": MaterialMeshVisual;
    "MeshVisual": RenderMeshVisual;
    "ObjectTracker2D": ObjectTracking;
    "ObjectTracking": ObjectTracking;
    "ObjectTracking3D": ObjectTracking3D;
    "Physics.BodyComponent": BodyComponent;
    "Physics.ColliderComponent": ColliderComponent;
    "Physics.ConstraintComponent": ConstraintComponent;
    "Physics.WorldComponent": WorldComponent;
    "PinToMeshComponent": PinToMeshComponent;
    "PostEffectVisual": PostEffectVisual;
    "RectangleSetter": RectangleSetter;
    "RenderMeshVisual": RenderMeshVisual;
    "RetouchVisual": RetouchVisual;
    "ScreenRegionComponent": ScreenRegionComponent;
    "ScreenTransform": ScreenTransform;
    "Script": ScriptComponent;
    "ScriptComponent": ScriptComponent;
    "Skin": Skin;
    "SplineComponent": SplineComponent;
    "SpriteAligner": SpriteAligner;
    "SpriteVisual": SpriteVisual;
    "SpriteVisualV2": Image;
    "Text": Text;
    "Text3D": Text3D;
    "TextVisual": Label;
    "Touch": InteractionComponent;
    "TouchComponent": InteractionComponent;
    "TrackedPointComponent": TrackedPointComponent;
    "VFXComponent": VFXComponent;
    "VertexCache": VertexCache;
    "Visual": Visual;
    "WorldTracking": DeviceTracking;
}

interface EventNameMap {
    "BrowsLoweredEvent": BrowsLoweredEvent;
    "BrowsRaisedEvent": BrowsRaisedEvent;
    "BrowsReturnedToNormalEvent": BrowsReturnedToNormalEvent;
    "CameraBackEvent": CameraBackEvent;
    "CameraFrontEvent": CameraFrontEvent;
    "ConnectedLensEnteredEvent": ConnectedLensEnteredEvent;
    "DelayedCallbackEvent": DelayedCallbackEvent;
    "FaceFoundEvent": FaceFoundEvent;
    "FaceLostEvent": FaceLostEvent;
    "FaceTrackingEvent": FaceTrackingEvent;
    "HoverEndEvent": HoverEndEvent;
    "HoverEvent": HoverEvent;
    "HoverStartEvent": HoverStartEvent;
    "KissFinishedEvent": KissFinishedEvent;
    "KissStartedEvent": KissStartedEvent;
    "LateUpdateEvent": LateUpdateEvent;
    "ManipulateEndEvent": ManipulateEndEvent;
    "ManipulateStartEvent": ManipulateStartEvent;
    "MouthClosedEvent": MouthClosedEvent;
    "MouthOpenedEvent": MouthOpenedEvent;
    "OnAwakeEvent": OnAwakeEvent;
    "OnDestroyEvent": OnDestroyEvent;
    "OnDisableEvent": OnDisableEvent;
    "OnEnableEvent": OnEnableEvent;
    "OnPauseEvent": OnPauseEvent;
    "OnResumeEvent": OnResumeEvent;
    "OnStartEvent": OnStartEvent;
    "SceneEvent.BrowsWereJustFrownedEvent": BrowsLoweredEvent;
    "SceneEvent.BrowsWereJustRaisedEvent": BrowsRaisedEvent;
    "SceneEvent.BrowsWereJustReturnedToNormalEvent": BrowsReturnedToNormalEvent;
    "SceneEvent.ClientInterfacePlayButtonTriggerEvent": ConnectedLensEnteredEvent;
    "SceneEvent.KissJustFinishedEvent": KissFinishedEvent;
    "SceneEvent.KissJustStartedEvent": KissStartedEvent;
    "SceneEvent.MouthWasJustClosedEvent": MouthClosedEvent;
    "SceneEvent.MouthWasJustOpenedEvent": MouthOpenedEvent;
    "SceneEvent.SmileJustFinishedEvent": SmileFinishedEvent;
    "SceneEvent.SmileJustStartedEvent": SmileStartedEvent;
    "SceneEvent.SurfaceTrackingResetEvent": SurfaceTrackingResetEvent;
    "SceneObjectEvent": SceneObjectEvent;
    "SmileFinishedEvent": SmileFinishedEvent;
    "SmileStartedEvent": SmileStartedEvent;
    "SnapImageCaptureEvent": SnapImageCaptureEvent;
    "SnapRecordStartEvent": SnapRecordStartEvent;
    "SnapRecordStopEvent": SnapRecordStopEvent;
    "TapEvent": TapEvent;
    "TouchEndEvent": TouchEndEvent;
    "TouchMoveEvent": TouchMoveEvent;
    "TouchStartEvent": TouchStartEvent;
    "TriggerPrimaryEvent": TriggerPrimaryEvent;
    "TurnOffEvent": TurnOffEvent;
    "TurnOnEvent": TurnOnEvent;
    "UpdateEvent": UpdateEvent;
    "WorldTrackingMeshesAddedEvent": WorldTrackingMeshesAddedEvent;
    "WorldTrackingMeshesRemovedEvent": WorldTrackingMeshesRemovedEvent;
    "WorldTrackingMeshesUpdatedEvent": WorldTrackingMeshesUpdatedEvent;
    "WorldTrackingPlanesAddedEvent": WorldTrackingPlanesAddedEvent;
    "WorldTrackingPlanesRemovedEvent": WorldTrackingPlanesRemovedEvent;
    "WorldTrackingPlanesUpdatedEvent": WorldTrackingPlanesUpdatedEvent;
    "WorldTrackingResetEvent": SurfaceTrackingResetEvent;
}

/**
* Unhandled Promise rejections are silently ignored by default.
* `failAsync` rethrows the error, ensuring it results in an unhandled exception. Note: error is not rethrown immediately, some pending JS code might still be executed.
* Use it as a default `.catch()` handler when no other error handling is provided.

* Example:

* ```js
* someAsyncFunction().catch(failAsync);
* ```

*/
declare function failAsync(error: any): void

/**
* The start time of the Lens since UNIX Epoch.
*/
declare function getAbsoluteStartTime(): number

/**
* Returns the time difference in seconds between the current frame and previous frame.
*/
declare function getDeltaTime(): number

/**
* Get current time in Nanoseconds. Useful when optimizing a Lens to understand its performance.
*/
declare function getRealTimeNanos(): number

/**
* Returns the time in seconds since the lens was started.
*/
declare function getTime(): number

/**
* Returns true if the passed in object is null or destroyed. Useful as a safe way to check if a SceneObject or Component has been destroyed.
*/
declare function isNull(reference: any): boolean

/**
* Prints out a message to the Logger window.
*/
declare function print(message: any): void

/**
* Load a JavaScript module. Used for importing another JavaScript file found in the Resources panel to be used in the current script. Similar to `require` found in CommonJS specification. You can access Lens Studio specific modules (like `RemoteServiceModule`) using the `LensStudio` prefix, such as: `require("LensStudio:RemoteServiceModule)`.

* You can use the modules name or path, meaning you can have two version of the same module in your project if needed. Both by name or by path will be relative to the script which is calling require, but by name will check the same folder as the script, then the parent folder, and so on. In addition, any require can be relative to your library folder. Take a look at the [Scripting guide](https://developers.snap.com/lens-studio/features/scripting/script-overview) to learn more.
*/
declare function require(moduleName: (ScriptAsset|string)): any

/**
* Load an asset like a `Texture`, `Material`, or `MLAsset` directly from script.

* You can use the modules name or path. Both by name or by path will be relative to the script which is calling require, but by name will check the same folder as the script, then the parent folder, and so on. In addition, any require can be relative to your library folder. Take a look at the [Scripting guide](https://developers.snap.com/lens-studio/essential-skills/scripting/script-overview) to learn more.
*/
declare function requireAsset(name: string): Asset

/**
* Load a script by name or path in order to use that type to create or get a component at runtime.

* You can use the modules name or path. Both by name or by path will be relative to the script which is calling require, but by name will check the same folder as the script, then the parent folder, and so on. In addition, any require can be relative to your library folder. Take a look at the [Scripting guide](https://developers.snap.com/lens-studio/features/scripting/script-overview) to learn more.
*/
declare function requireType(name: string): string

declare namespace global {
    /**
    * Returns the {@link DebugRender}, which provides methods to draw primitive visuals for debugging.
    */
    let debugRenderSystem: DebugRender
    
    /**
    * Returns the global DeviceInfoSystem, which provides information about the device running the Lens.
    */
    let deviceInfoSystem: DeviceInfoSystem
    
    let hapticFeedbackSystem: HapticFeedbackSystem
    
    /**
    * Returns the global `GeneralDataStore` for Launch Params, which provides any special data passed in when the Lens is launched.
    */
    let launchParams: GeneralDataStore
    
    /**
    * Returns the global {@link LocalizationSystem}, which helps convert times, dates, and other units to user friendly strings.
    */
    let localizationSystem: LocalizationSystem
    
    /**
    * Returns the global {@link PersistentStorageSystem}, which allows data to persist between Lens sessions.
    */
    let persistentStorageSystem: PersistentStorageSystem
    
    /**
    * Returns the global {@link ScriptScene} object, which offers information and controls for the current scene.
    */
    let scene: ScriptScene
    
    let textInputSystem: TextInputSystem
    
    /**
    * Returns the global {@link TouchDataProvider}, which controls how the Lens handles touch events.
    */
    let touchSystem: TouchDataProvider
    
    /**
    * Returns the global {@link UserContextSystem}, which provides information about the user such as display name, birthday, and even current weather.
    */
    let userContextSystem: UserContextSystem
    
}

/**
* An object containing the position of an object relative to a LocationAsset.

* @see Returned By: {@link LocationAsset.getGeoAnchoredPosition}
*/
declare class Anchor extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The LocationAsset which this anchor is relative to.
    
    * @readonly
    */
    location: LocationAsset
    
    /**
    * The position of this anchor relative to the LocationAsset.
    
    * @readonly
    */
    position: vec3
    
}

/**
* Provider for animated texture resource.

* @remarks
* Can be accessed from {@link Asset | Texture.Control} on an animated texture.

* @see [2D Animation Guide](https://developers.snap.com/lens-studio/assets-pipeline/2d/2d-animation).

* @see Used By: {@link AnimatedTextureFileProvider#setOnFinish}
* @see Returned By: {@link AnimatedTextureFileProvider#clone}

* @example
* ```
* // Plays an animated sprite
* //@input Component.SpriteVisual spriteVisual

* var loops = 100;
* var offset = 0.0;

* var provider = script.spriteVisual.mainPass.baseTex.control;
* provider.play(loops, offset);
* ```
*/
declare class AnimatedTextureFileProvider extends TextureProvider {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Duplicates the AnimatedTextureFileProvider and returns the new copy. Can be used for playing the same animation at different offsets.
    */
    clone(): AnimatedTextureFileProvider
    
    /**
    * Returns the index of the frame that is currently playing.
    */
    getCurrentPlayingFrame(): number
    
    /**
    * Returns how long the animation is in seconds.
    
    * @deprecated
    */
    getDuration(): number
    
    /**
    * Returns the number of frames in the animation.
    */
    getFramesCount(): number
    
    /**
    * Returns whether the animation is finished playing.
    */
    isFinished(): boolean
    
    /**
    * Returns whether the animation is currently paused.
    */
    isPaused(): boolean
    
    /**
    * Returns whether the animation is currently playing.
    */
    isPlaying(): boolean
    
    /**
    * Pauses the animation.
    */
    pause(): void
    
    /**
    * Pauses the animation at frame `frameIndex`.
    */
    pauseAtFrame(frameIndex: number): void
    
    /**
    * Plays the animation `loops` times, starting with an offset of `offset` seconds.
    */
    play(loops: number, offset: number): void
    
    /**
    * Start playing the animation from frame `frameIndex`, `loops` times.
    */
    playFromFrame(frameIndex: number, loops: number): void
    
    /**
    * Resumes a paused animation from the frame that was last played.
    */
    resume(): void
    
    /**
    * Sets the callback function to be called whenever the animation stops playing.
    */
    setOnFinish(eventCallback: (animatedTexture: AnimatedTextureFileProvider) => void): void
    
    /**
    * Stops the animation.
    */
    stop(): void
    
    /**
    * Length of the animation in seconds.
    */
    duration: number
    
    /**
    * Returns whether the animation was set to automatically play and loop.
    */
    isAutoplay: boolean
    
    /**
    * If enabled, the animation will alternate between normal and reverse each time it loops.
    */
    isPingPong: boolean
    
    /**
    * Whether the animation plays in reverse.
    */
    isReversed: boolean
    
    /**
    * The animation track used to control the frame animation.
    
    * @deprecated
    */
    track: IntStepAnimationTrackKeyFramed
    
}

/**
* This class is deprecated. Used in Lens Studio 4.55 to animate single object in the hierarchy. See {@link AnimationPlayer} instead.

* @remarks These are automatically added to scene objects when importing animated FBX files. Used by {@link AnimationMixer}.

* @see {@link AnimationMixer}
* @see {@link AnimationLayer}

* @deprecated
*/
declare class Animation extends Component {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns the AnimationLayer under the name `layerName`.
    
    * @deprecated
    */
    getAnimationLayerByName(layerName: string): AnimationLayer
    
    /**
    * Removes the AnimationLayer under the name `layerName`.
    
    * @deprecated
    */
    removeAnimationLayerByName(layerName: string): void
    
    /**
    * Adds an AnimationLayer under the name `layerName`.
    
    * @deprecated
    */
    setAnimationLayerByName(layerName: string, animationLayer: AnimationLayer): void
    
}

/**
* Represents animation data. Can have multiple {@link AnimationPropertyLayer}s. Used in {@link AnimationClip}.

* @remarks
* AnimationAssets themselves do not handle playing or orchestrating animations. This is left to the {@link AnimationPlayer} component to handle.

* @see Used By: {@link AnimationClip#animation}, {@link AnimationClip.createFromAnimation}

* @example
* ```js
* // @input Asset.AnimationAsset animAsset
* const startEventName = 'animStarted';
* const startTimestamp = 0;
* const e = script.animAsset.createEvent(startEventName, startTimestamp);
* ```

* ```js
* const endEventName = 'animEnded';
* const endTimestamp = script.animAsset.duration;
* script.animAsset.createEvent(endEventName, endTimestamp);
* ```
*/
declare class AnimationAsset extends Asset {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Adds AnimationPropertyLayer to Animation asset.
    */
    addLayer(layerName: string, layer: AnimationPropertyLayer): void
    
    /**
    * Delete all the AnimationPropertyLayer in this AnimationAsset.
    */
    clearLayers(): void
    
    /**
    * Creates an event that will be triggered at a given time of this animation asset.
    */
    createEvent(eventName: string, time: number): AnimationPropertyEventRegistration
    
    /**
    * Deletes an event that will be triggered at a given time.
    */
    deleteEvent(registration: AnimationPropertyEventRegistration): void
    
    /**
    * Delete the AnimationPropertyLayer named `layerName`.
    */
    deleteLayer(layerName: string): void
    
    /**
    * Retrieves an animation associated with a particular object.
    */
    getLayer(layerName: string): AnimationPropertyLayer
    
    /**
    * Animation duration in seconds.
    
    * @readonly
    */
    duration: number
    
    /**
    * Denotes how many key frames this animation was sampled at.
    
    * @readonly
    */
    fps: number
    
}

/**
* Animation Clip is what an Animation Player uses to manage playback for a specific animation. It defines that animation by referencing an Animation Asset and providing start and end points, playback speed and direction, and blending information.

* @see Used By: {@link AnimationPlayer#addClip}
* @see Returned By: {@link AnimationClip#clone}, {@link AnimationClip.create}, {@link AnimationClip.createFromAnimation}, {@link AnimationPlayer#getClip}

* @example
* ```js
* //@input Component.AnimationPlayer animationPlayer
* //@input Asset.AnimationAsset myAnimation

* const myClip = AnimationClip.create("myAnimationClip");
* myClip.animation = script.myAnimation;
* myClip.begin = 0;
* myClip.end = 3.75;
* myClip.weight = 1.0;
* myClip.playbackSpeed = 0.5;
* myClip.playbackMode = PlaybackMode.Loop;
* myClip.reversed = true;
* myClip.disabled = false;

* script.animationPlayer.addClip(myClip);
* script.animationPlayer.playClipAt(myClip.name, 0.0);
* ```
*/
declare class AnimationClip extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Clones the existing clip with a new name.
    */
    clone(clipName: string): AnimationClip
    
    /**
    * Points to the animation asset to be played by the Animation Player.
    */
    animation: AnimationAsset
    
    /**
    * Returns begin time of clip (in seconds).
    */
    begin: number
    
    /**
    * The blend mode for this particular clip.
    */
    blendMode: AnimationLayerBlendMode
    
    /**
    * Whether the animation clip is disabled.
    */
    disabled: boolean
    
    /**
    * Returns the duration of the clip which is calculated based on the begin and end times (in seconds).
    
    * @readonly
    */
    duration: number
    
    /**
    * Returns end time of clip (in seconds).
    */
    end: number
    
    /**
    * Name of the clip.
    
    * @readonly
    */
    name: string
    
    /**
    * The offset (in seconds) which is the starting point for an animation. After the first iteration, it starts back at the beginning of the clip.
    */
    offset: number
    
    /**
    * Choose whether to play animation clip once, loop the clip, or ping pong it.
    */
    playbackMode: PlaybackMode
    
    /**
    * Scalar value to represent playback speed percentage. 1.0 is 100% playback speed.
    */
    playbackSpeed: number
    
    /**
    * Specifies if the clip should be played reversed.
    */
    reversed: boolean
    
    /**
    * How scale is accumulated. Usually does not need to be changed after import.
    */
    scaleMode: AnimationLayerScaleMode
    
    /**
    * Strength of animation clip contribution. Lies between [0.0, 1.0] inclusive. For default blending a 1.0 weight indicates this clip will override all earlier clips, a less than 1.0 weight indicates it will blend onto the calculated pose using a weighted average.
    */
    weight: number
    
    /**
    * Creates a clip.
    */
    static create(clipName: string): AnimationClip
    
    /**
    * Creates an animation clip from an animation asset.
    */
    static createFromAnimation(clipName: string, animation: AnimationAsset): AnimationClip
    
}

declare namespace AnimationClip {
    /**
    * Used by {@link AnimationMixerLayer} for setting animation looping behavior.
    
    * @deprecated
    
    * @example
    * ```js
    * // Set an AnimationMixerLayer to oscillate when looping
    * //@input Component.AnimationMixer mixer
    
    * var layer = script.mixer.getLayers()[0];
    * layer.postInfinity = AnimationClip.PostInfinity.Oscillate;
    * ```
    */
    enum PostInfinity {
        /**
        * The animation will restart from the beginning each time it loops.
        
        * @deprecated
        */
        Cycle,
        /**
        * The animation will switch between normal and reverse playback each time it loops.
        
        * @deprecated
        */
        Oscillate
    }

}

declare namespace AnimationClip {
    /**
    * Used by {@link AnimationMixerLayer} for setting animation clip range type.
    
    * @deprecated
    
    * @example
    * ```js
    * // Set an AnimationMixerLayer's range using start and end time
    * //@input Component.AnimationMixer mixer
    
    * var layer = script.mixer.getLayers()[0];
    * layer.rangeType = AnimationClip.RangeType.Time;
    * layer.from = 1.0;
    * layer.to = 2.0;
    * ```
    */
    enum RangeType {
        /**
        * Range is specified by start and end time, in seconds
        
        * @deprecated
        */
        Time,
        /**
        * Range is specified by start and end frame numbers
        
        * @deprecated
        */
        Frames
    }

}

/**
* A curve that contains a set of keyframes and can evaluate values at specific timestamps.

* @see Used By: {@link AnimationCurveTrack#setProperty}
* @see Returned By: {@link AnimationCurve.create}, {@link AnimationCurve.createEasingCurve}, {@link AnimationCurveTrack#getProperty}

* @example
* ```js
* // Example of using Animation Curve

* // creating keyframes
* var A = AnimationCurve.createKeyFrame();
* var B = AnimationCurve.createKeyFrame();

* // creating AnimationCurve
* var curve = AnimationCurve.create();
* A.time = 0.0;
* A.value = 0.0;

* B.time = 1.0;
* B.value = 1.0;

* curve.addKeyframe(A);
* curve.addKeyframe(B);
* let idxB = curve.addKeyframe(B);

* // evaluating value
* var value = curve.evaluate(0.5);
* ```
*/
declare class AnimationCurve extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Adds keyframe to the curve.
    */
    addKeyframe(frame: AnimationKeyFrame): void
    
    /**
    * Evaluate value of the curve at specific point.
    */
    evaluate(time: number): number
    
    /**
    * Get an `AnimationKeyFrame` at the passed in index.
    */
    getKeyFrame(index: number): AnimationKeyFrame
    
    /**
    * Remove animation keyframe at specific timestamp. The closest keyframe will be deleted.
    */
    removeKeyFrame(t: number): void
    
    /**
    * The number of keyframes in the animation curve.
    
    * @readonly
    */
    keyFrameCount: number
    
    /**
    * Create new animation curve.
    */
    static create(): AnimationCurve
    
    /**
    * Creates curve based on CSS easing values.
    */
    static createEasingCurve(startValue: number, endValue: number, x1: number, y1: number, x2: number, y2: number): AnimationCurve
    
    /**
    * Create new animation keyframe
    */
    static createKeyFrame(): AnimationKeyFrame
    
}

/**
* Container for one or more {@link AnimationCurve}s. Can be sampled to drive attributes (e.g. animation, vfx, code etc.)

* @remarks
* When evaluating multiple values, the values are selected from left to right in order. For example, for a {@link vec3} containing `x`,`y`,`z` it will correspond to track index `0`, `1`, `2` from left to right.

* @see Returned By: {@link AnimationCurveTrack.create}
*/
declare class AnimationCurveTrack extends AnimationTrack {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Samples the track at the given time to get some value.
    */
    evaluateNumber(time: number): number
    
    /**
    * Samples the track at the given time to get some value. Returns 0 for non-existent channels.
    */
    evaluateRotation(time: number): quat
    
    /**
    * Samples the track at the given time to get some value. Returns 0 for non-existent channels.
    */
    evaluateVec2(time: number): vec2
    
    /**
    * Samples the track at the given time to get some value. Returns 0 for non-existent channels.
    */
    evaluateVec3(time: number): vec3
    
    /**
    * Samples the track at the given time to get some value. Returns 0 for non-existent channels.
    */
    evaluateVec4(time: number): vec4
    
    /**
    * Retrieves an AnimationCurve associated with the given key.
    */
    getProperty(key: string): AnimationCurve
    
    /**
    * Returns an array of strings, which are the names of properties associated with the AnimationCurveTrack.
    */
    getPropertyKeys(): string[]
    
    /**
    * Sets an AnimationCurve to a given key.
    */
    setProperty(key: string, curve: AnimationCurve): void
    
    /**
    * Create new animation curve track.
    */
    static create(name: string): AnimationCurveTrack
    
}

/**
* A keyframe with time and respective value. Could be added to Animation Curve.

* @see Used By: {@link AnimationCurve#addKeyframe}
* @see Returned By: {@link AnimationCurve#getKeyFrame}, {@link AnimationCurve.createKeyFrame}

* @example
* ```js
* // Example of creating keyframe

* var keyFrame = AnimationCurve.createKeyFrame();
* // inTangent = outTangent = 0, inWeight = outWeight = 0.33(3)
* // weightedMode = None, leftTangentType = Free, rightTangentType = Free

* //Change weighted control points
* keyFrame.time = 0.0;
* keyFrame.value = 0.0;
* keyFrame.inWeightPoint.y = 5;
* keyFrame.inWeightPoint.x = 0.5;

* keyFrame.time = 1.0;
* keyFrame.value = 1.0;
* keyFrame.outWeightPoint.y = -10;
* keyFrame.outWeightPoint.x = 0.7;
* ```
*/
declare class AnimationKeyFrame extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Incoming Control Point.
    */
    inWeightPoint: vec2
    
    leftTangentType: TangentType
    
    /**
    * Outgoing Control Point.
    */
    outWeightPoint: vec2
    
    rightTangentType: TangentType
    
    /**
    * Timestamp of the keyframe.
    */
    time: number
    
    /**
    * Value of the respective timestamp.
    */
    value: number
    
    weightedMode: WeightedMode
    
}

/**
* Configures an animation layer for a single {@link SceneObject}.
* Gives access to position, rotation, scale and blend shape animation tracks.

* @see [Playing 3D Animation Guide](https://developers.snap.com/lens-studio/essential-skills/adding-interactivity/additional-examples/playing-3d-animation)
* @see {@link AnimationMixer}

* @deprecated
*/
declare class AnimationLayer extends AnimationTrack {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns a {@link FloatAnimationTrack} from this AnimationLayer's blend shapes.
    
    * @deprecated
    */
    getBlendShapeTrack(shapeName: string): FloatAnimationTrack
    
    /**
    * Sets or adds a {@link FloatAnimationTrack} to this AnimationLayer's blend shapes.
    
    * @deprecated
    */
    setBlendShapeTrack(shapeName: string, track: FloatAnimationTrack): void
    
    /**
    * The {@link Vec3AnimationTrack} controlling position in this AnimationLayer.
    
    * @deprecated
    */
    position: Vec3AnimationTrack
    
    /**
    * The {@link QuaternionAnimationTrack} controlling rotation in this AnimationLayer.
    
    * @deprecated
    */
    rotation: QuaternionAnimationTrack
    
    /**
    * The {@link Vec3AnimationTrack} controlling scale in this AnimationLayer.
    
    * @deprecated
    */
    scale: Vec3AnimationTrack
    
    /**
    * The {@link IntAnimationTrack} controlling visibility in this AnimationLayer.
    
    * @deprecated
    */
    visibility: IntAnimationTrack
    
}

/**
* How animation layers are blended.

* @see Used By: {@link AnimationClip#blendMode}
*/
declare enum AnimationLayerBlendMode {
    /**
    * The higher layer will override all other animation layers.
    */
    Default,
    /**
    * The higher layer will be added on top of other animation layers.
    */
    Additive
}

/**
* The method in which an Animation Layer should be scaled to other layers in an `AnimationClip`.

* @see Used By: {@link AnimationClip#scaleMode}
*/
declare enum AnimationLayerScaleMode {
    /**
    * Multiply the layers value.
    */
    Multiply,
    /**
    * Add the layers value.
    */
    Additive
}

/**
* Controls playback of animations on the attached {@link SceneObject} and its child objects.
* Please refer to the [Playing 3D Animation Guide](https://developers.snap.com/lens-studio/essential-skills/adding-interactivity/additional-examples/playing-3d-animation) for setting up and playing animations.

* @deprecated

* @example
* ```js
* // PlayAnimation.js
* // Version: 0.0.3
* // Event: Lens Turned On
* // Description: Plays a single animation on an animated mesh. If an AnimationMixer is not set, the script will attempt to find one on the SceneObject.
* //    @input Component.AnimationMixer animationMixer
* //    @input string animationLayerName = "BaseLayer"
* //    @input float animationWeight = 1.0 {"widget":"slider", "min": 0, "max": 1, "step": 0.01}
* //    @input float animationStartOffset = 0.0
* //    @input int numberOfLoops = -1
* if(!script.animationMixer)
* {
* 	script.animationMixer = script.getSceneObject().getFirstComponent("Component.AnimationMixer");
* }
* if(script.animationMixer)
* {
* 	script.animationMixer.setWeight(script.animationLayerName, script.animationWeight);
* 	script.animationMixer.start(script.animationLayerName, script.animationStartOffset, script.numberOfLoops);
* }
* ```
*/
declare class AnimationMixer extends Component {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Makes a copy of the layer `name` and stores it as `newName`.
    
    * @deprecated
    */
    cloneLayer(name: string, newName: string): AnimationMixerLayer
    
    /**
    * Adds a new AnimationMixerLayer to this AnimationMixer.
    
    * @deprecated
    */
    createClip(name: string): AnimationMixerLayer
    
    /**
    * Returns a list of names of AnimationLayers in this AnimationMixer.
    
    * @deprecated
    */
    getAnimationLayerNames(): string[]
    
    /**
    * Returns the AnimationMixerLayer with the name `name`.
    
    * @deprecated
    */
    getLayer(name: string): AnimationMixerLayer
    
    /**
    * Returns the current time (in seconds) of the layer named `name`.
    
    * @deprecated
    */
    getLayerTime(name: string): number
    
    /**
    * Returns a list of all AnimationMixerLayers controlled by the AnimationMixer.
    
    * @deprecated
    */
    getLayers(): AnimationMixerLayer[]
    
    /**
    * Pauses animation layers named `name`, or all layers if `name` is empty.
    
    * @deprecated
    */
    pause(name: string): void
    
    /**
    * Rebuild the animation hierarchy by finding all {@link Animation} components in the SceneObject and its children.
    
    * @deprecated
    */
    resetAnimations(): void
    
    /**
    * Resumes any paused animation layer with name `name`, or all layers if `name` is empty.
    
    * @deprecated
    */
    resume(name: string): void
    
    /**
    * Sets the weight of any layers with name `name`.
    
    * @deprecated
    */
    setWeight(name: string, weight: number): void
    
    /**
    * Starts playing animation layers named `name`, or all layers if `name` is empty. The animation will start with an offset of `offset` seconds. The animation will play `cycles` times, or loop forever if `cycles` is -1.
    
    * @deprecated
    */
    start(name: string, offset: number, cycles: number): void
    
    /**
    * Starts playing animation layers named `name`, or all layers if `name` is empty. The animation will start with an offset of `offset` seconds. The animation will play `cycles` times, or loop forever if `cycles` is -1. `eventCallback` will be called after any animation layer finishes playing.
    
    * @deprecated
    */
    startWithCallback(name: string, offset: number, cycles: number, eventCallback: (name: string, animationMixer: AnimationMixer) => void): void
    
    /**
    * Stops any animation layer with name `name`, or all layers if `name` is empty.
    
    * @deprecated
    */
    stop(name: string): void
    
    /**
    * Whether this AnimationMixer is set to automatically play animations on start.
    
    * @deprecated
    */
    autoplay: boolean
    
    /**
    * A multiplying value for the speed of all animations being controlled by the AnimationMixer.
    * For example, a value of 2.0 will double animation speed, while a value of 0.5 will cut the speed in half.
    
    * @deprecated
    */
    speedRatio: number
    
}

/**
* Controls animation playback for a single animation layer.
* See also: {@link AnimationMixer}.

* @deprecated

* @example
* ```js
* // Double the speed of the first AnimationMixerLayer and play it once
* //@input Component.AnimationMixer mixer

* var layer = script.mixer.getLayers()[0];
* layer.speedRatio = 2.0;
* layer.start(0, 1);
* ```
*/
declare class AnimationMixerLayer extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns a copy of this AnimationMixerLayer, with the name changed to `newName`.
    
    * @deprecated
    */
    clone(newName: string): AnimationMixerLayer
    
    /**
    * Returns the length of the animation in seconds.
    
    * @deprecated
    */
    getDuration(): number
    
    /**
    * Returns the current playback position of the animation in seconds.
    
    * @deprecated
    */
    getTime(): number
    
    /**
    * Returns whether the animation is currently playing.
    
    * @deprecated
    */
    isPlaying(): boolean
    
    /**
    * Pauses the animation.
    
    * @deprecated
    */
    pause(): void
    
    /**
    * Resumes the animation if it has been paused.
    
    * @deprecated
    */
    resume(): void
    
    /**
    * Starts playing the animation with an offset of `offsetArg` seconds.
    * The animation will play `cycles` times, or loop forever if `cycles` is -1.
    
    * @deprecated
    */
    start(offset: number, cycles: number): void
    
    /**
    * Starts the animation with an offset of `offsetArg` seconds.
    * The animation will play `cycles` times, or loop forever if `cycles` is -1.
    * `eventCallback` will be called after the animation finishes.
    
    * @deprecated
    */
    startWithCallback(offset: number, cycles: number, eventCallback: (name: string, animationMixer: AnimationMixer) => void): void
    
    /**
    * Stops the animation from playing and jumps to the animation's end.
    
    * @deprecated
    */
    stop(): void
    
    /**
    * The name of the animation layer being used for this animation.
    
    * @deprecated
    */
    animationLayerName: string
    
    /**
    * @deprecated
    */
    blendMode: AnimationLayerBlendMode
    
    /**
    * The number of times this animation will play. If -1, the animation will loop forever.
    
    * @deprecated
    */
    cycles: number
    
    /**
    * If true, the animation will stop having an effect.
    
    * @deprecated
    */
    disabled: boolean
    
    /**
    * The framerate (frames per second) of the animation.
    
    * @deprecated
    */
    fps: number
    
    /**
    * The starting point for this animation clip.
    * If `rangeType` is set to `Time`, this is the point to start at in seconds.
    * If `rangeType` is set to `Frames`, this is the frame number to start at.
    
    * @deprecated
    */
    from: number
    
    /**
    * The name of the AnimationMixerLayer.
    
    * @deprecated
    */
    name: string
    
    /**
    * Defines the animation's looping behavior.
    * If set to `AnimationClip.PostInfinity.Cycle`, the animation will restart from the beginning each time it loops.
    * If set to `AnimationClip.PostInfinity.Oscillate`, the animation will switch between normal and reverse playback each time it loops.
    * This is set to `Cycle` by default.
    
    * @deprecated
    */
    postInfinity: AnimationClip.PostInfinity
    
    /**
    * The range type used for defining the animation clip.
    * If set to `AnimationClip.RangeType.Time`, `to` and `from` represent times in seconds.
    * If set to `AnimationClip.RangeType.Frames`, `to` and `from` represent frame numbers.
    
    * @deprecated
    */
    rangeType: AnimationClip.RangeType
    
    /**
    * If true, the animation will play play in reverse.
    
    * @deprecated
    */
    reversed: boolean
    
    /**
    * @deprecated
    */
    scaleMode: AnimationLayerScaleMode
    
    /**
    * A multiplying value for the speed of this animation.
    * For example, a value of 2.0 will double animation speed, while a value of 0.5 will cut the speed in half.
    
    * @deprecated
    */
    speedRatio: number
    
    /**
    * The ending point for this animation clip.
    * If `rangeType` is set to `Time`, this is the point to end at in seconds.
    * If `rangeType` is set to `Frames`, this is the frame number to end at.
    
    * @deprecated
    */
    to: number
    
    /**
    * The weight of this animation layer. Range is from [0-1], 0 being no animation strength and 1 being full animation strength.
    
    * @deprecated
    */
    weight: number
    
}

/**
* Controls animation playback. The component takes in a list of {@link AnimationClip}s, and allows you to play, stop, resume, subscribe to animation events, and more.

* @see [Working With Animation](https://developers.snap.com/lens-studio/features/animation/overview)
* @see [Animation Player Guide](https://developers.snap.com/lens-studio/features/animation/animation-player)
* @see [Animation State Manager](https://developers.snap.com/lens-studio/features/animation/animation-state-manager) for managing animation states and blending.

* @example
* ```js
* /**
*  * Transition between two Animation Clips `firstClipName` and `secondClipName`
*  * on tap by alternating secondClip weight between 0 or 1 within some `transitionInSec`,
*  * ensuring that secondClip is processed after firstClip as to blend the animations correctly.
*  *\/

* //@input Component.AnimationPlayer animationPlayer
* //@input number transitionInSec = 1
* //@input string firstClipName
* //@input string secondClipName

* // Keep track of current state of transition.
* let isTransitioning = false;
* let accumulatedTime = 0;
* let secondClipTargetWeight = 1;

* // Store a reference to the clips so we don't
* // have to keep asking engine.
* let firstClip, secondClip;

* /*********************************************************
* Events that triggers and responds to the transitions.
*********************************************************\/

* // Trigger the transition to the new clip.
* const startTransition = script.createEvent("TapEvent");
* startTransition.bind(triggerTransition);

* const onUpdateEvent = script.createEvent("UpdateEvent");
* onUpdateEvent.bind(interpolateBetweenAnimations);

* /*********************************************************
* Define how transitioning works.
*********************************************************\/

* // Play the first clip immediately.
* // and make sure clips are in the right order.
* function init() {
*     script.animationPlayer.playClipAt(script.firstClipName, 0);

*     // Clips are processed in order. Thus, if we want to transition
*     // to a second clip, we need to make sure the second clip
*     // is processed after the first clip.
*     putClipAtEndIfNeeded(script.animationPlayer, script.secondClipName);

*     // Store references to the clips.
*     firstClip = script.animationPlayer.getClip(script.firstClipName);
*     secondClip = script.animationPlayer.getClip(script.secondClipName);
* }

* // Trigger the transition to the new clip.
* function triggerTransition() {
*     // Only transition, if we're not in the middle of transitioning.
*     if (!isTransitioning) {
*         // If the target weight is 0, that means the clip is already playing
*         // so we don't need to tell it to play again.
*         if (secondClipTargetWeight === 1) {
*             script.animationPlayer.playClipAt(script.secondClipName, 0);
*         }

*         accumulatedTime = 0;
*         isTransitioning = true;

*         // Begin the update loop that will transition the clips.
*         onUpdateEvent.enabled = isTransitioning;
*     }
* }

* // Blend the weight of a second clip to the first clip,
* // based on the time elapsed in transition.
* function interpolateBetweenAnimations () {
*     if (isTransitioning) {
*         // Keep track of how many seconds elapsed since the start of the transition.
*         accumulatedTime += getDeltaTime();

*         // Once time has elapsed beyond our total transition time
*         // we can finalize our weight and callback.
*         if (accumulatedTime >= script.transitionInSec) {
*             secondClip.weight = secondClipTargetWeight;

*             onTransitionEnd();

*             return;
*         }

*         // Set the current weight of the clips based on % between how long we've beeen transitioning for
*         // and how long we want total transition to take.
*         let currentWeight = accumulatedTime / script.transitionInSec ;
*         secondClip.weight = lerp(1-secondClipTargetWeight, secondClipTargetWeight, currentWeight);
*     }
* }


* function onTransitionEnd() {
*     // Set our target weight to be the opposite.
*     secondClipTargetWeight = 1 - secondClipTargetWeight;

*     // Enable triggering transition, and disable the update loop.
*     isTransitioning = false;
*     onUpdateEvent.enabled = isTransitioning;
* }

* /*********************************************************
* Helpers
*********************************************************\/

* function putClipAtEndIfNeeded(animationPlayer, clipName) {
*     const clips = animationPlayer.getAnimationClips();
*     const currentOrderOfClips = clips.map(function(clip) {return clip.name;});
*     const clipIndex = currentOrderOfClips.indexOf(clipName);

*     if (clipIndex != clips.length - 1) {
*         if (clipIndex < 0) {
*             print("Clip with that name is not found in the provided Animation Player.");
*             return;
*         }

*         const clonedClip = clips[clipIndex].clone(clipName);
*         animationPlayer.removeClip(clipName);
*         animationPlayer.addClip(clonedClip);
*     }
* }

* function lerp(a, b, t) {
*     return a * (1.0 - t) + b * t;
* }

* /*********************************************************
* Initialize the script!
*********************************************************\/

* init();
* ```
*/
declare class AnimationPlayer extends Component {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Adds a clip to the player. If one exists, replace existing clip.
    */
    addClip(clip: AnimationClip): void
    
    /**
    * Updates the animation player forcing sampling, resulting in the setting of transforms and firing of animation events.
    */
    forceUpdate(deltaTime: number): void
    
    /**
    * Get currently playing clips.
    */
    getActiveClips(): string[]
    
    /**
    * Tries to get a clip from the player, returns null if it does not exist.
    */
    getClip(name: string): AnimationClip
    
    /**
    * Returns the current time for a clip.
    */
    getClipCurrentTime(name: string): number
    
    /**
    * Returns if a clip is enabled for playback.
    */
    getClipEnabled(name: string): boolean
    
    /**
    * Returns if a clip is playing.
    */
    getClipIsPlaying(name: string): boolean
    
    /**
    * Get currently inactive clips.
    */
    getInactiveClips(): string[]
    
    /**
    * Pauses all clips.
    */
    pauseAll(): void
    
    /**
    * Pause the clip with name.
    */
    pauseClip(name: string): void
    
    /**
    * Plays all clips.
    */
    playAll(): void
    
    /**
    * Plays the named clip on the Animation Player.
    */
    playClip(name: string): void
    
    /**
    * Plays clip with the given name and starting from the given time.
    */
    playClipAt(name: string, time: number): void
    
    /**
    * Removes a clip from the player.
    */
    removeClip(name: string): void
    
    /**
    * Resumes all clips.
    */
    resumeAll(): void
    
    /**
    * Resumes clip with name.
    */
    resumeClip(name: string): void
    
    /**
    * Sets the clip to be enabled.
    */
    setClipEnabled(name: string, enabled: boolean): void
    
    /**
    * Stops all clips and resets time to t = 0.
    */
    stopAll(): void
    
    /**
    * Stops the clip and resets time to t = 0.
    */
    stopClip(name: string): void
    
    /**
    * Array of animation clips
    
    * @readonly
    */
    clips: AnimationClip[]
    
    /**
    * Bind a function to listen to the specified events emitted by `AnimationAsset` events.
    
    * @readonly
    */
    onEvent: event1<AnimationPlayerOnEventArgs, void>
    
}

/**
* Args used for AnimationPlayer's event, which is triggered every time the animation playback passes the given time in the event.
*/
declare class AnimationPlayerOnEventArgs extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Name of the event to emit.
    
    * @readonly
    */
    eventName: string
    
}

/**
* The event registration returned by `AnimationAsset`'s `createEvent`.

* @see Used By: {@link AnimationAsset#deleteEvent}
* @see Returned By: {@link AnimationAsset#createEvent}
*/
declare class AnimationPropertyEventRegistration extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
}

/**
* A layer containing different properties. Examples include position, rotation, scale or any other arbitrary properties a user would like to add and sample from.

* @see Used By: {@link AnimationAsset#addLayer}
* @see Returned By: {@link AnimationAsset#getLayer}

* @example
* The name of the layer.
*/
declare class AnimationPropertyLayer extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Represents the base class for animation tracks.
*/
declare class AnimationTrack extends Asset {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Allows the Lens to incorporate voice transcription with higher quality than the {@link VoiceMlModule} and supports a vast number of different languages.

* @example
* # JavaScript
* ```js
* const asrModule = require("LensStudio:AsrModule");

* function onTranscriptionError(errorCode) {
*   print(`onTranscriptionErrorCallback errorCode: ${errorCode}`);
*   switch (errorCode) {
*     case AsrModule.AsrStatusCode.InternalError:
*       print("stopTranscribing: Internal Error");
*       break;
*     case AsrModule.AsrStatusCode.Unauthenticated:
*       print("stopTranscribing: Unauthenticated");
*       break;
*     case AsrModule.AsrStatusCode.NoInternet:
*       print("stopTranscribing: No Internet");
*       break;
*   }
* }

* function onTranscriptionUpdate(eventArgs) {
*    var text = eventArgs.text;
*    var isFinal = eventArgs.isFinal;
*    print(`onTranscriptionUpdateCallback text=${text}, isFinal=${isFinal}`)
* }

* function startSession() {
*   var options = AsrModule.AsrTranscriptionOptions.create();
*   options.silenceUntilTerminationMs = 1000;
*   options.mode = AsrModule.AsrMode.HighAccuracy;
*   options.onTranscriptionUpdateEvent.add(onTranscriptionUpdateCallback);
*   options.onTranscriptionErrorEvent.add(onTranscriptionErrorCallback);

*   // Start session
*   asrModule.startTranscribing(options);
* }

* function stopSession() {
*   asrModule
*   .stopTranscribing()
*   .then(function () {
*     print(
*       `stopTranscribing successfully`
*     );
*   });
* }
* ```
* # TypeScript
* ```ts
* @component
* export class AsrExample extends BaseScriptComponent {
*   private asrModule = require("LensStudio:AsrModule")

*   private onTranscriptionUpdate(eventArgs: AsrModule.TranscriptionUpdateEvent) {
*     print(`onTranscriptionUpdateCallback text=${eventArgs.text}, isFinal=${eventArgs.isFinal}`)
*   }

*   private onTranscriptionError(eventArgs: AsrModule.AsrStatusCode) {
*     print(`onTranscriptionErrorCallback errorCode: ${eventArgs}`);
*     switch (eventArgs) {
*       case AsrModule.AsrStatusCode.InternalError:
*         print("stopTranscribing: Internal Error");
*         break;
*       case AsrModule.AsrStatusCode.Unauthenticated:
*         print("stopTranscribing: Unauthenticated");
*         break;
*       case AsrModule.AsrStatusCode.NoInternet:
*         print("stopTranscribing: No Internet");
*         break;
*     }
*   }

*   onAwake(): void {
*     const options = AsrModule.AsrTranscriptionOptions.create()
*     options.silenceUntilTerminationMs = 1000
*     options.mode = AsrModule.AsrMode.HighAccuracy
*     options.onTranscriptionUpdateEvent.add((eventArgs) => this.onTranscriptionUpdate(eventArgs))
*     options.onTranscriptionErrorEvent.add((eventArgs) => this.onTranscriptionError(eventArgs))

*     this.asrModule.startTranscribing(options)
*   }

*   private stopSession(): void {
*     this.asrModule.stopTranscribing()
*   }
* }
* ```
*/
declare class AsrModule extends Asset {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Starts a new ASR session. While active the Lens will transcribe speech to text.
    * `asrTranscriptionOptions` provides the options for the session
    
    * The event `onTranscriptionUpdateEvent` is triggered when the transcription is updated.
    * The event `onTranscriptionErrorEvent` is triggered when there is an error in the transcription process.
    
    * If `startTranscribing` is called again while a session is active, the current session is cancelled and a new one started.
    
    * @exposesUserData
    
    * @wearableOnly
    */
    startTranscribing(transcriptionOptions: AsrModule.AsrTranscriptionOptions): void
    
    /**
    * Stops an active ASR Session before transcription is finished and discards the current session.
    
    * @exposesUserData
    
    * @wearableOnly
    */
    stopTranscribing(): Promise<void>
    
}

declare namespace AsrModule {
    /**
    * The mode of the transcription to prioritise, between speed and accuracy.
    
    * @see Used By: {@link AsrModule.AsrTranscriptionOptions#mode}
    
    * @wearableOnly
    
    * @example
    * ```ts
    * const highAccuracyMode = AsrModule.AsrMode.HighAccuracy
    * const balancedMode = AsrModule.AsrMode.Balanced
    * const highSpeedMode = AsrModule.AsrMode.HighSpeed
    * ```
    */
    enum AsrMode {
        /**
        * Focus on higher accuracy at the cost of speed.
        
        * @wearableOnly
        */
        HighAccuracy,
        /**
        * A balance between accuracy and speed.
        
        * @wearableOnly
        */
        Balanced,
        /**
        * Faster transcription time but at the cost of accuracy.
        
        * @wearableOnly
        */
        HighSpeed
    }

}

declare namespace AsrModule {
    /**
    * An enum stating the status of the transcription.
    
    * @wearableOnly
    
    * @example
    * ```ts
    * const successCode = AsrModule.AsrStatusCode.Success
    * const internalErrorCode = AsrModule.AsrStatusCode.InternalError
    * const unauthenticatedCode = AsrModule.AsrStatusCode.Unauthenticated
    * const noInternetCode = AsrModule.AsrStatusCode.NoInternet
    * ```
    */
    enum AsrStatusCode {
        /**
        * Asr operation was successfully executed.
        
        * @wearableOnly
        */
        Success,
        /**
        * An unknown internal error occurred.
        
        * @wearableOnly
        */
        InternalError,
        /**
        * User is not authenticated.
        
        * @wearableOnly
        */
        Unauthenticated,
        /**
        * Device is not connected to the internet.
        
        * @wearableOnly
        */
        NoInternet
    }

}

declare namespace AsrModule {
    /**
    * Provides the options for the session.
    
    * @see Used By: {@link AsrModule#startTranscribing}
    * @see Returned By: {@link AsrModule.AsrTranscriptionOptions.create}
    
    * @wearableOnly
    
    * @example
    * ```ts
    * const options = AsrModule.AsrTranscriptionOptions.create()
    * options.silenceUntilTerminationMs = 1000
    * options.mode = AsrModule.AsrMode.HighAccuracy
    * options.onTranscriptionUpdateEvent.add((eventArgs) => print(eventArgs))
    * options.onTranscriptionErrorEvent.add((eventArgs) => print(`Error while transcribing: ${eventArgs}`))
    * ```
    */
    class AsrTranscriptionOptions extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * The operation mode of the ASR session.
        
        * @wearableOnly
        */
        mode: AsrModule.AsrMode
        
        /**
        * An event triggered by transcription errors.
        
        * @readonly
        
        * @wearableOnly
        */
        onTranscriptionErrorEvent: event1<AsrModule.AsrStatusCode, void>
        
        /**
        * An event triggered by transcription updates.
        
        * @readonly
        
        * @wearableOnly
        */
        onTranscriptionUpdateEvent: event1<AsrModule.TranscriptionUpdateEvent, void>
        
        /**
        * Silence duration in milliseconds to detect the end of the speech.
        
        * @wearableOnly
        */
        silenceUntilTerminationMs: number
        
        /**
        * Creates a new instance.
        
        * @wearableOnly
        */
        static create(): AsrModule.AsrTranscriptionOptions
        
    }

}

declare namespace AsrModule {
    /**
    * Argument passed for a transcription update event.
    
    * @see Returned By: {@link AsrModule.TranscriptionUpdateEvent.create}
    
    * @wearableOnly
    
    * @example
    * ```ts
    * const onTranscriptionUpdate = (eventArgs: AsrModule.TranscriptionUpdateEvent) => {
    *     print(`onTranscriptionUpdateCallback text=${eventArgs.text}, isFinal=${eventArgs.isFinal}`)
    * }
    * ```
    */
    class TranscriptionUpdateEvent extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * Specifies whether the transcription returned is final, or partial (interim) which can be updated later as the sentence continues.
        
        * @wearableOnly
        */
        isFinal: boolean
        
        /**
        * Transcribed text.
        
        * @wearableOnly
        */
        text: string
        
        /**
        * Creates a new instance.
        
        * @wearableOnly
        */
        static create(): AsrModule.TranscriptionUpdateEvent
        
    }

}

/**
* Base class for all assets used in the engine. Assets can be unique to Lens Studio, such as {@link VFXAsset}, or a representation of an imported asset, such as {@link Texture} for jpg, png, and other image formats. In most cases, assets are added to the scene via a {@link Component}.

* @remarks
* For example, you might import an `.jpg` file into your project to be used in the Lens by dragging your file into the Asset Browser panel, which will create a {@link Texture} asset. Then, in the Scene Hierarchy panel, you can add a {@link SceneObject} containing the {@link Image} component (Scene Hierarchy panel > + > Image), and set the Texture field of the Image component to the newly imported asset in the Inspector panel.

* @see [Importing and Exporting](https://developers.snap.com/lens-studio/assets-pipeline/importing-and-exporting-resources) Guide.
* @see [Image](https://developers.snap.com/lens-studio/assets-pipeline/2d/image) Guide.

* @see Used By: {@link RemoteReferenceAsset#downloadAsset}

* @example
* ```js
* // @input Asset.Texture myTexture

* const currentObj = script.sceneObject;
* const imageComponent = currentObj.createComponent("Component.Image");
* const imageMaterial = imageComponent.mainPass;
* imageMaterial.baseTex = script.myTexture;
* ```

* ```ts
* @component
* export class AssetExample extends BaseScriptComponent {
*   @input()
*   myTexture: Texture;

*   onAwake() {
*     const currentObj = this.sceneObject
*     const imageComponent = currentObj.createComponent("Component.Image");
*     const imageMaterial = imageComponent.mainPass;
*     imageMaterial.baseTex = this.myTexture;
*   }
* }
* ```
*/
declare class Asset extends SerializableWithUID {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The name of the Asset in Lens Studio.
    */
    name: string
    
}

/**
* Used by {@link Head.setAttachmentPointType} to specify the type of attachment used with a Head binding.

* @see Used By: {@link Head#setAttachmentPointType}

* @example
* ```js
* //@input Component.Head head
* script.head.setAttachmentPointType(AttachmentPointType.HeadCenter);
* ```
*/
declare enum AttachmentPointType {
    CandideCenter,
    Chin,
    Forehead,
    HeadCenter,
    LeftCheek,
    LeftEyeballCenter,
    LeftForehead,
    MouthCenter,
    RightCheek,
    RightEyeballCenter,
    RightForehead,
    TriangleBarycentric
}

declare namespace Audio {
    /**
    * The curve that specifies how sound fades with the distance from Audio Component to the Audio Listener.****
    
    * @see Used By: {@link DistanceEffect#type}
    
    * @example
    * ```js
    * // @input Component.AudioComponent audio
    * var spatialAudio = script.audio.spatialAudio;
    * var distanceEffect = spatialAudio.distanceEffect;
    * distanceEffect.type = Audio.DistanceCurveType.Linear;
    * ```
    */
    enum DistanceCurveType {
        /**
        * Linear Curve ~ y(x) = ax+b
        */
        Linear,
        /**
        * Inverse Curve ~ y(x) = (a/x)+b
        */
        Inverse,
        /**
        * Logarithmic Curve ~ -log(x)
        */
        Logarithm,
        /**
        * Inverse Logarithmic Curve ~ +log(x)
        */
        InverseLogarithm
    }

}

declare namespace Audio {
    /**
    * The Playback Mode property of the `AudioComponent` used in Lenses targeting Spectacles. Spectacles default all Playback Modes to Low Power.
    
    * @see Used By: {@link AudioComponent#playbackMode}
    
    * @wearableOnly
    
    * @example
    * ```ts
    * @component
    * export class NewScript extends BaseScriptComponent {
    *   @input
    *   audio: AudioComponent;
    *   onAwake() {
    *     this.audio.playbackMode = Audio.PlaybackMode.LowLatency;
    *   }
    * }
    * ```
    */
    enum PlaybackMode {
        /**
        * Reduces power usage for the Spectacles device. but introduces latency in audio playback. Suitable for ambient sounds or background music where slight delays are acceptable.
        
        * @wearableOnly
        */
        LowPower,
        /**
        * Minimizes audio playback latency but increases power usage for the Spectacles device. Recommended for audio requiring immediate auditory reaction, such as button press feedback.
        
        * @wearableOnly
        */
        LowLatency
    }

}

/**
* Used to play audio in a Lens.

* @remarks
* You can assign an {@link AudioTrackAsset} to play through script, or through the Audio Component input in the Inspector panel of Lens Studio.

* @see [Playing Audio](https://developers.snap.com/lens-studio/features/audio/playing-audio) guide for more information.

* @see Used By: {@link AudioComponent#setOnFinish}

* @example
* ```js
* //@input Component.AudioComponent audio
* // Play once
* script.audio.play(1);
* ```
* ```js
* // Play forever
* script.audio.play(-1);
* ```
* ```js
* // If playing, stop
* if(script.audio.isPlaying())
* {
* 	script.audio.stop(true);
* }
* ```
* ```js
* // Set a callback for when the sound stops playing
* script.audio.setOnFinish(function()
* {
* 	print("sound finished playing");
* });
* ```
*/
declare class AudioComponent extends Component {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns whether the sound is currently paused.
    */
    isPaused(): boolean
    
    /**
    * Returns whether the AudioComponent is currently playing sound.
    */
    isPlaying(): boolean
    
    /**
    * Pauses the sound.
    */
    pause(): boolean
    
    /**
    * Plays the current sound `loops` number of times.  If `loops` is -1, the sound will repeat forever.
    */
    play(loops: number): void
    
    /**
    * Resumes a paused sound.
    */
    resume(): boolean
    
    /**
    * Sets the callback function to be called whenever this sound stops playing.
    */
    setOnFinish(eventCallback: (audioComponent: AudioComponent) => void): void
    
    /**
    * Stops the current sound if already playing.
    */
    stop(fade: boolean): void
    
    /**
    * The audio asset currently assigned to play.
    */
    audioTrack: AudioTrackAsset
    
    /**
    * The length (in seconds) of the current sound assigned to play.
    
    * @readonly
    */
    duration: number
    
    /**
    * Length (in seconds) of a volume fade in applied to the beginning of sound playback.
    */
    fadeInTime: number
    
    /**
    * Length (in seconds) of a volume fade out applied to the end of sound playback.
    */
    fadeOutTime: number
    
    /**
    * When true, records sound directly into the snap. This mode works only when all Audio Components in the scene are using mix to snap. In this case input from microphone will be ignored.
    */
    mixToSnap: boolean
    
    /**
    * How the audio should be played back. Useful in optimizing audio in Spectacles.
    
    * @wearableOnly
    */
    playbackMode: Audio.PlaybackMode
    
    /**
    * The current playback time in seconds
    */
    position: number
    
    /**
    * The volume of audio recorded to the snap, from 0 to 1.
    */
    recordingVolume: number
    
    /**
    * Spatial Audio settings.
    
    * @readonly
    */
    spatialAudio: SpatialAudio
    
    /**
    * A volume multiplier for any sounds played by this AudioComponent.
    */
    volume: number
    
}

/**
* Configures an audio effect for {@link AudioEffectComponent}.
*/
declare class AudioEffectAsset extends Asset {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Used to add effects to audio recorded by the device, such as Robot, Alien, etc.

* @remarks
* When present in the scene, it will automatically apply the selected audio effect to recordings made with the Lens.

* @see [Audio Effect](https://developers.snap.com/lens-studio/features/audio/audio-effect) Guide.

* @example
* ```js
* // Disable the audio effect
* // @input Component.AudioEffectComponent audioEffect
* script.audioEffect.enabled = false;
* ```
*/
declare class AudioEffectComponent extends Component {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Provider for {@link AudioEffectAsset}.
*/
declare class AudioEffectProvider extends Provider {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Processes input from {@link AudioComponent}s that use Spatial Audio.

* @remarks
* The Audio Listener component acts as a microphone-like device. It receives input from {@link AudioComponent}s that have Spatial Audio setting enabled and allows to calculate their relative positions to the scene object it is attached to and properly mix them.

* @see [Audio Listener](https://developers.snap.com/lens-studio/features/audio/audio-listener) guide
*/
declare class AudioListenerComponent extends Component {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Provider of the Audio Output Audio Track asset.

* @example
* ```js
* // @input Asset.AudioTrackAsset audioOutput
* audioOutput = script.audioOutput.control;
* audioOutput.sampleRate = script.sampleRate;
* ```
*/
declare class AudioOutputProvider extends AudioTrackProvider {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Enqueue audio data into an audio playback system.
    
    * `shape.x` - buffer size, must be less or equal to `audioFrame` length.
    */
    enqueueAudioFrame(audioFrame: Float32Array, inShape: vec3): void
    
    /**
    * Preferred size for audio output at the current frame update.
    */
    getPreferredFrameSize(): number
    
}

/**
* Represents an audio file asset.

* @see {@link AudioComponent}.
* @see [Audio Tracks](https://developers.snap.com/lens-studio/features/audio/audio-track-assets).

* @see Used By: {@link AudioComponent#audioTrack}, {@link RemoteMediaModule#loadResourceAsAudioTrackAsset}, {@link TextToSpeechModule#synthesize}

* @example
* ```js
* //@input Component.AudioComponent audio
* //@input Asset.AudioTrackAsset track

* script.audio.audioTrack = script.track;
* ```
*/
declare class AudioTrackAsset extends Asset {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The provider for this audio track asset.
    */
    control: AudioTrackProvider
    
}

/**
* Base class for Audio Track providers.

* @see Used By: {@link AudioTrackAsset#control}

* @example
* ```js
* // @input Asset.AudioTrackAsset audioTracks
* script.audioTrack.control.sampleRate = script.sampleRate;
* ```
*/
declare class AudioTrackProvider extends Provider {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The maximum frame size of the audio track asset.
    
    * @readonly
    */
    maxFrameSize: number
    
    /**
    * Sample rate (samples per second) of the audio track asset.
    */
    sampleRate: number
    
}

/**
* Cardinal axis enumeration.

* @see Used By: {@link CapsuleShape#axis}, {@link ConeShape#axis}, {@link CylinderShape#axis}
*/
declare enum Axis {
    /**
    * The X axis.
    */
    X,
    /**
    * The Y axis.
    */
    Y,
    /**
    * The Z axis.
    */
    Z
}

/**
* Settings for rendering the background on a {@link Text} component.
* Accessible through the {@link Text} component's `backgroundSettings` property.

* @see Used By: {@link Text#backgroundSettings}

* @example
* ```js
* //@input Component.Text textComponent

* // Access the background object for a Text component
* var bg = script.textComponent.backgroundSettings;

* // Enable the background
* bg.enabled = true;

* // Set the background color
* bg.fill.color = new vec4(1, 0, .5, 1);

* // Extend the background left and right by 1 unit
* bg.margins.left = 1.0;
* bg.margins.right = 1.0;
* ```
*/
declare class BackgroundSettings extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Controls how rounded the corner of the background should be.
    */
    cornerRadius: number
    
    /**
    * If enabled, the background will be rendered.
    */
    enabled: boolean
    
    /**
    * Settings for how the inside of the background is drawn.
    */
    fill: TextFill
    
    /**
    * Controls how far in each direction the background should extend away from the text.
    */
    margins: Rect
    
}

/**
* Handles encoding and decoding images and textures into Base64 format, commonly used to embed images in JSON or other text-based formats.

* @example
* ```js
* //@input Asset.Texture texture
* //@input Component.Image outputImage

* function printEncodedString(result) {
*     print("Encoded texture: " + result)
*     //decode the string back and display
*     decode(result).then(displayTexture).catch(printError)
* }

* function printError(error) {
*     print("Error: " + error)

* }

* function displayTexture(texture) {
*     print("Texture: " + texture)
*     if (script.outputImage) {
*         script.outputImage.mainMaterial.mainPass.baseTex = texture
*     }
* }

* function encode(texture) {
*     return new Promise(function (resolve, reject) {
*         Base64.encodeTextureAsync(texture, resolve, reject, CompressionQuality.LowQuality, EncodingType.Png)
*     })
* }

* function decode(encodedString) {
*     return new Promise(function (resolve, reject) {
*         Base64.decodeTextureAsync(encodedString, resolve, reject)
*     })
* }

* encode(script.texture).then(printEncodedString).catch(printError);
* ```
*/
declare class Base64 {
    
    /** @hidden */
    protected constructor()
    
    static decode(value: string): Uint8Array
    
    /**
    * Decode a texture from Base64, asynchronously.
    */
    static decodeTextureAsync(value: string, onSuccess: (decodedTexture: Texture) => void, onFailure: () => void): void
    
    static encode(data: Uint8Array): string
    
    /**
    * Encode a texture according to Base64 encoding algorithm, asynchronously.
    */
    static encodeTextureAsync(texture: Texture, onSuccess: (encodedTexture: string) => void, onFailure: () => void, compressionQuality: CompressionQuality, encodingType: EncodingType): void
    
}

/**
* Base for all mesh rendering components.

* @remarks
* Comparable to the former `MeshVisual` class, which was split into the classes:
* - {@link BaseMeshVisual}: serves as the foundational class for all visual components that use meshes for rendering.
* - {@link MaterialMeshVisual}: inherits from {@link BaseMeshVisual} and provides access to the {@link Material} used in the rendering process.
* - {@link RenderMeshVisual}: extends {@link MaterialMeshVisual}, adding the capability to utilize specific {@link RenderMesh} assets to depict 3D models within a scene.

* @see Used By: {@link InteractionComponent#addMeshVisual}, {@link InteractionComponent#removeMeshVisual}, {@link PinToMeshComponent#target}
*/
declare class BaseMeshVisual extends Visual {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Range maximum of the local-space axis-aligned bounding box (AABB) of the visual.
    */
    localAabbMax(): vec3
    
    /**
    * Range minimum of the local-space axis-aligned bounding box (AABB) of the visual.
    */
    localAabbMin(): vec3
    
    /**
    * Projects screen positions from `camera`'s view onto the mesh's UVs.
    * If the MeshVisual's material uses the same texture as the camera input, the MeshVisual will look identical to the part of the screen it covers.
    */
    snap(camera: Camera): void
    
    /**
    * Range maximum of the world-space axis-aligned bounding box (AABB) of the visual.
    */
    worldAabbMax(): vec3
    
    /**
    * Range minimum of the world-space axis-aligned bounding box (AABB) of the visual.
    */
    worldAabbMin(): vec3
    
    /**
    * When a {@link ScreenTransform} is present on this SceneObject,
    * and `extentsTarget` is a child of this SceneObject, `extentsTarget` will be repositioned to match the exact
    * area this MeshVisual is being rendered. Very useful for {@link Image} and {@link Text} components.
    */
    extentsTarget: ScreenTransform
    
    /**
    * When a {@link ScreenTransform} is attached to the same SceneObject, this controls how the mesh will be positioned horizontally depending on `stretchMode`.
    */
    horizontalAlignment: HorizontalAlignment
    
    /**
    * None = 0, Caster = 1, Receiver = 2
    */
    meshShadowMode: MeshShadowMode
    
    /**
    * Affects the color of shadows being cast by this MeshVisual. The color of the cast shadow is a mix between shadowColor and the material's base texture color. The alpha value of shadowColor controls the mixing of these two colors, with 0 = shadowColor and 1 = shadowColor * textureColor.
    */
    shadowColor: vec4
    
    /**
    * Density of shadows cast by this MeshVisual.
    */
    shadowDensity: number
    
    /**
    * When a {@link ScreenTransform} is attached to the same SceneObject, this controls how the mesh will be stretched relative to the ScreenTransform's boundaries.
    */
    stretchMode: StretchMode
    
    /**
    * When a {@link ScreenTransform} is attached to the same SceneObject, this controls how the mesh will be positioned vertically depending on `stretchMode`.
    */
    verticalAlignment: VerticalAlignment
    
}

/**
* Base class for MultiplayerSession options. This class is not used directly - use ConnectedLensSessionOptions instead.
*/
declare class BaseMultiplayerSessionOptions extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Provides a single host for every session. Useful when an experience has a single authority. This should not be used in latency sensitive situations. The creator of the session will by default be the host. If the host leaves, thee server will determine a new host and transfer all ownership of entities owned by the original host. Use with `onHostUpdated` API to get a callback.
    */
    hostManagementEnabled: boolean
    
    /**
    * Function to be called when a connection to the realtime backend is established. All realtime requests can be called after this callback. When you invite others to join, a new session will be created, hence you should update your session handler with the argument passed in this callback.
    */
    onConnected: (session: MultiplayerSession, connectionInfo: ConnectedLensModule.ConnectionInfo) => void
    
    /**
    * Function to be called when the connection to the realtime backend is lost, either via successful disconnect or passive disconnect due to error.
    */
    onDisconnected: (session: MultiplayerSession, disconnectInfo: string) => void
    
    /**
    * Function to be called when an error occurs in the session life cycle.
    */
    onError: (session: MultiplayerSession, code: string, description: string) => void
    
    onHostUpdated: (session: MultiplayerSession, removalInfo: ConnectedLensModule.HostUpdateInfo) => void
    
    /**
    * Function to be called when a string-based message sent by sendMessage() is received from another user via the realtime backend.
    */
    onMessageReceived: (session: MultiplayerSession, userId: string, message: string, senderInfo: ConnectedLensModule.UserInfo) => void
    
    /**
    * Callback function that will be executed when a realtime store is created.
    */
    onRealtimeStoreCreated: (session: MultiplayerSession, store: GeneralDataStore, ownerInfo: ConnectedLensModule.UserInfo, creationInfo: ConnectedLensModule.RealtimeStoreCreationInfo) => void
    
    /**
    * Callback function that will be executed when a realtime store is deleted.
    */
    onRealtimeStoreDeleted: (session: MultiplayerSession, store: GeneralDataStore, deleteInfo: ConnectedLensModule.RealtimeStoreDeleteInfo) => void
    
    /**
    * Function to be called when a key is removed from a RealtimeStore.
    */
    onRealtimeStoreKeyRemoved: (session: MultiplayerSession, removalInfo: ConnectedLensModule.RealtimeStoreKeyRemovalInfo) => void
    
    /**
    * Callback function that will be executed when ownership of a realtime store is updated.
    */
    onRealtimeStoreOwnershipUpdated: (session: MultiplayerSession, store: GeneralDataStore, ownerInfo: ConnectedLensModule.UserInfo, ownershipUpdateInfo: ConnectedLensModule.RealtimeStoreOwnershipUpdateInfo) => void
    
    /**
    * Callback function that will be executed when a realtime store is updated.
    */
    onRealtimeStoreUpdated: (session: MultiplayerSession, store: GeneralDataStore, key: string, updateInfo: ConnectedLensModule.RealtimeStoreUpdateInfo) => void
    
    /**
    * Function to be called when another user joins the session. When joining a session, the current user will get a callback for each of the existing active users in the current session. This way you can build a list of existing players in game.
    */
    onUserJoinedSession: (session: MultiplayerSession, userInfo: ConnectedLensModule.UserInfo) => void
    
    /**
    * Function to be called when another user leaves the session, either deliberately or via passive disconnect due to error.
    */
    onUserLeftSession: (session: MultiplayerSession, userInfo: ConnectedLensModule.UserInfo) => void
    
}

/**
* Base class for Input and Output Placeholders used by MLComponent.
*/
declare class BasePlaceholder extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The data layout of the current BasePlaceholder, which defines the order in which multidimensional data is accessed in memory. The default layout is {@link MachineLearning.DataLayout.NHWC}.
    */
    dataLayout: MachineLearning.DataLayout
    
    /**
    * The internal data layout of the current BasePlaceholder. This layout is used by ML backend. If `dataLayout` is not the same as `internalDataLayout`, a layout conversion will happen when process models input/output.
    
    * @readonly
    */
    internalDataLayout: MachineLearning.DataLayout
    
    /**
    * The name of the Placeholder.
    
    * @readonly
    */
    name: string
    
    /**
    * The shape of the Placeholder's data.
    
    * @readonly
    */
    shape: vec3
    
    /**
    * Transformer object for applying transformations on the PlaceHolder's data.
    
    * @readonly
    */
    transformer: Transformer
    
}

/**
* Provides basic information about a transformation.
* See also: {@link DeviceTracking}
*/
declare class BasicTransform extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns the inverted world matrix of the BasicTransform.
    */
    getInvertedMatrix(): mat4
    
    /**
    * Returns the world matrix of the BasicTransform.
    */
    getMatrix(): mat4
    
    /**
    * Returns the world position of the BasicTransform.
    */
    getPosition(): vec3
    
    /**
    * Returns the world rotation of the BasicTransform.
    */
    getRotation(): quat
    
    /**
    * Returns the world scale of the BasicTransform.
    */
    getScale(): vec3
    
}

/**
* A file based asset.
*/
declare class BinAsset extends Asset {
    
    /** @hidden */
    protected constructor()
    
}

/**
* The options used with `requestBitmoji2DResource`.

* @see Used By: {@link BitmojiModule#requestBitmoji2DResource}
* @see Returned By: {@link Bitmoji2DOptions.create}
*/
declare class Bitmoji2DOptions extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The pose id for the 2D Bitmoji.
    */
    poseId: string
    
    /**
    * The user which the Bitmoji should represent.
    */
    user: SnapchatUser
    
    /**
    * Create the option.
    */
    static create(): Bitmoji2DOptions
    
}

/**
* The `DynamicResource` of a 2D Bitmoji which can be loaded with `RemoteMediaModule`.

* @see Used By: {@link BitmojiModule#requestBitmoji2DResource}
*/
declare class Bitmoji2DResource extends DynamicResource {
    
    /** @hidden */
    protected constructor()
    
}

/**
* The options used with `requestBitmoji3DResource`

* @see Used By: {@link BitmojiModule#requestBitmoji3DResourceWithOptions}
* @see Returned By: {@link Bitmoji3DOptions.create}
*/
declare class Bitmoji3DOptions extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Used to set the animation params for the request type.
    */
    animationParams: Bitmoji3DOptions.AnimationParams
    
    /**
    * Used to set the custom params for the request type. You shouldn't need to modify these values directly. Instead you can use this API via the [Bitmoji Suite](https://developers.snap.com/lens-studio/features/bitmoji-suite/overview).
    */
    customParams: Bitmoji3DOptions.CustomParams
    
    /**
    * Options used with {@link requestBitmoji3DResourceWithOptions}
    */
    requestType: Bitmoji3DOptions.RequestType
    
    /**
    * The user which the Bitmoji should represent.
    */
    user: SnapchatUser
    
    /**
    * Create the option.
    */
    static create(): Bitmoji3DOptions
    
}

declare namespace Bitmoji3DOptions {
    /**
    * Used to set the animation params for the request type.
    
    * @see Used By: {@link Bitmoji3DOptions#animationParams}
    * @see Returned By: {@link Bitmoji3DOptions.AnimationParams.create}
    */
    class AnimationParams extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * Creates the animation params.
        */
        static create(): Bitmoji3DOptions.AnimationParams
        
    }

}

declare namespace Bitmoji3DOptions {
    /**
    * Enum representing avatar scope used in custom params of bitmoji3d options.
    
    * @see Used By: {@link Bitmoji3DOptions.CustomParams#avatarScope}
    */
    enum AvatarScope {
        /**
        * Unset scope.
        */
        Unset,
        /**
        * Full scope.
        */
        Full,
        /**
        * Head scope.
        */
        Head,
        /**
        * Body scope.
        */
        Body,
        /**
        * Hair scope.
        */
        Hair,
        /**
        * Glasses scope.
        */
        Glasses,
        /**
        * Hat hair scope.
        */
        Hathair,
        /**
        * Piercing scope.
        */
        Piercing,
        /**
        * Clothes scope.
        */
        Clothes,
        /**
        * Mannequin Head scope.
        */
        MannequinHead,
        /**
        * MannequinHeadFeatureless scope.
        */
        MannequinHeadFeatureless,
        /**
        * MannequinEarLeft scope.
        */
        MannequinEarLeft,
        /**
        * MannequinEarRight scope.
        */
        MannequinEarRight,
        /**
        * MannequinHeadFeaturelessWithBody scope.
        */
        MannequinHeadFeaturelessWithBody,
        /**
        * .GlassesWithSkeleton scope.
        */
        GlassesWithSkeleton
    }

}

declare namespace Bitmoji3DOptions {
    /**
    * Enum representing different types of clothing
    
    * @see Used By: {@link Bitmoji3DOptions.CustomParams#clothingType}
    */
    enum ClothingType {
        /**
        * Unset clothing type.
        */
        Unset,
        /**
        * Split clothing type.
        */
        SplitClothes,
        /**
        * Granular clothing type.
        */
        GranularClothes,
        /**
        * Full outfit type.
        */
        FullOutfit,
        /**
        * Detailed full outfit type.
        */
        DetailedFullOutfit
    }

}

declare namespace Bitmoji3DOptions {
    /**
    * The custom params can be used for requesting bitmoji assets like garments. These can be set as custom params of Bitmoji3D options.
    
    * @see Used By: {@link Bitmoji3DOptions#customParams}
    * @see Returned By: {@link Bitmoji3DOptions.CustomParams.create}
    */
    class CustomParams extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * The {@link Bitmoji3DOptions.AvatarScope} to use for the request.
        */
        avatarScope: Bitmoji3DOptions.AvatarScope
        
        /**
        * Parameter set to customize the bag of the avatar.
        */
        bag?: Bitmoji3DOptions.ParamSet
        
        /**
        * Parameter set to customize the bottom wear of the avatar.
        */
        bottomWear?: Bitmoji3DOptions.ParamSet
        
        /**
        * Parameter set to customize the clothing tyoe of the avatar.
        */
        clothingType: Bitmoji3DOptions.ClothingType
        
        /**
        * Parameter set to customize the cranium of the avatar.
        */
        cranium?: string
        
        /**
        * Parameter set to customize the foot wear of the avatar.
        */
        footWear?: Bitmoji3DOptions.ParamSet
        
        /**
        * Parameter set to customize the glasses of the avatar.
        */
        glasses?: string
        
        /**
        * Parameter set to customize the hat of the avatar.
        */
        hat?: Bitmoji3DOptions.ParamSet
        
        /**
        * Parameter set to customize the outerwear of the avatar.
        */
        outerWear?: Bitmoji3DOptions.ParamSet
        
        /**
        * Parameter set to customize the sock of the avatar.
        */
        sock?: Bitmoji3DOptions.ParamSet
        
        /**
        * Parameter set to customize the top of the avatar.
        */
        top?: Bitmoji3DOptions.ParamSet
        
        /**
        * The rendering options can be used for customizing bitmoj3D avatars. These can be set as overridden options of Bitmoji3D options.
        */
        static create(): Bitmoji3DOptions.CustomParams
        
    }

}

declare namespace Bitmoji3DOptions {
    /**
    * Set of parameters used to customize specific parts of avatar such as garment, outerwear etc.
    
    * @see Used By: {@link Bitmoji3DOptions.CustomParams#bag}, {@link Bitmoji3DOptions.CustomParams#bottomWear}, {@link Bitmoji3DOptions.CustomParams#footWear}, {@link Bitmoji3DOptions.CustomParams#hat}, {@link Bitmoji3DOptions.CustomParams#outerWear}, {@link Bitmoji3DOptions.CustomParams#sock}, {@link Bitmoji3DOptions.CustomParams#top}
    * @see Returned By: {@link Bitmoji3DOptions.ParamSet.create}
    */
    class ParamSet extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * Unique identifier of the category defined using this ParamSet.
        */
        optionId: string
        
        /**
        * Used to set the parameters.
        */
        params: {[key:string]:(string|number|vec4)}
        
        /**
        * Used to create the parameter set for customizing specific parts of avatar such as garment, outerwear etc.
        */
        static create(): Bitmoji3DOptions.ParamSet
        
    }

}

declare namespace Bitmoji3DOptions {
    /**
    * Enum representing different request types.
    
    * @see Used By: {@link Bitmoji3DOptions#requestType}
    */
    enum RequestType {
        /**
        * Used to request avatar.
        */
        Avatar,
        Animation,
        /**
        * Used to request custom asset of avatar
        */
        Custom
    }

}

/**
* Provides information about the current user's 3D Bitmoji avatar to be downloaded via the RemoteMediaModule.

* @see Used By: {@link BitmojiModule#requestBitmoji3DResource}, {@link BitmojiModule#requestBitmoji3DResourceWithOptions}

* @example
* ```
* // @input Asset.BitmojiModule bitmojiModule
* // @input Asset.RemoteMediaModule remoteMediaModule
* // @input Asset.Material pbrMaterialHolder

* script.bitmojiModule.requestBitmoji3DResource(
*     function (bitmoji3DResource) {
*       script.remoteMediaModule.loadResourceAsGltfAsset(
*         bitmoji3DResource,
*         onDownloaded,
*         onFail
*       )
*     }
*   )

* function onDownloaded (gltfAsset){
*   var root = scene.createSceneObject("BitmojiAvatar");
*   var gltfSettings = GltfSettings.create();
*   gltfSettings.convertMetersToCentimeters = true;
*   var avatar = gltfAsset.tryInstantiateWithSetting(root, script.pbrMaterialHolder, gltfSettings);
* }

* function onFail (e){
*     print(e);
* }
* ```
*/
declare class Bitmoji3DResource extends DynamicResource {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Provides access to functionalities related to [Bitmoji](https://developers.snap.com/lens-studio/features/bitmoji-avatar/overview) avatar.

* @see [Bitmoji Overview](https://developers.snap.com/lens-studio/features/bitmoji-avatar/overview) guide.

* @example
* ```
* // @input Asset.BitmojiModule bitmojiModule
* // @input Asset.RemoteMediaModule remoteMediaModule
* // @input Asset.Material pbrMaterialHolder

* script.bitmojiModule.requestBitmoji3DResource(
*     function (bitmoji3DResource) {
*       script.remoteMediaModule.loadResourceAsGltfAsset(
*         bitmoji3DResource,
*         onDownloaded,
*         onFail
*       )
*     }
*   )

* function onDownloaded (gltfAsset){
*   var root = scene.createSceneObject("BitmojiAvatar");
*   var gltfSettings = GltfSettings.create();
*   gltfSettings.convertMetersToCentimeters = true;
*   var avatar = gltfAsset.tryInstantiateWithSetting(root, script.pbrMaterialHolder, gltfSettings);
* }

* function onFail (e){
*     print(e);
* }
* ```
*/
declare class BitmojiModule extends Asset {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns a `DynamicResource` via the provided `callback`, which can be resolved into a texture using `RemoteMediaModule`.
    */
    requestBitmoji2DResource(options: Bitmoji2DOptions, callback: (resource: Bitmoji2DResource) => void): void
    
    /**
    * Request the current user's 3D Bitmoji avatar.
    */
    requestBitmoji3DResource(callback: (resource: Bitmoji3DResource) => void): void
    
    /**
    * Request the Bitmoji3D resource.
    */
    requestBitmoji3DResourceWithOptions(options: Bitmoji3DOptions, callback: (resource: Bitmoji3DResource) => void): void
    
}

/**


* @see Used By: {@link Pass#blendMode}, {@link PassWrapper#blendMode}, {@link Text#blendMode}
*/
declare enum BlendMode {
    Normal,
    /**
    * @deprecated
    */
    MultiplyLegacy,
    /**
    * @deprecated
    */
    AddLegacy,
    Screen,
    PremultipliedAlpha,
    AlphaToCoverage,
    Disabled,
    Add,
    AlphaTest,
    ColoredGlass,
    Multiply,
    Min,
    Max,
    PremultipliedAlphaHardware,
    PremultipliedAlphaAuto
}

/**
* Controls blend shapes connected to imported animation content.

* Note: this class has been deprecated. Please use the blend shapes functions in {@link RenderMeshVisual}.

* @deprecated

* @example
* ```
* // Sets the weight of a blend shape by ID
* //@input Component.BlendShapes blendShape
* //@input string blendID
* //@input float blendWeight

* script.blendShape.setBlendShape(script.blendID, script.blendWeight);
* ```
*/
declare class BlendShapes extends Component {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Removes all blend shapes from the BlendShapesVisual.
    
    * @deprecated
    */
    clearBlendShapes(): void
    
    /**
    * Returns the weight of blend shape `name`.
    
    * @deprecated
    */
    getBlendShape(name: string): number
    
    /**
    * Returns whether this BlendShapesVisual has a blend shape named `name`.
    
    * @deprecated
    */
    hasBlendShape(name: string): boolean
    
    /**
    * Sets the weight of blend shape `name`.
    
    * @deprecated
    */
    setBlendShape(name: string, weight: number): void
    
    /**
    * Clears the blendshape with the matching name from the BlendShapes component.
    
    * @deprecated
    */
    unsetBlendShape(name: string): void
    
    /**
    * If enabled, normal directions are also blended by blend shapes.
    
    * @deprecated
    */
    blendNormals: boolean
    
}

/**
* Represents a blob, which is a file-like object of immutable, raw data. Can be read as text or binary data. Currently the binary data is only supported as `Uint8Array`.

* @wearableOnly

* @CameraKit

* @example
* ```
* socket.onmessage = async (event) => {
*     if (event.data instanceof Blob) {
*         // Binary frame, can be retrieved as either Uint8Array or string
*         let bytes = await event.data.bytes();
*         let text = await event.data.text();

*         print("Received binary message, printing as text: " + text);
*     }
* });
* ```
*/
declare class Blob extends ScriptObject {
    /**
    * Construct a new Blob. Currently unused.
    
    * @wearableOnly
    
    * @CameraKit
    */
    constructor()
    
    /**
    * Returns a Promise that resolves with a `Uint8Array` containing the contents of the blob as an array of bytes.
    
    * @wearableOnly
    
    * @CameraKit
    */
    bytes(): Promise<Uint8Array>
    
    /**
    * Returns a Promise that resolves with a string containing the contents of the blob, interpreted as UTF-8.
    
    * @wearableOnly
    
    * @CameraKit
    */
    text(): Promise<string>
    
    /**
    * The size of the blob's data in bytes.
    
    * @readonly
    
    * @wearableOnly
    
    * @CameraKit
    */
    size: number
    
}

declare namespace Bluetooth {
    /**
    * Provides access to Bluetooth GATT devices. APIs include scanning for and connecting to these devices, and reading and writing to their descriptors and characteristics.
    
    * @example
    * ```js
    * // @input Asset.BluetoothCentralModule bluetoothModule
    
    * var bluetoothGatt;
    * async function scanForGatt() {
    *     var scanFilter = new Bluetooth.ScanFilter();
    *     var scanSetting = new Bluetooth.ScanSettings();
    *     scanSetting.timeoutSeconds = 30;
    *     scanSetting.scanMode = Bluetooth.ScanMode.LowPower;
    
    *     var scanResult = await script.bluetoothModule.startScan([scanFilter], scanSetting,
    *         function(result) {
    *             print("Running predicate on " + result.deviceName);
    *             return result.deviceName == "[DEVICE NAME]";
    *         }
    *     );
    
    *     print("Found GATT device " + scanResult.deviceName);
    
    *     bluetoothGatt = await script.bluetoothModule.connectGatt(scanResult.deviceAddress);
    *     sprint("Connected to GATT device");
    
    *     bluetoothGatt.onConnectionStateChangedEvent.add(function(event) {
    *         print("Connection state changed to " + event.state);
    *     });
    *     bluetoothGatt.onMtuChangedEvent.add(function(event) {
    *         print("Mtu changed to " + event.mtu);
    *     });
    
    
    *     let services = bluetoothGatt.getServices()
    *     if (services.length == 0) {
    *         print("No services found");
    *         return;
    *     }
    *     print("Services found [" + services.length + "]");
    
    *     let service = services[0];
    *     let characteristics = service.getCharacteristics();
    *     if (characteristics.length == 0) {
    *         print('No characteristics found');
    *         return;
    *     }
    *     print("Characteristics found [" + characteristics.length + "]");
    
    *     let characteristic = characteristics[0];
    *     let value = await characteristic.readValue();
    *     print("Read value [" + value + "]");
    * }
    
    * async function run() {
    *     scanForGatt();
    *     return "";
    * }
    
    * await run();
    * ```
    */
    class BluetoothCentralModule extends Asset {
        
        /** @hidden */
        protected constructor()
        
        /**
        * Connect to a GATT server on a given device address.
        
        * `deviceAddress` Address to which to connect. Received in {@link Bluetooth.ScanResult.deviceAddress}
        
        * __Returns:__ Promise resolving to a {Bluetooth.BluetoothGatt} object if successful. The Promise is rejected if the connection cannot be made.
        
        * @experimental
        
        * @wearableOnly
        */
        connectGatt(deviceAddress: Uint8Array): Promise<Bluetooth.BluetoothGatt>
        
        /**
        * Start a scan for Bluetooth GATT devices. The first device which passes the predicate will be returned, at which point the scan will stop.
        
        * `filters` Filters to apply to the scan. If a device passes any filter then the predicate will be invoked for that device. If no filters are passed then the predicate will be invoked for all devices.
        
        * `settings` {@link Bluetooth.ScanSettings} to configure the scan.
        
        * `predicate` Predicate to select a device. Returning true will stop the scan and return the device.
        
        * __Returns:__ Promise resolving to the first device which passes the predicate. The promise is rejected if the scan times out.
        
        * ```javascript
        * var scanFilter = new Bluetooth.ScanFilter();
        * var scanSettings = new Bluetooth.ScanSettings();
        * scanSettings.timeoutSeconds = 30;
        * scanSettings.scanMode = Bluetooth.ScanMode.LowPower;
        
        * var scanResult = await script.bluetoothModule.startScan([scanFilter], scanSettings,
        *     function(result) {
        *         print("Running predicate on " + result.deviceName);
        *         return result.deviceName == "[DEVICE NAME]";
        *     }
        * );
        
        * print("Found GATT device " + scanResult.deviceName);
        * ```
        
        * @experimental
        
        * @wearableOnly
        */
        startScan(filters: Bluetooth.ScanFilter[], settings: Bluetooth.ScanSettings, predicate: (result: Bluetooth.ScanResult) => any): Promise<Bluetooth.ScanResult>
        
        /**
        * Stop a scan for Bluetooth devices, if one is running.
        
        * @experimental
        
        * @wearableOnly
        */
        stopScan(): Promise<void>
        
        /**
        * Event for Bluetooth status changes. _Currently unused_.
        
        * @readonly
        
        * @experimental
        
        * @wearableOnly
        */
        onBluetoothStatusChangedEvent: event1<Bluetooth.BluetoothStatusChangedEvent, void>
        
        /**
        * Get the current status of the Bluetooth adapter.
        
        * @readonly
        
        * @experimental
        
        * @wearableOnly
        */
        status: Bluetooth.BluetoothStatus
        
    }

}

declare namespace Bluetooth {
    /**
    * Provides access to the Bluetooth connection.
    */
    class BluetoothGatt extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * Terminate the Bluetooth client connection, disconnection included if applicable.
        
        * @experimental
        
        * @wearableOnly
        */
        close(): void
        
        /**
        * Re-establish connection to the device. Asynchronous call: to detect when the device is connected listen on the {@link Bluetooth.BluetoothGatt.onConnectionStateChangedEvent}.
        
        * @experimental
        
        * @wearableOnly
        */
        connect(): void
        
        /**
        * Unpair or disconnect from the device. Asynchronous call: to detect when the device is disconnected listen on the {@link Bluetooth.BluetoothGatt.onConnectionStateChangedEvent}.
        
        * @experimental
        
        * @wearableOnly
        */
        disconnect(): void
        
        /**
        * Retrieve a specific {@link Bluetooth.BluetoothGattService} using its UUID from the GATT server.
        
        * @experimental
        
        * @wearableOnly
        */
        getService(serviceUUID: string): Bluetooth.BluetoothGattService
        
        /**
        * Retrieve all available services from the GATT server.
        
        * @experimental
        
        * @wearableOnly
        */
        getServices(): Bluetooth.BluetoothGattService[]
        
        /**
        * The state of the GATT connection represented as a {@link Bluetooth.ConnectionState} enum.
        
        * @readonly
        
        * @experimental
        
        * @wearableOnly
        */
        connectionState: Bluetooth.ConnectionState
        
        /**
        * The current Maximum Transmission Unit (MTU).
        
        * @readonly
        
        * @experimental
        
        * @wearableOnly
        */
        mtu: number
        
        /**
        * Event to observe changes in connection state.
        
        * @readonly
        
        * @experimental
        
        * @wearableOnly
        */
        onConnectionStateChangedEvent: event1<Bluetooth.ConnectionStateChangedEvent, void>
        
        /**
        * Event to observe changes in Maximum Transmission Unit (MTU).
        
        * @readonly
        
        * @experimental
        
        * @wearableOnly
        */
        onMtuChangedEvent: event1<Bluetooth.MtuChangedEvent, void>
        
    }

}

declare namespace Bluetooth {
    /**
    * A characteristic of the Bluetooth GATT device.
    
    * Characteristics are the individual pieces of data within a {@link Bluetooth.BluetoothGattService} that provide specific information. For example, a heart rate sensor might have a Heart Rate service, and that service might have characteristics including "Heart Rate Measurement," "Sensor Location," etc.
    
    * __Value__ ({@link Bluetooth.BluetoothGattCharacteristic.readValue}, {@link Bluetooth.BluetoothGattCharacteristic.writeValue}): The actual data being exchanged.
    
    * __Properties__ ({@link Bluetooth.BluetoothGattCharacteristic.properties}): Define how the characteristic can be accessed (e.g., read, write, notify, indicate).
    
    * __Descriptors__ ({{@link Bluetooth.BluetoothGattCharacteristic.getDescriptor}, {@link Bluetooth.BluetoothGattCharacteristic.getDescriptors}): Provide additional information about the characteristic, such as its units or format.
    
    * __UUID__ ({@link Bluetooth.BluetoothGattCharacteristic.uuid}): A unique identifier that identifies the characteristic.
    
    * Common characteristics include battery level, heart rate, temperature, and device name.
    
    * @see Returned By: {@link Bluetooth.BluetoothGattService#getCharacteristic}
    */
    class BluetoothGattCharacteristic extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * Retrieve a specific {@link Bluetooth.BluetoothGattDescriptor} by its UUID from this characteristic.
        
        * @experimental
        
        * @wearableOnly
        */
        getDescriptor(descriptorUUID: string): Bluetooth.BluetoothGattDescriptor
        
        /**
        * Retrieve all descriptors associated with this characteristic.
        
        * @experimental
        
        * @wearableOnly
        */
        getDescriptors(): Bluetooth.BluetoothGattDescriptor[]
        
        /**
        * Read and receive the current value of the characteristic.
        
        * __Returns:__ The value in a `Uint8Array`
        
        * @experimental
        
        * @wearableOnly
        */
        readValue(): Promise<Uint8Array>
        
        /**
        * Register the given callback to receive notifications. Registering a callback will unsubscribe any previous registration on this Characteristic.
        
        * `callback` The callback to register, which takes a single parameter `value` of type `Uint8Array`.
        
        * ```javascript
        * try {
        *     const value = await characteristic.registerNotifications();
        *     console.log("Received Uint8Array data: ", value);
        * } catch (error) {
        *     console.error("Failed to register notifications: ", error);
        * }
        * ```
        
        * __Returns:__ Promise resolving to void when the callback is successfully registered.
        
        * @experimental
        
        * @wearableOnly
        */
        registerNotifications(callback: (value: Uint8Array) => void): Promise<void>
        
        /**
        * Unregister all notifications.
        
        * __Returns:__ Promise resolving to void when all notifications are successfully unregistered.
        
        * @experimental
        
        * @wearableOnly
        */
        unregisterNotifications(): Promise<void>
        
        /**
        * Write a new value to the characteristic.
        
        * `value` The value in a `Uint8Array`
        
        * __Returns:__ Promise resolving to void when the value is successfully written.
        
        * @experimental
        
        * @wearableOnly
        */
        writeValue(value: Uint8Array): Promise<void>
        
        /**
        * Write a new value to the characteristic without awaiting acknowledgment.
        
        * `value` The value in a `Uint8Array`
        
        * @experimental
        
        * @wearableOnly
        */
        writeValueWithoutResponse(value: Uint8Array): Promise<void>
        
        /**
        * The supported properties (e.g., read, write) of the characteristic, as an Array of {@link Bluetooth.CharacteristicProperty} values.
        
        * @readonly
        
        * @experimental
        
        * @wearableOnly
        */
        properties: Bluetooth.CharacteristicProperty[]
        
        /**
        * The unique identifier of this characteristic.
        
        * @readonly
        
        * @experimental
        
        * @wearableOnly
        */
        uuid: string
        
    }

}

declare namespace Bluetooth {
    /**
    * Descriptors contain additional information and attributes of a {@link Bluetooth.BluetoothGattCharacteristic}.
    
    * @see Returned By: {@link Bluetooth.BluetoothGattCharacteristic#getDescriptor}
    */
    class BluetoothGattDescriptor extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * Read and receive the current value of the descriptor.
        
        * __Returns:__ The value in a `Uint8Array`
        
        * @experimental
        
        * @wearableOnly
        */
        readValue(): Promise<Uint8Array>
        
        /**
        * Write a new value to the descriptor.
        
        * `value` The value in a `Uint8Array`
        
        * __Returns:__ Promise resolving to void when the value is successfully written.
        
        * @experimental
        
        * @wearableOnly
        */
        writeValue(value: Uint8Array): Promise<void>
        
        /**
        * The UUID of the current descriptor.
        
        * @readonly
        
        * @experimental
        
        * @wearableOnly
        */
        uuid: string
        
    }

}

declare namespace Bluetooth {
    /**
    * Bluetooth GATT Services are collections of related {@link Bluetooth.BluetoothGattCharacteristic}s representing specific functions or features of a device. For example, a heart rate sensor might have a "Heart Rate" service, which in turn has characteristics like  "Heart Rate Measurement".
    
    * @see Returned By: {@link Bluetooth.BluetoothGatt#getService}
    */
    class BluetoothGattService extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * Retrieve a {@link Bluetooth.BluetoothGattCharacteristic} from this service using its UUID.
        
        * @experimental
        
        * @wearableOnly
        */
        getCharacteristic(characteristicUUID: string): Bluetooth.BluetoothGattCharacteristic
        
        /**
        * Retrieve all {@link Bluetooth.BluetoothGattCharacteristic}s offered by this service.
        
        * @experimental
        
        * @wearableOnly
        */
        getCharacteristics(): Bluetooth.BluetoothGattCharacteristic[]
        
        /**
        * The UUID of the GATT service.
        
        * @readonly
        
        * @experimental
        
        * @wearableOnly
        */
        uuid: string
        
    }

}

declare namespace Bluetooth {
    /**
    * The statuses for Bluetooth availability. Currently unsupported.
    
    * @see Used By: {@link Bluetooth.BluetoothCentralModule#status}, {@link Bluetooth.BluetoothStatusChangedEvent#status}
    
    * @experimental
    
    * @wearableOnly
    */
    enum BluetoothStatus {
        /**
        * Status indicating that the Bluetooth permissions or status have not been established.
        
        * @experimental
        
        * @wearableOnly
        */
        Unknown,
        /**
        * Status indicating that user has denied Bluetooth permissions.
        
        * @experimental
        
        * @wearableOnly
        */
        PermissionDenied,
        /**
        * Status indicating that Bluetooth functionality is not available.
        
        * @experimental
        
        * @wearableOnly
        */
        Unavailable,
        /**
        * Status indicating that Bluetooth is ready for use.
        
        * @experimental
        
        * @wearableOnly
        */
        Available
    }

}

declare namespace Bluetooth {
    /**
    * The event received when Bluetooth status has changed. Currently unsupported.
    
    * @experimental
    
    * @wearableOnly
    */
    class BluetoothStatusChangedEvent extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * The new Bluetooth status.
        
        * @readonly
        
        * @experimental
        
        * @wearableOnly
        */
        status: Bluetooth.BluetoothStatus
        
    }

}

declare namespace Bluetooth {
    /**
    * Properties of a {@link Bluetooth.BluetoothGattCharacteristic}.
    
    * @experimental
    
    * @wearableOnly
    */
    enum CharacteristicProperty {
        /**
        * Property indicating that the characteristic supports broadcasting.
        
        * @experimental
        
        * @wearableOnly
        */
        Broadcast,
        /**
        * Read and receive the current value of the characteristic.
        
        * @experimental
        
        * @wearableOnly
        */
        Read,
        /**
        * Write a new value to the characteristic without awaiting acknowledgment.
        
        * @experimental
        
        * @wearableOnly
        */
        WriteWithoutResponse,
        /**
        * Write a new value to the characteristic and await acknowledgment.
        
        * @experimental
        
        * @wearableOnly
        */
        Write,
        /**
        * Property indicating characteristic supports notifications.
        
        * @experimental
        
        * @wearableOnly
        */
        Notify,
        /**
        * Property indicating characteristic can send indications
        
        * @experimental
        
        * @wearableOnly
        */
        Indicate,
        /**
        * Property indicating characteristic supports signed write operations.
        
        * @experimental
        
        * @wearableOnly
        */
        SignedWrite,
        /**
        * Property indicating characteristic has extra properties.
        
        * @experimental
        
        * @wearableOnly
        */
        ExtendedProps,
        /**
        * Property indicating notifications must be encrypted.
        
        * @experimental
        
        * @wearableOnly
        */
        NotifyEncryptionRequired,
        /**
        * Property indicating indications must be encrypted.
        
        * @experimental
        
        * @wearableOnly
        */
        IndicateEncryptionRequired
    }

}

declare namespace Bluetooth {
    /**
    * State of a Bluetooth connection.
    
    * `Disconnected` The Bluetooth device is disconnected from Spectacles.
    
    * `Connected` The Bluetooth device is connected and bonded with Spectacles.
    
    * @see Used By: {@link Bluetooth.BluetoothGatt#connectionState}, {@link Bluetooth.ConnectionStateChangedEvent#state}
    
    * @experimental
    
    * @wearableOnly
    */
    enum ConnectionState {
        /**
        * State indicating that the device is disconnected.
        
        * @experimental
        
        * @wearableOnly
        */
        Disconnected,
        /**
        * State indicating that the device is successfully connected.
        
        * @experimental
        
        * @wearableOnly
        */
        Connected
    }

}

declare namespace Bluetooth {
    /**
    * The event received when Bluetooth connection status has changed.
    
    * @experimental
    
    * @wearableOnly
    */
    class ConnectionStateChangedEvent extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * The new state of the Bluetooth connection.
        
        * @readonly
        
        * @experimental
        
        * @wearableOnly
        */
        state: Bluetooth.ConnectionState
        
    }

}

declare namespace Bluetooth {
    /**
    * The event received when Maximum Transmission Unit (MTU) of a Bluetooth device has changed.
    
    * @experimental
    
    * @wearableOnly
    */
    class MtuChangedEvent extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * The new Maximum Transmission Unit (MTU).
        
        * @readonly
        
        * @experimental
        
        * @wearableOnly
        */
        mtu: number
        
    }

}

declare namespace Bluetooth {
    /**
    * The filters to be used with {@link BluetoothCentralModule.startScan}.
    */
    class ScanFilter extends ScriptObject {
        /**
        * Construct a new filter.
        
        * @experimental
        
        * @wearableOnly
        */
        constructor()
        
        /**
        * @experimental
        
        * @wearableOnly
        */
        deviceName?: string
        
        /**
        * Filter for devices based on a prefix of manufacturer data. If undefined, no filtering on manufacturer data is applied.
        
        * @experimental
        
        * @wearableOnly
        */
        manufacturerDataPrefix?: Uint8Array
        
        /**
        * Filter for devices based on a manufacturer ID. If undefined, no filtering on manufacturer ID is applied.
        
        * @experimental
        
        * @wearableOnly
        */
        manufacturerId?: number
        
        /**
        * Filter for devices based on a service UUID. If undefined, no filtering on service UUID is applied.
        
        * @experimental
        
        * @wearableOnly
        */
        serviceUUID?: string
        
    }

}

declare namespace Bluetooth {
    /**
    * Defines how Bluetooth devices should be scanned. Used with {@link BluetoothCentralModule.startScan}.
    
    * @see Used By: {@link Bluetooth.ScanSettings#scanMode}
    
    * @experimental
    
    * @wearableOnly
    */
    enum ScanMode {
        /**
        * The default setting.
        
        * @experimental
        
        * @wearableOnly
        */
        Unset,
        /**
        * Scan devices opportunistically.
        
        * @experimental
        
        * @wearableOnly
        */
        Opportunistic,
        /**
        * Scan devices with low power.
        
        * @experimental
        
        * @wearableOnly
        */
        LowPower,
        /**
        * Scan devices while balancing power and latency.
        
        * @experimental
        
        * @wearableOnly
        */
        Balanced,
        /**
        * Scan devices with low latency.
        
        * @experimental
        
        * @wearableOnly
        */
        LowLatency
    }

}

declare namespace Bluetooth {
    /**
    * The results of a Bluetooth scan.
    
    * @see Used By: {@link Bluetooth.BluetoothCentralModule#startScan}
    */
    class ScanResult extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * Address of the device.
        
        * @readonly
        
        * @experimental
        
        * @wearableOnly
        */
        deviceAddress: Uint8Array
        
        /**
        * Name of the advertised device, if available.
        
        * @readonly
        
        * @experimental
        
        * @wearableOnly
        */
        deviceName?: string
        
        /**
        * Whether the device allows connections.
        
        * @readonly
        
        * @experimental
        
        * @wearableOnly
        */
        isConnectable: boolean
        
        /**
        * Data provided by the manufacturer, if available.
        
        * @readonly
        
        * @experimental
        
        * @wearableOnly
        */
        manufacturerData: {[key:number]:Uint8Array}
        
        /**
        * ID of the device's manufacturer, if available.
        
        * @readonly
        
        * @experimental
        
        * @wearableOnly
        */
        manufacturerId?: number
        
        /**
        * Signal strength of the received packet, measured in dBm.
        
        * @readonly
        
        * @experimental
        
        * @wearableOnly
        */
        rssi?: number
        
        /**
        * Data related to the services offered by the device.
        
        * @readonly
        
        * @experimental
        
        * @wearableOnly
        */
        serviceData: {[key:string]:Uint8Array}
        
        /**
        * Transmission power of the packet, measured in dBm.
        
        * @readonly
        
        * @experimental
        
        * @wearableOnly
        */
        txPower?: number
        
    }

}

declare namespace Bluetooth {
    /**
    * Settings when scanning for Bluetooth devices via {@link BluetoothCentralModule.startScan}.
    
    * @see Used By: {@link Bluetooth.BluetoothCentralModule#startScan}
    */
    class ScanSettings extends ScriptObject {
        /**
        * Construct a new setting.
        
        * @experimental
        
        * @wearableOnly
        */
        constructor()
        
        /**
        * Set the scanning method.
        
        * @experimental
        
        * @wearableOnly
        */
        scanMode: Bluetooth.ScanMode
        
        /**
        * Set the scanning timeout duration in seconds. After the timeout period the scan will stop and the promise will resolve.
        
        * @experimental
        
        * @wearableOnly
        */
        timeoutSeconds: number
        
        /**
        * Specify whether to call the predicate function only for unique devices or not.
        
        * @experimental
        
        * @wearableOnly
        */
        uniqueDevices: boolean
        
    }

}

/**
* Used to analyze camera input and apply similar image artifacts to AR objects to better blend and match with the real world.

* @example
* ```
* // @input Component.Camera camera
* script.camera.getSceneObject().createComponent("Component.BlurNoiseEstimation");
* ```
*/
declare class BlurNoiseEstimation extends Component {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Allows Physics simulation to control SceneObject.

* @remarks
* Derived from {@link ColliderComponent}, attaching this to a {@link SceneObject} turns it into a dynamic rigid-body that is automatically moved by the physics simulation in response to gravity, collisions, and other forces.

* @see [Physics Body](https://developers.snap.com/lens-studio/features/physics/physics-component#physics-body) guide.
* @see [Physics Examples](https://developers.snap.com/lens-studio/features/physics/physics-examples/physics).

* @example
* ```js
* // @input Physics.BodyComponent bodyComponent

* // Add instant impulse force upward
* script.bodyComponent.addForce(new vec3(0, 500, 0), Physics.ForceMode.Impulse);

* // Add instant impulse torque to rotate the object
* script.bodyComponent.addTorque(new vec3(0, 0, Math.PI * -25), Physics.ForceMode.Impulse);

* // Every frame, add upward force over time, relative to the object rotation
* // This is similar to a rocket boosting the object upward
* script.createEvent("UpdateEvent").bind(function() {
*     script.bodyComponent.addRelativeForce(new vec3(0, 2000, 0), Physics.ForceMode.Force);
* });
* ```
*/
declare class BodyComponent extends ColliderComponent {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Apply linear force at the object's center-of-mass.
    */
    addForce(force: vec3, mode: Physics.ForceMode): void
    
    /**
    * Apply force at a point offset from the object's origin, effectively generating torque.
    */
    addForceAt(force: vec3, offset: vec3, mode: Physics.ForceMode): void
    
    /**
    * Add a point constraint between this body and the given collider, at the given position. `target` is optional. If null, it is statically constrained to the world.   This is a convenience function that creates a child SceneObject with a ConstraintComponent, and sets its type, target, and constraint parameters. To fully remove the constraint, its SceneObject must be removed - not the component. Call `removeConstraint()` to do this.
    */
    addPointConstraint(target: ColliderComponent, position: vec3): ConstraintComponent
    
    /**
    * Relative to local rotation, apply linear force at the object's center-of-mass.
    */
    addRelativeForce(force: vec3, mode: Physics.ForceMode): void
    
    /**
    * Relative to local rotation, apply force at a point offset from the object's origin, effectively generating torque.
    */
    addRelativeForceAt(force: vec3, position: vec3, mode: Physics.ForceMode): void
    
    /**
    * Relative to local rotation, apply torque (angular force).
    */
    addRelativeTorque(torque: vec3, mode: Physics.ForceMode): void
    
    /**
    * Apply torque (angular force).
    */
    addTorque(torque: vec3, mode: Physics.ForceMode): void
    
    /**
    * Removes a constraint that was added with one of the `add*Constraint()` functions.
    */
    removeConstraint(constraint: ConstraintComponent): void
    
    /**
    * Damping applied to angular velocity, in the range 0.0 (no damping) to 1.0 (maximum damping).
    */
    angularDamping: number
    
    /**
    * Damping applied to linear velocity, in the range 0.0 (no damping) to 1.0 (maximum damping). This produces an effect similar to drag in that it causes the object to slow down over time. It is not however physically accurate, and it doesn't take into account surface area or mass.
    */
    damping: number
    
    /**
    * Density (kg/L) of the object, used to derive mass from volume. If modified, mass changes according to shape and scale.
    */
    density: number
    
    /**
    * If enabled, the body is dynamically simulated, such that it responds to forces and collisions, nested collider/body components will be merged into compound shapes. Otherwise, it acts as a static collider, functionally equivalent to Physics.ColliderComponent.
    */
    dynamic: boolean
    
    /**
    * Mass (kg) of the object. If modified from this field, mass is constant irrespective of shape and scale.
    */
    mass: number
    
}

/**
* Provides depth values of the tracked body encoded as D24_UNORM_S8_UINT in screen resolution. Depth is normalized between 0-1, and the stencil component is ignored. The values may be remapped from normalized units (0 to 1) to negative centimeters from the camera (-near in cm to -far in cm) using the `Depth Map` mode of the `Texture 2D Sample` node in Material Editor.
*/
declare class BodyDepthTextureProvider extends TextureProvider {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The index of the body to track. The first body detected is `0`.
    */
    bodyIndex: number
    
    /**
    * A background depth (where confidence < `minimumConfidence`) is represented by 1.0 (zFar). A value from 0.0 to 1.0 (default value 0.5).
    */
    minimumConfidence: number
    
    /**
    * The tracking context this effect is being applied to.
    */
    trackingScope: PersonTrackingScope
    
    /**
    * Far plane value in cm, Read only, always outputs 1000.0.
    
    * @readonly
    */
    zFar: number
    
    /**
    * Near plane value in cm, Read only, always outputs 1.0.
    
    * @readonly
    */
    zNear: number
    
}

/**
* Provides segmentation confidence values encoded in the `R` channel. The texture has the {@link Colorspace#RGBA} format in screen resolution, but `G`, `B`, `A` channels should be ignored.

* @example
* ```js
* // @input Component.Image image

* // Get the Body Instance Segmentation Texture from passed in Image component
* const bodySegmentationTexture = script.image.mainPass.baseTex;

* // Get the provider of the texture
* const segmentationTextureProvider = bodySegmentationTexture.control;

* // On tap, switch betweeen first and second body to segment
* script.createEvent("TapEvent").bind(function() {
*     const isTrackingBodyZero = segmentationTextureProvider.bodyIndex === 0;
*     const bodyToSegment = isTrackingBodyZero ? 1 : 0;
*     segmentationTextureProvider.bodyIndex = bodyToSegment;

*     print("Segmenting body: " + bodyToSegment);
* })
* ```
*/
declare class BodyInstanceSegmentationTextureProvider extends TextureProvider {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Which body to segment if multiple are available.
    */
    bodyIndex: number
    
    /**
    * Invert the segmentation of the body.
    */
    invertMask: boolean
    
    /**
    * Refine the edge of the segmentation of the body.
    */
    refineEdge: boolean
    
    /**
    * The tracking context this effect is being applied to.
    */
    trackingScope: PersonTrackingScope
    
}

/**
* Provides surface normal values of the tracked body encoded as RGBA (x, y, z, confidence) in `R8G8B8A8_UNORM` format in screen resolution. The XYZ values may need to be remapped from [0 -> 1] to [-1 -> 1] using the "Normal Map" mode of the `Texture 2D Sample` node in Material Editor.
*/
declare class BodyNormalsTextureProvider extends TextureProvider {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The index of the body to track. The first body detected is `0`.
    */
    bodyIndex: number
    
    /**
    * The tracking context this effect is being applied to.
    */
    trackingScope: PersonTrackingScope
    
}

/**
* Provider for full Body Mesh render object.
*/
declare class BodyRenderObjectProvider extends RenderObjectProvider {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Enable main Body Mesh geometry.
    */
    bodyGeometryEnabled: boolean
    
    /**
    * Index of body in scene to track.
    */
    bodyIndex: number
    
    /**
    * Enable head mesh.
    */
    headGeometryEnabled: boolean
    
    /**
    * Enable left hand mesh.
    */
    leftHandGeometryEnabled: boolean
    
    /**
    * Enable right hand mesh.
    */
    rightHandGeometryEnabled: boolean
    
    /**
    * The tracking context this effect is being applied to.
    */
    trackingScope: PersonTrackingScope
    
}

/**
* Configures 3D Body Tracking for the {@link ObjectTracking3D} component.

* @see [3D Body and Hand Tracking](https://developers.snap.com/lens-studio/features/ar-tracking/body/object-tracking-3d) guide.
*/
declare class BodyTrackingAsset extends Object3DAsset {
    
    /** @hidden */
    protected constructor()
    
    /**
    * When true, hand tracking will be enabled.
    */
    handTrackingEnabled: boolean
    
    /**
    * The tracking context this effect is being applied to.
    */
    trackingScope: PersonTrackingScope
    
    /**
    * Key for Head attachment point.
    */
    static Head: string
    
    /**
    * Key for Hips attachment point.
    */
    static Hips: string
    
    /**
    * Key for Left Arm attachment point.
    */
    static LeftArm: string
    
    /**
    * Key for Left Foot attachment point.
    */
    static LeftFoot: string
    
    /**
    * Key for Left Forearm attachment point.
    */
    static LeftForeArm: string
    
    /**
    * Key for Left Hand attachment point.
    */
    static LeftHand: string
    
    /**
    * Key for Left Hand Index 1 attachment point.
    */
    static LeftHandIndex1: string
    
    /**
    * Key for Left Hand Index 2 attachment point.
    */
    static LeftHandIndex2: string
    
    /**
    * Key for Left Hand Index 3 attachment point.
    */
    static LeftHandIndex3: string
    
    /**
    * Key for Left Hand Middle 1 attachment point.
    */
    static LeftHandMiddle1: string
    
    /**
    * Key for Left Hand Middle 2 attachment point.
    */
    static LeftHandMiddle2: string
    
    /**
    * Key for Left Hand Middle 3 attachment point.
    */
    static LeftHandMiddle3: string
    
    /**
    * Key for Left Hand Pinky 1 attachment point.
    */
    static LeftHandPinky1: string
    
    /**
    * Key for Left Hand Pinky 2 attachment point.
    */
    static LeftHandPinky2: string
    
    /**
    * Key for Left Hand Pinky 3 attachment point.
    */
    static LeftHandPinky3: string
    
    /**
    * Key for Left Hand Ring 1 attachment point.
    */
    static LeftHandRing1: string
    
    /**
    * Key for Left Hand Ring 2 attachment point.
    */
    static LeftHandRing2: string
    
    /**
    * Key for Left Hand Ring 3 attachment point.
    */
    static LeftHandRing3: string
    
    /**
    * Key for Left Hand Thumb 1 attachment point.
    */
    static LeftHandThumb1: string
    
    /**
    * Key for Left Hand Thumb 2 attachment point.
    */
    static LeftHandThumb2: string
    
    /**
    * Key for Left Hand Thumb 3 attachment point.
    */
    static LeftHandThumb3: string
    
    /**
    * Key for Left Leg attachment point.
    */
    static LeftLeg: string
    
    /**
    * Key for Left Shoulder attachment point.
    */
    static LeftShoulder: string
    
    /**
    * Key for Left Toe Base attachment point.
    */
    static LeftToeBase: string
    
    /**
    * Key for Upper Left Leg attachment point.
    */
    static LeftUpLeg: string
    
    /**
    * Key for Neck attachment point.
    */
    static Neck: string
    
    /**
    * Key for Right Arm attachment point.
    */
    static RightArm: string
    
    /**
    * Key for Right Foot attachment point.
    */
    static RightFoot: string
    
    /**
    * Key for Right Forearm attachment point.
    */
    static RightForeArm: string
    
    /**
    * Key for Right Hand attachment point.
    */
    static RightHand: string
    
    /**
    * Key for Right Hand Index 1 attachment point.
    */
    static RightHandIndex1: string
    
    /**
    * Key for Right Hand Index 2 attachment point.
    */
    static RightHandIndex2: string
    
    /**
    * Key for Right Hand Index 3 attachment point.
    */
    static RightHandIndex3: string
    
    /**
    * Key for Right Hand Middle 1 attachment point.
    */
    static RightHandMiddle1: string
    
    /**
    * Key for Right Hand Middle 2 attachment point.
    */
    static RightHandMiddle2: string
    
    /**
    * Key for Right Hand Middle 3 attachment point.
    */
    static RightHandMiddle3: string
    
    /**
    * Key for Right Hand Pinky 1 attachment point.
    */
    static RightHandPinky1: string
    
    /**
    * Key for Right Hand Pinky 2 attachment point.
    */
    static RightHandPinky2: string
    
    /**
    * Key for Right Hand Pinky 3 attachment point.
    */
    static RightHandPinky3: string
    
    /**
    * Key for Right Hand Ring 1 attachment point.
    */
    static RightHandRing1: string
    
    /**
    * Key for Right Hand Ring 2 attachment point.
    */
    static RightHandRing2: string
    
    /**
    * Key for Right Hand Ring 3 attachment point.
    */
    static RightHandRing3: string
    
    /**
    * Key for Right Hand Thumb 1 attachment point.
    */
    static RightHandThumb1: string
    
    /**
    * Key for Right Hand Thumb 2 attachment point.
    */
    static RightHandThumb2: string
    
    /**
    * Key for Right Hand Thumb 3 attachment point.
    */
    static RightHandThumb3: string
    
    /**
    * Key for Right Leg attachment point.
    */
    static RightLeg: string
    
    /**
    * Key for Right Shoulder attachment point.
    */
    static RightShoulder: string
    
    /**
    * Key for Right Toe Base attachment point.
    */
    static RightToeBase: string
    
    /**
    * Key for Upper Right Leg attachment point.
    */
    static RightUpLeg: string
    
    /**
    * Key for Spine attachment point.
    */
    static Spine: string
    
    /**
    * Key for Spine1 attachment point.
    */
    static Spine1: string
    
    /**
    * Key for Spine2 attachment point.
    */
    static Spine2: string
    
}

/**
* A box collision shape.

* @see Returned By: {@link Shape.createBoxShape}
*/
declare class BoxShape extends Shape {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The size of the box on each local axis.
    */
    size: vec3
    
}

/**
* Triggered when eyebrows are lowered on the tracked face.

* @example
* ```js
* var event = script.createEvent("BrowsLoweredEvent");
* event.faceIndex = 0;
* event.bind(function (eventData)
* {
*     print("Brows were lowered on face 0");
* });
* ```
*/
declare class BrowsLoweredEvent extends FaceTrackingEvent {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Triggered when eyebrows are raised on the tracked face.

* @example
* ```js
* var event = script.createEvent("BrowsRaisedEvent");
* event.faceIndex = 0;
* event.bind(function (eventData)
* {
*     print("Brows were raised on face 0");
* });
* ```
*/
declare class BrowsRaisedEvent extends FaceTrackingEvent {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Triggered when eyebrows are returned to normal on the tracked face.

* @example
* ```js
* var event = script.createEvent("BrowsReturnedToNormalEvent");
* event.faceIndex = 0;
* event.bind(function (eventData)
* {
*     print("Brows were returned to normal on face 0");
* });
* ```
*/
declare class BrowsReturnedToNormalEvent extends FaceTrackingEvent {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Renders {@link SceneObject}s to one or more Render Target textures.

* @remarks
* A Camera will only render a  {@link SceneObject} if its render layer is enabled on the Camera.

* @see
* [Camera and Layers](https://developers.snap.com/lens-studio/lens-studio-workflow/scene-set-up/camera) guide.

* @see Used By: {@link BaseMeshVisual#snap}, {@link InteractionComponent#setCamera}
*/
declare class Camera extends Component {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Adds layer `id` to the list of layers the Camera will render.
    
    * @deprecated
    */
    addRenderLayer(id: number): void
    
    /**
    * Returns whether rendering layer `id` is enabled for the Camera.
    
    * @deprecated
    */
    checkRenderLayer(id: number): boolean
    
    /**
    * Returns a list of 32 numbers, one for every render layer. Values will either be 0 or 1, 0 meaning the Camera won't render the layer and 1 meaning it will.
    
    * @deprecated
    */
    getAllRenderLayers(): number[]
    
    /**
    * For orthographic cameras, returns the camera size as (width, height).
    */
    getOrthographicSize(): vec2
    
    /**
    * Returns true if a sphere with the specified world space center position and radius is visible within the camera frustum, false otherwise.
    */
    isSphereVisible(center: vec3, radius: number): boolean
    
    /**
    * Converts a world space position to a raw screen space position.
    * The screen space position will be returned as a `vec3` with `x`,`y` representing normalized screen space,
    * and `z` representing a raw depth value not directly convertible to world units.
    * This returned value will mostly be useful for passing into `unproject()`.
    */
    project(worldSpacePoint: vec3): vec3
    
    /**
    * Removes layer `id` from the list of layers the Camera will render.
    
    * @deprecated
    */
    removeRenderLayer(id: number): void
    
    /**
    * Converts a screen space position to a world space position, given an absolute depth.
    * The screen space position should be provided as a `vec2` in the range ([0-1], [0-1]),
    * (0,0) being the top-left of the screen and (1,1) being the bottom-right.
    * The returned world space position will be the point `absoluteDepth` units away from the Camera's
    * `near` plane at the point specified in screen space.
    */
    screenSpaceToWorldSpace(normalizedScreenSpacePoint: vec2, absoluteDepth: number): vec3
    
    /**
    * Converts a raw screen space position to a world space position.
    * `clipSpacePoint` should be a `vec3` returned from a previous `project()` call, since the
    * `z` value represents a raw depth value not directly convertible to world units.
    */
    unproject(clipSpacePoint: vec3): vec3
    
    /**
    * Converts the world space position `worldSpacePoint` to a screen space position.
    * Screen positions are represented in the range ([0-1], [0-1]), (0,0) being the top-left of the screen and (1,1) being the bottom-right.
    */
    worldSpaceToScreenSpace(worldSpacePoint: vec3): vec2
    
    /**
    * The aspect ratio of the camera (width/height).
    */
    aspect: number
    
    /**
    * When `enableClearColor` is true and `inputTexture` is null, this color is used to clear this Camera's `renderTarget` before drawing to it.
    */
    clearColor: vec4
    
    /**
    * Returns an array of Color Render Targets. The first color render target is always available.
    */
    colorRenderTargets: Camera.ColorRenderTarget[]
    
    /**
    * Determines the way depth is handled on this Camera. Changing this can help sort objects at different distance ranges.
    */
    depthBufferMode: Camera.DepthBufferMode
    
    /**
    * Descriptor of depth/stencil textures and clear options.
    */
    depthStencilRenderTarget: Camera.DepthStencilRenderTarget
    
    /**
    * The Texture this Camera will be used to render content onto, and will be used to configure device properties which are set to physical. When unset it is assumed to be the DeviceCameraTexture. Relevant for cases where TextureTrackingScope will be used with 3D content like Face Mesh which will need the save fov used for the Camera.
    */
    devicePropertiesSource: TextureTrackingScope
    
    /**
    * Controls which Camera settings will be overridden by physical device properties.
    * For example, this can be used to override the `fov` property to match the device camera's actual field of view.
    */
    devicePropertyUsage: Camera.DeviceProperty
    
    /**
    * If enabled, this Camera will clear the color on its `renderTarget` before drawing to it.
    * `inputTexture` will be used to clear it unless it is null, in which case `clearColor` is used instead.
    
    * @deprecated
    */
    enableClearColor: boolean
    
    /**
    * If enabled, this Camera will clear the depth buffer on its `renderTarget` before drawing to it.
    */
    enableClearDepth: boolean
    
    /**
    * The distance of the far clipping plane.
    */
    far: number
    
    /**
    * The Camera's field of view in radians.
    */
    fov: number
    
    /**
    * When `enableClearColor` is true, this texture is used to clear this Camera's `renderTarget` before drawing.
    * If this texture is null, `clearColor` will be used instead.
    */
    inputTexture: Texture
    
    /**
    * If true, the camera FOV will be overridden to match the device's physical camera FOV.
    
    * @deprecated
    */
    isPhysical: boolean
    
    /**
    * A texture controlling which parts of the output texture the camera will draw to.
    * The "red" value of each pixel determines how strongly the camera will draw to that part of the image.
    * For example, a completely black section will cause the camera to not draw there at all. A completely
    * white (or red) section will cause the camera to draw normally. Colors in between, like gray, will be semitransparent.
    */
    maskTexture: Texture
    
    /**
    * The distance of the near clipping plane.
    */
    near: number
    
    /**
    * Toggles Ray Tracing for the camera. When true, Ray Tracing is enabled.
    */
    rayTracing: any
    
    /**
    * Controls the set of layers this Camera will render.
    */
    renderLayer: LayerSet
    
    /**
    * The sorting order the Camera renders in. Every frame, Cameras render in ascending order determined by their `renderOrder` properties.
    */
    renderOrder: number
    
    /**
    * The RenderTarget this Camera will draw to.
    */
    renderTarget: Texture
    
    /**
    * Sets which face of the cubemap this camera will render to.
    */
    renderTargetCubemapFace: Camera.CubemapFace
    
    /**
    * The orthographic size of the camera.
    */
    size: number
    
    /**
    * Returns a number of hardware supported render targets. The max number is 4. If the device doesnt support Multiple Render Targets this property equals 1.
    
    * @deprecated
    
    * @readonly
    */
    supportedColorRenderTargetCount: number
    
    /**
    * Controls which type of rendering the camera uses.
    */
    type: Camera.Type
    
    /**
    * Creates and returns a new Color Render Target.
    */
    static createColorRenderTarget(): Camera.ColorRenderTarget
    
    /**
    * Create a depth/stencil render target descriptor for the camera.
    */
    static createDepthStencilRenderTarget(): Camera.DepthStencilRenderTarget
    
    /**
    * Return true if the device supports stencil operations and render to depth texture.
    */
    static depthStencilRenderTargetSupported(): boolean
    
    /**
    * Returns the number of possible render target bindings to the camera.
    */
    static getSupportedColorRenderTargetCount(): number
    
}

declare namespace Camera {
    /**
    * The base class from which ColorRenderTarget and DepthStencilRenderTarget are derived from
    */
    class BaseRenderTarget extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * Color texture used in clear color operation in "CustomTexture" mode.
        */
        inputTexture: Texture
        
        /**
        * Texture used like a color mask for target texture.
        */
        maskTexture: Texture
        
        /**
        * Render target texture. Camera will render scene color values to this texture.
        */
        targetTexture: Texture
        
    }

}

declare namespace Camera {
    /**
    * Color based RenderTarget.
    
    * @see Returned By: {@link Camera.createColorRenderTarget}
    */
    class ColorRenderTarget extends Camera.BaseRenderTarget {
        
        /** @hidden */
        protected constructor()
        
        /**
        * Sets the clear color of the camera when its `clearColorOption` is set to `ClearColorOption.CustomColor`
        */
        clearColor: vec4
        
        /**
        * Sets how the RenderTarget's color will be cleared before rendering during each frame.
        */
        clearColorOption: ClearColorOption
        
    }

}

declare namespace Camera {
    /**
    * Different faces of the cubemap that a camera can render into.
    
    * @see Used By: {@link Camera#renderTargetCubemapFace}
    
    * @example
    * ```js
    * // @input Component.Camera camera
    
    * if (script.camera.renderTarget.control.textureType == RenderTargetProvider.TextureType.TextureCubemap) {
    *     script.camera.renderTargetCubemapFace = Camera.CubemapFace.NegativeZ;
    * }
    * ```
    */
    enum CubemapFace {
        /**
        * The positive X cubemap face.
        */
        PositiveX,
        /**
        * The right cubemap face, same as PositiveX.
        */
        Right,
        /**
        * The left cubemap face, same as NegativeX.
        */
        Left,
        /**
        * The negative X cubemap face.
        */
        NegativeX,
        /**
        * The positive Y cubemap face.
        */
        PositiveY,
        /**
        * The top cubemap face, same as PositiveY.
        */
        Top,
        /**
        * The bottom cubemap face, same as NegativeY.
        */
        Bottom,
        /**
        * The negative Y cubemap face.
        */
        NegativeY,
        /**
        * The front cubemap face, same as PositiveZ.
        */
        Front,
        /**
        * The positive Z cubemap face.
        */
        PositiveZ,
        /**
        * The back cubemap face, same as NegativeZ.
        */
        Back,
        /**
        * The negative Z cubemap face.
        */
        NegativeZ
    }

}

declare namespace Camera {
    /**
    * Used in {@link Camera}'s `depthBufferMode` property.
    * Each mode is suited for handling objects at a certain distance range.
    * For more information on depth modes, see the [Camera and Layers](https://developers.snap.com/lens-studio/lens-studio-workflow/scene-set-up/camera) guide.
    
    * @see Used By: {@link Camera#depthBufferMode}
    
    * @example
    * ```js
    * // @input Component.Camera camera
    
    * script.camera.depthBufferMode = Camera.DepthBufferMode.Logarithmic;
    * ```
    */
    enum DepthBufferMode {
        /**
        * Gives higher depth precision on nearby objects, so is better suited for scenes near to the camera.
        */
        Regular,
        /**
        * Gives higher depth precision on far away objects, so is better suited for scenes far away from the camera.
        */
        Logarithmic
    }

}

declare namespace Camera {
    /**
    * This class inherits from the BaseRenderTarget class. BaseRenderTarget class is not available for creation and is used like the base class for DepthStencilRenderTarget class to provide access to targetTexture, inputTexture and maskTexture properties.
    
    * @see Used By: {@link Camera#depthStencilRenderTarget}
    * @see Returned By: {@link Camera.createDepthStencilRenderTarget}
    */
    class DepthStencilRenderTarget extends Camera.BaseRenderTarget {
        
        /** @hidden */
        protected constructor()
        
        /**
        * Float value in range [0.0..1.0] used in depth buffer clear operation in "CustomValue" mode. The initial value is 1.0.
        */
        clearDepthValue: number
        
        /**
        * Unsigned int value in range [0..0xFF] used in stencil buffer clear operation in "CustomValue" mode. The initial value is 0.
        */
        clearStencilValue: number
        
        /**
        * The same as "depthClearOption" property of DepthStencilRenderTargetProvider. But if Camera's depth clear option property is set to "CustomValue" or "CustomTexture" then this has priority over depth/stencil provider settings.
        */
        depthClearOption: DepthClearOption
        
        /**
        * The same as "stencilClearOption" property of DepthStencilRenderTargetProvider. But if Camera's stencil clear option property is set to "CustomValue" or "CustomTexture" then this has priority over depth/stencil provider settings.
        */
        stencilClearOption: StencilClearOption
        
    }

}

declare namespace Camera {
    /**
    * Used in {@link Camera}'s `devicePropertyUsage` property.
    * Specifies which camera properties should be overridden by device properties.
    
    * @see Used By: {@link Camera#devicePropertyUsage}
    
    * @example
    * ```js
    * // @input Component.Camera camera
    
    * script.camera.devicePropertyUsage = Camera.DeviceProperty.None;
    
    * script.camera.fov = Math.PI * 0.25;
    * script.camera.aspect = 1.0;
    * ```
    */
    enum DeviceProperty {
        /**
        * No Camera properties are overridden with device properties.
        */
        None,
        /**
        * Overrides the Camera's `aspect` property to use the device's aspect ratio instead.
        */
        Aspect,
        /**
        * Overrides the Camera's `fov` property to use the device's field of view instead.
        */
        Fov,
        /**
        * Overrides both `aspect` and `fov` with the device's properties.
        */
        All
    }

}

declare namespace Camera {
    /**
    * Returned from {@link Camera}'s `type` property.
    
    * @see Used By: {@link Camera#type}
    
    * @example
    * ```js
    * // @input Component.Camera camera
    
    * if (script.camera.type == Camera.Type.Orthographic) {
    *     var orthoSize = script.camera.getOrthographicSize();
    * }
    * ```
    */
    enum Type {
        /**
        * Simulates how perspective and depth perception work in the real world. Useful for rendering objects in 3D space.
        */
        Perspective,
        /**
        * Does not simulate perspective distortion. Useful for 2D effects like Images and Text.
        */
        Orthographic
    }

}

/**
* Triggered when the device's back facing camera becomes active.

* @example
* ```js
* var event = script.createEvent("CameraBackEvent");
* event.bind(function (eventData)
* {
* 	print("back camera active");
* });
* ```
*/
declare class CameraBackEvent extends SceneEvent {
    
    /** @hidden */
    protected constructor()
    
}

/**
* An entity which provided metadata about the current camera image provided by CameraTextureProvider. Modeled after VideoFrame web API

* @exposesUserData

* @wearableOnly

* @example
* ```js let onNewFrame = cameraTexture.control.onNewFrame; let registration = onNewFrame.add((frame) => {   let cameraTs = frame.timestampMillis;   // ... });  script.onStop.add(() => onNewFrame.remove(registration)); ```
*/
declare class CameraFrame extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The timestamp (in seconds) in which this frame was received.
    
    * @readonly
    
    * @exposesUserData
    
    * @wearableOnly
    */
    timestampSeconds: number
    
}

/**
* Triggered when the device's front facing camera becomes active.

* @example
* ```js
* var event = script.createEvent("CameraFrontEvent");
* event.bind(function (eventData)
* {
* 	print("front camera active");
* });
* ```
*/
declare class CameraFrontEvent extends SceneEvent {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Provides access to a specific camera on Spectacles device.

* @remarks
* Useful for requesting a specific camera on Spectacles or requesting images from both cameras simultaneously.

* @see [Camera Module](https://developers.snap.com/spectacles/about-spectacles-features/apis/camera-module) guide.

* @exposesUserData

* @wearableOnly

* @example
* ```js
* let cameraModule = require('LensStudio:CameraModule');

* let cameraRequest = CameraModule.createCameraRequest();
* cameraRequest.id = CameraModule.CameraId.Left_Color;

* let cameraTexture = cameraModule.requestCamera(cameraRequest);
* ```

* ```js
* // @input Asset.Image displayImage

* let cameraModule = require('LensStudio:CameraModule');

* script.createEvent('OnStartEvent').bind(function() {
*     let cameraRequest = CameraModule.createCameraRequest();
*     cameraRequest.id = CameraModule.CameraId.Left_Color;

*     let cameraTexture = cameraModule.requestCamera(cameraRequest);
*     script.displayImage.mainPass.baseTex = cameraTexture
* })
* ```

* ```
* // @input Component.MLComponent mlComponent
* // @input SceneObject image

* let cameraModule = require('LensStudio:CameraModule');
* let createCameraTexture = () => {
*     let request = CameraModule.createCameraRequest();
*     request.cameraId = CameraModule.CameraId.Left_Color;
*     return cameraModule.requestCamera(request);
* }

* let cameraTexControl;
* let runInference = (frame) => {
*     script.mlComponent.runImmediate(true);
*     // Use frame.timestampMillis if you need to use timestamps
*     script.image.enabled = true;
* }

* script.mlComponent.onLoadingFinished = () => {
*     let cameraTex = createCameraTexture();
*     cameraTexControl = cameraTex.control;
*     script.mlComponent.getInput('input').texture = cameraTex;
*     cameraTexControl.onNewFrame.add(runInference)

*     // For obtaining camera intrinsics / extrinsics
*     let cameraInfo =
*         global.deviceInfoSystem.getTrackingCameraForId(CameraModule.CameraId.Left_Color);
*     print(cameraInfo);
* }

* ```

* ```js
* let cameraModule = require('LensStudio:CameraModule');
* let remoteMediaModule = require('LensStudio:RemoteMediaModule');
* script.createEvent('OnStartEvent').bind(function() {
*     let cameraRequest = CameraModule.createCameraRequest();
*     let cameraTex = cameraModule.requestCamera(cameraRequest);
*     // Wait for image to be available before trying to use it
*     let registration = cameraTex.control.onNewFrame.add(async function() {
*         // Only need 1 frame, remove registration to stop receiving more frames
*         cameraTex.control.onNewFrame.remove(registration);

*         let imageResource =  await remoteMediaModule.createImageResourceForTexture(cameraTex, ImageUploadOptions.create());
*         // Use imageResource, like uploading to backend for further processing
*     })
* });
* ```
*/
declare class CameraModule extends Asset {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Get the list of resolutions this hardware camera can provide.
    
    * @exposesUserData
    
    * @wearableOnly
    */
    getSupportedImageResolutions(): vec2[]
    
    /**
    * Returns a {@link Texture} whose provider is {@link CameraTextureProvider} which provides images from the requested camera ID.
    
    * @exposesUserData
    
    * @wearableOnly
    */
    requestCamera(request: CameraModule.CameraRequest): Texture
    
    /**
    * Spectacles: Request a still image of the user's camera stream. Unlike {@CameraModule.requestCamera}, this method takes more time but yields a higher resolution image (3200x2400) suitable for tasks like OCR. This method is asynchronous and when complete will return an {@link ImageFrame} that contains a {@Texture} that can be attached to a visual.
    
    * ```js
    * let cameraModule = require("LensStudio:CameraModule");
    * let imageRequest = CameraModule.createImageRequest();
    
    * try {
    *   let imageFrame = await cameraModule.requestImage(imageRequest);
    
    *   // Use the texture in some visual
    *   script.image.mainPass.baseTex = imageFrame.texture;
    *   let timestamp = imageFrame.timestampMillis; // scene-relative time
    * } catch (error) {
    *   print(`Still image request failed: ${error}`);
    * }
    * ```
    
    * @exposesUserData
    
    * @wearableOnly
    */
    requestImage(request: CameraModule.ImageRequest): Promise<ImageFrame>
    
    /**
    * Creates a camera request object.
    
    * @exposesUserData
    
    * @wearableOnly
    */
    static createCameraRequest(): CameraModule.CameraRequest
    
    /**
    * Spectacles: create a {@link CameraImage.ImageRequest}. This object can be used to configure a request for a high resolution image of the user's camera stream. The resolution of this image will be fixed to 3200x2400.
    
    * @exposesUserData
    
    * @wearableOnly
    */
    static createImageRequest(): CameraModule.ImageRequest
    
}

declare namespace CameraModule {
    /**
    * A handle to specify which camera on the Spectacles to request from. Used with `CameraModule.createCameraRequest`.
    
    * @see Used By: {@link CameraModule.CameraRequest#cameraId}, {@link DeviceInfoSystem#getTrackingCameraForId}
    
    * @exposesUserData
    
    * @wearableOnly
    */
    enum CameraId {
        /**
        * The default color camera. On Spectacles (2024), the default color camera is located on the left side of the device.
        
        * @exposesUserData
        
        * @wearableOnly
        */
        Default_Color,
        /**
        * The color camera on the left side of the device.
        
        * @exposesUserData
        
        * @wearableOnly
        */
        Left_Color,
        /**
        * The color camera on the right side of the device.
        
        * @exposesUserData
        
        * @wearableOnly
        */
        Right_Color
    }

}

declare namespace CameraModule {
    /**
    * An object that is used to request the desired camera ID. It should be passed to the CameraModule to get back a camera texture.
    
    * @see Used By: {@link CameraModule#requestCamera}
    * @see Returned By: {@link CameraModule.createCameraRequest}
    
    * @exposesUserData
    
    * @wearableOnly
    */
    class CameraRequest extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * The id of the camera to be accessed.
        
        * @exposesUserData
        
        * @wearableOnly
        */
        cameraId: CameraModule.CameraId
        
        /**
        * The desired resolution of the received camera frame. If not specified, will use the system default resolution. It is recommended to use lowest resolution required for your use case to save on power and not overheat the device.
        
        * @exposesUserData
        
        * @wearableOnly
        */
        imageSmallerDimension?: number
        
    }

}

declare namespace CameraModule {
    /**
    * Spectacles: ImageRequest contains the parameterization for a still image request, which is a request for a high resolution image of the user's current camera stream.
    
    * @see Used By: {@link CameraModule#requestImage}
    * @see Returned By: {@link CameraModule.createImageRequest}
    
    * @exposesUserData
    
    * @wearableOnly
    
    * @example
    * ```js
    * let cameraModule = require('LensStudio:CameraModule');
    * let imageRequest = CameraModule.createImageRequest();
    
    * try {
    *   let imageFrame = await cameraModule.requestImage(imageRequest);
    
    *   // E.g, use the texture in some visual
    *   script.image.mainPass.baseTex = imageFrame.texture;
    *   let timestamp = imageFrame.timestampMillis; // scene-relative time
    * } catch (error) {
    *   print(`Still image request failed: ${error}`);
    * }
    * ```
    
    */
    class ImageRequest extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * The desired crop of the Image Request.
        
        * @exposesUserData
        
        * @wearableOnly
        */
        crop: Rect
        
        /**
        * The desired resolution of the Image Request
        
        * @exposesUserData
        
        * @wearableOnly
        */
        resolution: vec2
        
    }

}

/**
* Represents media selected from the device camera roll.

* @see Used By: {@link CameraRollMedia#isSameMedia}
*/
declare class CameraRollMedia extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Checks if two CameraRollMedia objects refer to the same media.
    */
    isSameMedia(other: CameraRollMedia): boolean
    
    /**
    * Unique hash of the source media. Can be used to identify the media, for example as a key to store in maps.
    
    * @readonly
    */
    mediaId: number
    
    /**
    * Type of media.
    
    * @readonly
    */
    mediaType: CameraRollMediaType
    
    /**
    * Dynamic resource for accessing the media content.
    
    * @readonly
    */
    resource: DynamicResource
    
}

/**
* Enum representing the type of media selected from camera roll.

* @see Used By: {@link CameraRollMedia#mediaType}
*/
declare enum CameraRollMediaType {
    /**
    * Media type not specified.
    */
    Unset,
    /**
    * Still image media type.
    */
    Image,
    /**
    * Video media type.
    */
    Video
}

/**
* Provides an interface to the devices camera roll and allows users to select multiple media.
*/
declare class CameraRollModule extends Asset {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Closes the device camera roll interface.
    */
    hideMediaPicker(): void
    
    /**
    * Opens the device camera roll interface to allow multiple media selection with specified configuration options.
    */
    showMediaPicker(options: CameraRollModule.ShowOptions): void
    
    /**
    * Event triggered when selections are updated.
    
    * @readonly
    */
    onSelectionsUpdated: event1<CameraRollMedia[], void>
    
    /**
    * Array of currently selected media objects.
    
    * @readonly
    */
    selectedMedia: CameraRollMedia[]
    
    /**
    * Returns the default selection limit for the media picker.
    */
    static getDefaultSelectionLimit(): number
    
    /**
    * Returns the maximum allowed selection limit for the media picker.
    */
    static getMaxSelectionLimit(): number
    
}

declare namespace CameraRollModule {
    /**
    * Configuration options for displaying the camera roll media picker.
    
    * @see Used By: {@link CameraRollModule#showMediaPicker}
    */
    class ShowOptions {
        /**
        * Creates a new ShowOptions instance with default selection limit of 10 media items.
        */
        constructor()
        
        /**
        * Maximum number of media items that can be selected (1-20, defaults to 10).
        */
        selectionLimit: number
        
    }

}

declare class CameraTextureProvider extends TextureProvider {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Register a callback to be called whenever a new frame is received from the camera. On spectacles, the render rate is different (and typically higher) than the camera update rate, so this callback can be used to know when a new camera frame is available. This allows you to only do additional processing (like run a SnapML model) when a new frame is available instead of doing redundant work on each scene update event.
    
    * @readonly
    
    * @exposesUserData
    
    * @wearableOnly
    */
    onNewFrame: event1<CameraFrame, void>
    
}

/**
* A root of the 2D {@link ScreenTransform} hierarchy in 3D space. Also used to configure unit settings of Orthographic {@link Camera}.

* @remarks

* {@link SceneObject}s with {@link ScreenTransform} can be placed on the Canvas, and the Canvas can be sized and placed anywhere in 3D space. It is like a painters canvas for ScreenTransforms.

* @see
* [Canvas](https://developers.snap.com/lens-studio/lens-studio-workflow/scene-set-up/2d/canvas-component) guide.
* [Screen Transform Overview](https://developers.snap.com/lens-studio/lens-studio-workflow/scene-set-up/2d/screen-transform-overview) guide.

* @example
* ```js
* var canvas = script.getSceneObject().getComponent("Component.Canvas");
* print(canvas.pivot); // Outputs: {x: 0, y: 0}

* // Note: this variable will be a COPY of canvas.pivot, not reference
* var pivotCopy = canvas.pivot;

* // Can mutate the copy then set it again
* pivotCopy.x = 1;
* pivotCopy.y = 1;

* canvas.pivot = pivotCopy;
* ```
*/
declare class Canvas extends Component {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Get size of rectangle as (width, height)
    */
    getSize(): vec2
    
    /**
    * Set size of rectangle as (width, height)
    */
    setSize(value: vec2): void
    
    /**
    * The point about which the Canvas Rectangle will rotate. Defined as fractional coordinates of the Canvas's dimensions. e.g. (1 , 1) brings the pivot to the top right corner of the canvas. Or (0.5, 0) moves the pivot to the right by half the canvas width.
    */
    pivot: vec2
    
    /**
    * The rendering sort order for the objects underneath this canvas.
    */
    sortingType: Canvas.SortingType
    
    /**
    * The unit for the objects underneath this canvas.
    */
    unitType: Canvas.UnitType
    
    /**
    * World Space Rectangle that defines the Canvas as offsets in world units from the SceneObject's position
    */
    worldSpaceRect: Rect
    
}

declare namespace Canvas {
    /**
    * Used by Canvas to set how an object is sorted during rendering.
    
    * @see Used By: {@link Canvas#sortingType}
    */
    enum SortingType {
        /**
        * Sort based on their position in the hierarchy. This is the default.
        */
        Hierarchy,
        /**
        * Sort based on their position in worldspace.
        */
        Depth
    }

}

declare namespace Canvas {
    /**
    * Used by Canvas to set how an object is positioned.
    
    * @see Used By: {@link Canvas#unitType}
    */
    enum UnitType {
        /**
        * How objects are positioned outside of canvas.
        */
        World,
        /**
        * The smallest addressable element on a display.
        */
        Pixels,
        /**
        * Sometimes referred to as Density Independent Pixels (dp), are abstractions of pixels that make it easier to deal with displays of different densities.
        */
        Points
    }

}

/**
* Changes the capitalization of the text component. Useful when using dynamic texts.

* @see Used By: {@link Text#capitilizationOverride}, {@link Text3D#capitilizationOverride}
*/
declare enum CapitilizationOverride {
    /**
    * Display the capitalization of the displayed text as provided.
    */
    None,
    /**
    * Sets the capitalization of the displayed text to lowercase.
    */
    AllLower,
    /**
    * Sets the capitalization of the displayed text to uppercase.
    */
    AllUpper
}

/**
* A capsule collision shape. Also known as a capped cylinder.

* @see Returned By: {@link Shape.createCapsuleShape}
*/
declare class CapsuleShape extends Shape {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Lengthwise local axis along which the capsule is oriented.
    */
    axis: Axis
    
    /**
    * Length of the capsule along its local axis. This is the distance between the two end-cap centers.
    */
    length: number
    
    /**
    * Radius of the capsule cylinder, and its end-cap spheres.
    */
    radius: number
    
}

/**
* Settings for how color will be cleared before rendering.

* @see Used By: {@link Camera.ColorRenderTarget#clearColorOption}, {@link RenderTargetProvider#clearColorOption}
*/
declare enum ClearColorOption {
    /**
    * The frame will not be cleared before being rendered to (draws over the previous frame).
    */
    None,
    /**
    * Use the Device Texture for the color color
    */
    Background,
    /**
    * The frame will be cleared with a color before being rendered to.
    */
    CustomColor,
    /**
    * The frame will be cleared with a texture before being rendered to.
    */
    CustomTexture
}

/**
* Clears depth in the drawing order.

* @deprecated
*/
declare class ClearDepth extends Visual {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Simulates and renders cloth visuals in a Lens.

* @remarks
* Handles the mesh data of cloth and prepares it for cloth simulation. Also controls all the parameters of the cloth simulator and colliders.

* @see [Cloth Simulation](https://developers.snap.com/lens-studio/features/physics/cloth-simulation)

* @see Used By: {@link ClothVisual#onInitialized}

* @example
* ```js
* // @input  Component.ClothVisual clothVisual
* // @input Asset.RenderMesh mesh
* // @input Asset.Material material

* var clothVisual = script.clothVisual;

* clothVisual.bendMode = ClothVisual.BendMode.Isometric;
* clothVisual.mesh = script.mesh;
* clothVisual.mainMaterial = script.material;
* clothVisual.gravity = new vec3(0, -100, 0);
* clothVisual.stretchStiffness = 0.1;
* clothVisual.bendStiffness = 0.1;
* clothVisual.friction = 0.1;
* clothVisual.repulsionEnabled = true;
* clothVisual.repulsionOffset = 0.1;
* clothVisual.repulsionStiffness = 0.1;
* clothVisual.repulsionFriction = 0.1;
* clothVisual.iterationsPerStep = 60;
* clothVisual.frameRate = 30;
* clothVisual.colliders = script.Colliders;
* clothVisual.debugModeEnabled = false;
* clothVisual.drawCollider = true;
* clothVisual.updateNormalsEnabled = true;
* clothVisual.mergeCloseVerticesEnabled = true;
* clothVisual.onInitialized = clothInitCallback;
* clothVisual.updatePriority = 0;

* function clothInitCallback(clothVisualArg) {
* 	clothVisualArg.resetSimulation();
* }
* ```
*/
declare class ClothVisual extends MaterialMeshVisual {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Adds a collider to the list of colliders.
    */
    addCollider(colliderComponent: ColliderComponent): void
    
    /**
    * Clears the colliders list.
    */
    clearColliders(): void
    
    /**
    * Returns all available vertex colors on cloth mesh.
    */
    getAllColors(): vec4[]
    
    /**
    * Returns the vertex color by vertex index.
    */
    getPointColorByIndex(index: number): vec4
    
    /**
    * Returns all the indices of vertices that are labeled by this color.
    */
    getPointIndicesByColor(color: vec4, colorMask: vec4b): number[]
    
    /**
    * Returns all the indices on the cloth mesh that are matching the color mask. Match means that the color has value on the channels which in colorMask is true.
    */
    getPointIndicesByMask(colorMask: vec4b): number[]
    
    /**
    * Gets binding SceneObject of the vertex.
    */
    getVertexBinding(index: number): SceneObject
    
    /**
    * Gets simulation settings of the vertex.
    */
    getVertexSettings(index: number): VertexSimulationSettings
    
    /**
    * Returns true if the Cloth Simulation feature is supported by the current device.
    */
    isHardwareSupported(): boolean
    
    /**
    * Returns true if the Cloth Simulation and resources are initialized. Always return false if device is not supported.
    */
    isInitialized(): boolean
    
    /**
    * Removes collider by its index and returns the removed collider.
    */
    removeColliderByIndex(index: number): ColliderComponent
    
    /**
    * Resets the cloth simulation.
    */
    resetSimulation(): void
    
    /**
    * Sets the binding SceneObject for the vertex.
    */
    setVertexBinding(index: number, bindingObj: SceneObject): void
    
    /**
    * Sets simulation settings of the vertex.
    */
    setVertexSettings(index: number, vertexSettings: VertexSimulationSettings): void
    
    /**
    * Select bend mode to use (Isometric bending/Linear bending).
    */
    bendMode: ClothVisual.BendMode
    
    /**
    * BendStiffness value.
    */
    bendStiffness: number
    
    /**
    * BendStiffness value weight on all the VertexSettings.
    */
    bendStiffnessVertexWeight: number
    
    /**
    * List of colliders assigned.
    */
    colliders: ColliderComponent[]
    
    /**
    * The influence of the external body mesh acting on the cloth visual.
    */
    externalBodyMeshWeight: number
    
    /**
    * Friction value.
    */
    friction: number
    
    /**
    * Friction value weight on all the VertexSettings.
    */
    frictionVertexWeight: number
    
    /**
    * Gravity force vector.
    */
    gravity: vec3
    
    /**
    * Number of simulation iterations to perform per each step. Higher number contributes to higher quality, but can be difficult for device performance.
    */
    iterationsPerStep: number
    
    /**
    * Mass value.
    */
    mass: number
    
    /**
    * Mass value weight on all the VertexSettings.
    */
    massVertexWeight: number
    
    /**
    * Control for the acceleration on motion of vertices in cloth simulation, by default set to 2000.
    */
    maxAcceleration: number
    
    /**
    * Whether to merge close vertices (Lens Studio might split vertices when loading FBX mesh). Changing this resets the simulation.
    */
    mergeCloseVerticesEnabled: boolean
    
    /**
    * Threshold of close vertices. Changing this value resets the simulation.
    */
    mergeCloseVerticesThreshold: number
    
    /**
    * Attached mesh.
    */
    mesh: RenderMesh
    
    /**
    * Function called when the ClothVisual is initialized.
    */
    onInitialized: (clothVisual: ClothVisual) => void
    
    /**
    * Indicates whether we will enable collision repulsion with collider models.
    */
    repulsionEnabled: boolean
    
    /**
    * Collision friction to dampen relative motion.
    */
    repulsionFriction: number
    
    /**
    * Indicates the offset we set when the cloth mesh is too close to the colliders.
    */
    repulsionOffset: number
    
    /**
    * Indicates the stiffness of repulsion when collision.
    */
    repulsionStiffness: number
    
    /**
    * Returns the modified simulated mesh which can be used in another {@link RenderMeshVisual} if the same simulated mesh is needed. Useful when creating effects that might require the same mesh to be rendered twice, such as with a mirror effect. Prevents the need to run a simulation twice.
    
    * @readonly
    */
    simulatedMesh: RenderMesh
    
    /**
    * StretchStiffness Value.
    */
    stretchStiffness: number
    
    /**
    * StretchStiffness Value weight on all the VertexSettings.
    */
    stretchStiffnessVertexWeight: number
    
    /**
    * Whether to update normals for the cloth mesh each frame in order to get reflection update.
    */
    updateNormalsEnabled: boolean
    
    /**
    * Creates a new instance of vertex simulation settings.
    */
    static createVertexSettings(): VertexSimulationSettings
    
}

declare namespace ClothVisual {
    /**
    * Cloth bend mode to use for cloth simulation.
    
    * @see Used By: {@link ClothVisual#bendMode}
    
    * @example
    * ```js
    * //@input  Component.ClothVisual clothVisual
    * clothVisual.bendMode = ClothVisual.BendMode.Isometric;
    * ```
    */
    enum BendMode {
        Isometric,
        Linear
    }

}

/**
* Options associated with the listValues method call.

* @see Used By: {@link CloudStore#listValues}, {@link LocationCloudStore#listValues}
* @see Returned By: {@link CloudStorageListOptions.create}
*/
declare class CloudStorageListOptions extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The position in the resulting list. Each time a list is requested, 10 entities are returned. If no cursor is provided, first 10 entities will be returned.
    */
    cursor: string
    
    /**
    * The scope of the listValues request. Required
    */
    scope: StorageScope
    
    /**
    * Creates the listValues options object for Cloud Storage
    */
    static create(): CloudStorageListOptions
    
}

/**
* Provides access to Cloud Storage capabilities.

* @see [Cloud Storage Overview](https://developers.snap.com/lens-studio/features/lens-cloud/lens-cloud-overview).

* @example
* ```
* // Check out Persistent Cloud Storage example in the Asset Library

* const cloudStorageOptions = CloudStorageOptions.create();

* script.cloudStorageModule.getCloudStore(
*     cloudStorageOptions,
*     onCloudStoreReady,
*     onError
* );

* function onCloudStoreReady(store) {

*     // Write
*     const writeOptions = CloudStorageWriteOptions.create();
*     writeOptions.scope = StorageScope.User;

*     store.setValue(
*         "myKey",
*         "myValue",
*         writeOptions,
*         function onSuccess() {
*             print("Stored Succesfully!");

*             // List
*             const listOptions = CloudStorageListOptions.create();
*             listOptions.scope = StorageScope.User;

*             store.listValues(
*                 listOptions,
*                 function(results, cursor) {
*                     // Results are returned as a list of [key, value] tuples
*                     for (var i = 0; i < results.length; ++i) {
*                         var key = results[i][0];
*                         var value = results[i][1];
*                         print(' - key: ' + key + ' value: ' + value);
*                     }
*                 },
*                 onError
*             );

*         },
*         onError
*     );
* }

* function onError(code, message) {
*     print('Error: ' + code + ' ' + message);
* }
* ```
*/
declare class CloudStorageModule extends Asset {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Get the Cloud Store. Future calls to this method will return the same cloud store, even if the options change.
    */
    getCloudStore(options: CloudStorageOptions, onCloudStoreReady: (store: CloudStore) => void, onError: (code: string, description: string) => void): void
    
}

/**
* Used to configure `Cloud Storage Module` with various options. Note: if `session` scoped storage is required, this option must be provided.

* @see Used By: {@link CloudStorageModule#getCloudStore}
* @see Returned By: {@link CloudStorageOptions.create}
*/
declare class CloudStorageOptions extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * If session scoped storage is required, set this property with the session object from {@link ConnectedLensModule}.
    */
    session: MultiplayerSession
    
    /**
    * Create options for use with Cloud Storage.
    */
    static create(): CloudStorageOptions
    
}

/**
* Options associated with the getValue/deleteValue methods for Cloud Storage.

* @see Used By: {@link CloudStore#deleteValue}, {@link CloudStore#getValue}, {@link LocationCloudStore#deleteValue}, {@link LocationCloudStore#getValue}
* @see Returned By: {@link CloudStorageReadOptions.create}
*/
declare class CloudStorageReadOptions extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The scope of the request.
    */
    scope: StorageScope
    
    /**
    * Options associated with the getValue/deleteValue methods for Cloud Storage.
    */
    static create(): CloudStorageReadOptions
    
}

/**
* Options associated with the setValue method for Cloud Storage.

* @see Used By: {@link CloudStore#setValue}, {@link LocationCloudStore#setValue}
* @see Returned By: {@link CloudStorageWriteOptions.create}
*/
declare class CloudStorageWriteOptions extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The scope of the write option.
    */
    scope: StorageScope
    
    /**
    * Creates the setValue options object for Cloud Storage.
    */
    static create(): CloudStorageWriteOptions
    
}

/**
* An instance of Cloud Storage that can store data in a multiplayer experience.

* @see Used By: {@link CloudStorageModule#getCloudStore}
*/
declare class CloudStore extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Deletes a value from the persistence backend. Note that the scope must match that which was used when the value was originally saved.
    */
    deleteValue(key: string, readOptions: CloudStorageReadOptions, onDeleted: () => void, onError: (code: string, description: string) => void): void
    
    /**
    * Gets a value from the persistence backend. Note that scope must match that which was used when the value was originally saved.
    */
    getValue(key: string, readOptions: CloudStorageReadOptions, onRetrieved: (key: string, value: (mat4|mat3|mat2|vec4|vec2|vec3|boolean|quat|number|string)) => void, onError: (code: string, description: string) => void): void
    
    /**
    * Lists values from the persistence backend. Note that the scope must match that which was used when the values were originally saved.
    */
    listValues(listOptions: CloudStorageListOptions, onRetrieved: (values: (mat4|mat3|mat2|vec4|vec2|vec3|boolean|quat|number|string)[][], cursor: string) => void, onError: (code: string, description: string) => void): void
    
    /**
    * Sets a value in the persistence backend.
    */
    setValue(key: string, value: (mat4|mat3|mat2|vec4|vec2|vec3|boolean|quat|number|string), writeOptions: CloudStorageWriteOptions, onSaved: () => void, onError: (code: string, description: string) => void): void
    
}

/**
* Used to define the physical boundaries of an object, allowing it to interact with other objects in Physics, Cloth or Hair simulation.

* @remarks
* Provides a way for scene objects to detect and respond to collisions. Useful for detecting when objects overlap or contact each other, which can then be used to trigger events or effects within the Lens experience.

* @see [Collision and Overlap](https://developers.snap.com/lens-studio/features/physics/collision-and-overlap).
* @see [Physics Examples](https://developers.snap.com/lens-studio/features/physics/physics-examples/physics).
* @see [Cloth Simulation](https://developers.snap.com/lens-studio/features/physics/cloth-simulation)

* @see Used By: {@link BodyComponent#addPointConstraint}, {@link ClothVisual#addCollider}, {@link Collision#collider}, {@link ConstraintComponent#target}, {@link HairVisual#addCollider}, {@link Overlap#collider}, {@link RayCastHit#collider}
* @see Returned By: {@link ClothVisual#removeColliderByIndex}, {@link HairVisual#removeColliderByIndex}

* @example
* ```js
* // Add a callback for onCollisionEnter on this collider

* // @input Physics.ColliderComponent collider

* script.collider.onCollisionEnter.add(function(eventArgs) {
*     var collision = eventArgs.collision;
*     print("CollisionEnter(" + collision.id + "): contacts=" + collision.contactCount + " ---> " + collision.collider.getSceneObject().name);
* });
* ```
*/
declare class ColliderComponent extends Component {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Clears all velocities and forces on a collider.
    */
    clearMotion(): void
    
    /**
    * Angular velocity, expressed as an axis of rotation scaled by angular speed (radians/s).
    */
    angularVelocity: vec3
    
    /**
    * Expects a LevelsetColliderAsset. Contains distance field data which describes a collider's shape. A selection of default Levelset assets can be found in Resource->Add->Hairstyle Colliders
    
    * @deprecated
    */
    asset: LevelsetColliderAsset
    
    /**
    * Toggle collider wire rendering for visualizing collider geometry shape and where it is placed.
    */
    debugDrawEnabled: boolean
    
    /**
    * Collision filter to use for this collider.
    */
    filter: Physics.Filter
    
    /**
    * If enabled, the collider shape extends to fit the visual mesh, if any. Only applicable for Box and Sphere shapes.
    */
    fitVisual: boolean
    
    /**
    * Nested collider/body components may be merged into compound shapes. By default, this occurs only for dynamic bodies and not for static colliders. If `forceCompound` is set, this occurs for static colliders as well.
    */
    forceCompound: boolean
    
    /**
    * If enabled, the collider doesn't interact with the world but may still be detected with ray casts and intersection events.
    */
    intangible: boolean
    
    /**
    * The matter used by the collider to define its physical substance, such as friction and bounciness.
    */
    matter: Matter
    
    /**
    * Signals when objects initially collide.
    
    * @readonly
    */
    onCollisionEnter: event1<CollisionEnterEventArgs, void>
    
    /**
    * Signals when objects stop colliding.
    
    * @readonly
    */
    onCollisionExit: event1<CollisionExitEventArgs, void>
    
    /**
    * Signals every frame while objects continue to collide.
    
    * @readonly
    */
    onCollisionStay: event1<CollisionStayEventArgs, void>
    
    /**
    * Signals when colliders first overlap.
    
    * @readonly
    */
    onOverlapEnter: event1<OverlapEnterEventArgs, void>
    
    /**
    * Signals when colliders stop overlapping.
    
    * @readonly
    */
    onOverlapExit: event1<OverlapExitEventArgs, void>
    
    /**
    * Signals every frame while colliders continue to overlap.
    
    * @readonly
    */
    onOverlapStay: event1<OverlapStayEventArgs, void>
    
    /**
    * Collision filter used for overlap events.
    */
    overlapFilter: Physics.Filter
    
    /**
    * Smoothing spring factor, for rotation.
    */
    rotateSmoothFactor: number
    
    /**
    * The Shape object used for collision.
    */
    shape: Shape
    
    /**
    * Smooth transform changes using a dampened spring. Useful to reduce motion noise. This only applies to the simulation and changes from outside the simulation. The scene object's transform is not affected. It has no effect for dynamic bodies.
    */
    smooth: boolean
    
    /**
    * Smoothing spring factor, for translation. This controls the restitution strength of the spring, so low values are smoother but lag more.
    */
    translateSmoothFactor: number
    
    /**
    * Linear velocity (cm/s).
    */
    velocity: vec3
    
    /**
    * The WorldSettingsAsset used by the collider to define the physics simulation settings.
    */
    worldSettings: Physics.WorldSettingsAsset
    
}

/**
* A state generated for ColliderComponent collision events.

* @see Used By: {@link CollisionEnterEventArgs#collision}, {@link CollisionExitEventArgs#collision}, {@link CollisionStayEventArgs#collision}
*/
declare class Collision extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Primitive shapes that physics objects interact with, such as spheres, boxes, and meshes, or compound shapes formed from multiple shapes. Used with Physics, Hair Simulation and Cloth Simulation.
    
    * @readonly
    */
    collider: ColliderComponent
    
    /**
    * The number of contact points in the collision.
    
    * @readonly
    */
    contactCount: number
    
    /**
    * Array of contacts in the collision.
    
    * @readonly
    */
    contacts: Contact[]
    
    /**
    * ID of the collision, unique for this collider.
    
    * @readonly
    */
    id: number
    
}

/**
* Args used for {@link ColliderComponent.onCollisionEnter}, which is triggered when a collision begins.

* @example
* ```js
* // Add a callback for onCollisionEnter on this collider

* // @input Physics.ColliderComponent collider

* script.collider.onCollisionEnter.add(function(eventArgs) {
*     var collision = eventArgs.collision;
*     print("CollisionEnter(" + collision.id + "): contacts=" + collision.contactCount + " ---> " + collision.collider.getSceneObject().name);
* });
* ```
*/
declare class CollisionEnterEventArgs extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Structure containing information about the current collision.
    
    * @readonly
    */
    collision: Collision
    
}

/**
* Args used for {@link ColliderComponent.onCollisionExit}, which is triggered when a collision ends.

* @example
* ```js
* // Add a callback for onCollisionExit on this collider

* // @input Physics.ColliderComponent collider

* script.collider.onCollisionExit.add(function(eventArgs) {
*     var collision = eventArgs.collision;
*     print("CollisionExit(" + collision.id + "): contacts=" + collision.contactCount + " ---> " + collision.collider.getSceneObject().name);
* });
* ```
*/
declare class CollisionExitEventArgs extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Structure containing information about the current collision.
    
    * @readonly
    */
    collision: Collision
    
}

/**
* Defines the physical boundaries of an object for collision detection.

* @remarks
* It represents the shape and form of a 3D object, allowing Physics engine to determine when and how objects interact with each other within a scene.

* @see Used By: {@link TriangleHit#mesh}
*/
declare class CollisionMesh extends Asset {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Args used for {@link ColliderComponent.onCollisionStay}, which is triggered every frame while a collision continues.

* @example
* ```js
* // Add a callback for onCollisionStay on this collider

* // @input Physics.ColliderComponent collider

* script.collider.onCollisionStay.add(function(eventArgs) {
*     var collision = eventArgs.collision;
*     print("CollisionEnter(" + collision.id + "): contacts=" + collision.contactCount + " ---> " + collision.collider.getSceneObject().name);
* });
* ```
*/
declare class CollisionStayEventArgs extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Structure containing information about the current collision.
    
    * @readonly
    */
    collision: Collision
    
}

/**
* Not usable from JS. Accessed via the Colocated Landmarks 2D Mesh resource. Contains the 2D keypoints when creating a Colocated map. Expanded by the material provided with template.
*/
declare class ColocatedLandmarks2DRenderObjectProvider extends ColocatedLandmarksRenderObjectProviderBase {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Not usable from JS. Accessed via the Colocated Landmarks 3D Mesh resource. Contains the 3D landmarks when creating a Colocated map. Expanded by the material provided with template.
*/
declare class ColocatedLandmarks3DRenderObjectProvider extends ColocatedLandmarksRenderObjectProviderBase {
    
    /** @hidden */
    protected constructor()
    
}

declare class ColocatedLandmarksRenderObjectProviderBase extends RenderObjectProvider {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Allows {@link SceneObject} to be tracked in a Connected Lens Experience.

* @remarks
* Creates Colocated Connected Lenses experiences by enabling the creation and tracking of a shared space which can be used to place several users in the same coordinate frame. This shared space will be made available and can be tracked by any friend you invite to join your session via Snapcode. Users are expected to be located in the same room when using the colocated feature. This component needs to be attached to the camera.

* @see [Connected Lenses Overview](https://developers.snap.com/lens-studio/features/connected-lenses/connected-lenses-overview#remote-and-colocated)
*/
declare class ColocatedTrackingComponent extends Component {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Joins an existing session, retrieving the shared space that the colocated session initiator just created. Throws an exception if a join or build operation is in progress or if a shared space is already present.
    */
    join(session: MultiplayerSession): void
    
    /**
    * Starts the shared space building process locally in the session initiator's device.
    
    * If a session is provided, the shared space is placed into the session after building has completed, which is required for Colocated experiences running on mobile devices. When running the experience in Lens Studio Preview or solo mode the session is not required.
    
    * Throws an exception if a join or build operation is in progress or if a map is already present.
    */
    startBuilding(session: MultiplayerSession): void
    
    /**
    * Provides the shared space building progress expressed in values 0-1. These values can be used to populate a progress bar. Once this value has reached 1, the colocated tracking component attempts to share the space which the user created.
    
    * @readonly
    */
    buildingProgress: number
    
    /**
    * Indicates whether a shared space can be built on this device. Colocated Tracking is available on devices with ARKit/ARCore in the rear camera. This flag should be checked before attempting to build or join a session. If it is false, you should inform users: "Sorry, your device does not support shared AR experiences."
    
    * @readonly
    */
    canBuild: boolean
    
    /**
    * Indicates whether a shared space is present and ready to be tracked.
    
    * @readonly
    */
    canTrack: boolean
    
    /**
    * Indicates whether a shared space building operation is in progress. Once the flag is false, the shared space still needs to be shared. Use `onTrackingAvailable` to determine when your space has been shared.
    
    * @readonly
    */
    isBuilding: boolean
    
    /**
    * Indicates whether a session joining operation is in progress.
    
    * @readonly
    */
    isJoining: boolean
    
    /**
    * Indicates whether a shared space is actively being tracked. This value will be true while tracking is active, either with or without the shared space in view. This property matches the equivalent one in `MarkerTrackingComponent`.
    
    * @readonly
    */
    isTracking: boolean
    
    /**
    * Event fired when the building operation fails (for example, sharing your space failed). Once `onTrackingAvailable` event is triggered, this event will not be triggered anymore.
    
    * @readonly
    */
    onBuildFailed: event0<void>
    
    /**
    * Event fired when a shared space starts being actively tracked. This property is an analogue of the `onMarkerFound` property in `MarkerTrackingComponent`.
    
    * @readonly
    */
    onFound: event0<void>
    
    /**
    * Event fired when a join operation completes but no shared space was found in the session.
    
    * @readonly
    */
    onJoinFailed: event0<void>
    
    /**
    * Event fired when a shared space stops being actively tracked. This property is an analogue of the `onMarkerLost` property in `MarkerTrackingComponent`.
    
    * @readonly
    */
    onLost: event0<void>
    
    /**
    * Event fired when a shared space has been shared or received and the device can attempt to start tracking.  If this event is not being triggered for a long period, the process will time out and `onBuildFailed` event will be triggered.
    
    * @readonly
    */
    onTrackingAvailable: event0<void>
    
}

/**
* Data type used for color values.

* @deprecated

* @example
* ```js
* var newTex = ProceduralTextureProvider.create(width, height, Colorspace.RGBA);
* ```
*/
declare enum Colorspace {
    /**
    * Color data has one value: Red
    
    * @deprecated
    */
    R,
    /**
    * Color data has 2 values: Red, Green
    
    * @deprecated
    */
    RG,
    /**
    * Color data has 4 values: Red, Green, Blue, Alpha
    
    * @deprecated
    */
    RGBA
}

/**
* The base class for all components. Components are attached to {@link SceneObject} and add various behaviors to it.

* @remarks
* For example, in the default scene, the `Camera Object` is a scene object which contains the {@link Camera} component to render the scene from the point of view of that object. You can add a {@link DeviceTracking} component onto the object, so that the {@link Transform} of that object is modified based on the device's movement.

* @see [Building Lenses](https://developers.snap.com/lens-studio/overview/building-your-first-lens/built-in-ar-effects) Guide.
* @see [Camera Overview](https://developers.snap.com/lens-studio/lens-studio-workflow/scene-set-up/camera) Guide.

* @example
* You can add a component to an object by selecting an object in the `Scene Hierarchy` panel, and pressing `+ Add Component` in the `Inspector` panel; or through script:

* ```js
* const currentObject =  script.sceneObject;
* const newImageComponent = currentObject.createComponent("Component.Image");
* ```

* You can access components on an object directyly through script:
* ```js
* const currentObject =  script.sceneObject;
* const firstImageOnObject = currentObject.getComponent("Component.Image");
* ```

* or creating an input for it:
* ```js
* // @input Component.image passedInComponent
* const imageComponent = script.passedInComponent;
* ```

* Component classes can be a child of other classes. For example, you can get a {@link ClothVisual} and {@link EyeColorVisual} by asking for the parent {@link MaterialMeshVisual}.

* ```js
* // Get the first MaterialMeshVisual on the object
* const someMaterialMeshVissual = currentObject.getComponent("Component.MaterialMeshVisual");

* // Confirm the type of visual that is needed
* if(someMaterialMeshVissual.isOfType("Component.ClothVisual")){
*     var mat = someMaterialMeshVissual.mainMaterial;
* }
* ```

* You can also write these scripts as TypeScript. Learn more in the [TypeScript guide](https://developers.snap.com/lens-studio/features/scripting/typescript).

* ```ts
* @component
* export class somethingWithImage extends BaseScriptComponent {
*   @input
*   imageComponent: Component.Image;

*   onAwake() {
*     let imageComponent = this.imageComponent;
*   }
* }
* ```
*/
declare class Component extends SerializableWithUID {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Destroys the component.
    */
    destroy(): void
    
    /**
    * Returns the SceneObject the component is attached to.
    */
    getSceneObject(): SceneObject
    
    /**
    * Returns the Transform this component is attached to.
    */
    getTransform(): Transform
    
    /**
    * If disabled, the Component will stop enacting its behavior.
    */
    enabled: boolean
    
    /**
    * The scene object this component is on.
    
    * @readonly
    */
    sceneObject: SceneObject
    
}

/**
* Enum used to define the quality of image or texture compression. Higher quality typically results in larger file sizes. Used with Base64.

* @see Used By: {@link Base64.encodeTextureAsync}

* @example
* ```js
* //@input Asset.Texture texture

* //@input int compressionQuality = 0 {"widget" : "combobox", "values" : [{"label" : "MaximumCompression", "value" : "0"}, {"label" : "LowQuality", "value" : "1"}, {"label" : "IntermediateQuality", "value" : "2"}, {"label" : "HighQuality", "value" : "3"}, {"label" : "MaximumQuality", "value" : "4"}]}

* //@input int encodingType = 0 {"widget" : "combobox", "values" : [{"label" : "Png", "value" : "0"}, {"label" : "Jpg", "value" : "1"}]}

* Base64.encodeTextureAsync(script.texture, onSuccess, onFailure, script.compressionQuality, script.encodingType)

* function onSuccess(encodedTexture) {
*     print("Encoded texture: " + encodedTexture)
* }

* function onFailure(error) {
*     print("Error: " + error)
* }
* ```
*/
declare enum CompressionQuality {
    /**
    * Optimizes for the smallest size, often sacrificing visual fidelity.
    */
    MaximumCompression,
    /**
    * Prioritizes smaller file size over quality.
    */
    LowQuality,
    /**
    * Balances quality and file size.
    */
    IntermediateQuality,
    /**
    * Retains more details than lower settings.
    */
    HighQuality,
    /**
    * Provides the highest quality, with the largest file size.
    */
    MaximumQuality
}

/**
* A cone collision shape.

* @see Returned By: {@link Shape.createConeShape}
*/
declare class ConeShape extends Shape {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Lengthwise local axis along which the cone is oriented.
    */
    axis: Axis
    
    /**
    * The length of the cone.
    */
    length: number
    
    /**
    * The radius of the cone.
    */
    radius: number
    
}

/**
* Event fired when the "Launch connected lens" button was pressed. Wait for this event to be triggered before creating a session, as having this event gaurantees the user has accepted the necessary disclosures to use a connected lens experience.

* @example
* ```js
* script.createEvent("ConnectedLensEnteredEvent").bind(function () {
*    print("Connected lens entered");
* });
* ```
*/
declare class ConnectedLensEnteredEvent extends SceneEvent {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Enables the creation and management of networked Lens experiences, allowing for real-time communication and interaction between users through [Connected Lenses](https://developers.snap.com/lens-studio/features/connected-lenses/connected-lenses-overview).

* @remarks
* Allows use of networked Lens communication capabilities such as real-time communication, co-located session creation and joining, and shared persistent storage.
* It's recommended to only use one ConnectedLensModule per Lens.

* @see [Connected Lenses Overview](https://developers.snap.com/lens-studio/features/connected-lenses/connected-lenses-overview).

* @example
* ```js
* function createSession() {
*     var options = ConnectedLensSessionOptions.create();

*     options.onSessionCreated = onSessionCreated;
*     options.onConnected = onConnected;
*     options.onDisconnected = onDisconnected;
*     options.onMessageReceived = onMessageReceived;
*     options.onUserJoinedSession = onUserJoinedSession;
*     options.onUserLeftSession = onUserLeftSession;
*     options.onError = onError;

*     script.connectedLensModule.createSession(options);
* }
* ```
*/
declare class ConnectedLensModule extends Asset {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Create session with the provided options. Will also check if there is a session that can be created from a session sharetype received from other users.
    */
    createSession(sessionOptions: ConnectedLensSessionOptions): void
    
    /**
    * Share the session with other users, specified by the SessionShareType. Note that if shared via Invitation, a new session will be created. Expect a new onConnected callback with the new session being passed in.
    */
    shareSession(sessionShareType: ConnectedLensModule.SessionShareType, onSessionShared: (session: MultiplayerSession, snapcode: Texture) => void): void
    
}

declare namespace ConnectedLensModule {
    /**
    * Information that is bootstrapped to the user who just connected to the session.
    
    * @see Used By: {@link BaseMultiplayerSessionOptions#onConnected}
    */
    class ConnectionInfo extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * Get the information about all the other users connected to the same session.
        
        * @readonly
        */
        externalUsersInfo: ConnectedLensModule.UserInfo[]
        
        /**
        * @readonly
        */
        hostUserInfo: ConnectedLensModule.UserInfo
        
        /**
        * Get the information about the local user.
        
        * @readonly
        */
        localUserInfo: ConnectedLensModule.UserInfo
        
        /**
        * Get all the Realtime Stores that are being used in the session.
        
        * @readonly
        */
        realtimeStores: GeneralDataStore[]
        
        /**
        * Provides creation info about every existing RealtimeStore.
        
        * @readonly
        */
        realtimeStoresCreationInfos: ConnectedLensModule.RealtimeStoreCreationInfo[]
        
    }

}

declare namespace ConnectedLensModule {
    /**
    * Information about the host update.
    
    * @see Used By: {@link BaseMultiplayerSessionOptions#onHostUpdated}
    */
    class HostUpdateInfo extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * Provides the server timestamp (in milliseconds) of when the host was updated.
        
        * @readonly
        */
        sentServerTimeMilliseconds: number
        
        /**
        * Information about the new host user.
        
        * @readonly
        */
        userInfo: ConnectedLensModule.UserInfo
        
    }

}

declare namespace ConnectedLensModule {
    /**
    * Provides extra context about a RealtimeStore's creation.
    
    * @see Used By: {@link BaseMultiplayerSessionOptions#onRealtimeStoreCreated}
    * @see Returned By: {@link MultiplayerSession#getRealtimeStoreInfo}
    */
    class RealtimeStoreCreationInfo extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * If true, ownership of the store can be claimed even if the store is already owned.
        
        * @readonly
        */
        allowOwnershipTakeOver: boolean
        
        /**
        * Provides the server timestamp (in milliseconds) of the last time the store was updated.
        
        * @readonly
        */
        lastUpdatedServerTimestamp: number
        
        /**
        * Provides the UserInfo of the current owner of the RealtimeStore. If the store is unowned, a UserInfo object with null fields will be returned.
        
        * @readonly
        */
        ownerInfo: ConnectedLensModule.UserInfo
        
        /**
        * The persistence setting that the store was created with.
        
        * @readonly
        */
        persistence: RealtimeStoreCreateOptions.Persistence
        
        /**
        * Provides the server timestamp (in milliseconds) of when the store was created.
        
        * @readonly
        */
        sentServerTimeMilliseconds: number
        
        /**
        * A string that can be used to identify the RealtimeStore.
        
        * @readonly
        */
        storeId: string
        
    }

}

declare namespace ConnectedLensModule {
    /**
    * Gives information about the Realtime Store delete operation.
    
    * @see Used By: {@link BaseMultiplayerSessionOptions#onRealtimeStoreDeleted}
    */
    class RealtimeStoreDeleteInfo extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * Get the information of the user who deleted the Store.
        
        * @readonly
        */
        deleterInfo: ConnectedLensModule.UserInfo
        
        /**
        * Provides the server timestamp (in milliseconds) of when the store was deleted.
        
        * @readonly
        */
        sentServerTimeMilliseconds: number
        
    }

}

declare namespace ConnectedLensModule {
    /**
    * Provides information about a key being removed from a RealtimeStore.
    
    * @see Used By: {@link BaseMultiplayerSessionOptions#onRealtimeStoreKeyRemoved}
    */
    class RealtimeStoreKeyRemovalInfo extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * Key of the property that was removed.
        
        * @readonly
        */
        key: string
        
        /**
        * User that removed the key.
        
        * @readonly
        */
        removerInfo: ConnectedLensModule.UserInfo
        
        /**
        * Provides the server timestamp (in milliseconds) of when the key was removed.
        
        * @readonly
        */
        sentServerTimeMilliseconds: number
        
        /**
        * The RealtimeStore that the key was removed from.
        
        * @readonly
        */
        store: GeneralDataStore
        
    }

}

declare namespace ConnectedLensModule {
    /**
    * Provides information about a RealtimeStore's ownership being updated.
    
    * @see Used By: {@link BaseMultiplayerSessionOptions#onRealtimeStoreOwnershipUpdated}
    */
    class RealtimeStoreOwnershipUpdateInfo extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * Provides the server timestamp (in milliseconds) of when the store ownership was updated.
        
        * @readonly
        */
        sentServerTimeMilliseconds: number
        
    }

}

declare namespace ConnectedLensModule {
    /**
    * Gives information about the Realtime Store update operation.
    
    * @see Used By: {@link BaseMultiplayerSessionOptions#onRealtimeStoreUpdated}
    */
    class RealtimeStoreUpdateInfo extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * Provides the server timestamp (in milliseconds) of when the store was updated.
        
        * @readonly
        */
        sentServerTimeMilliseconds: number
        
        /**
        * Get the information of the user who updated the store.
        
        * @readonly
        */
        updaterInfo: ConnectedLensModule.UserInfo
        
    }

}

declare namespace ConnectedLensModule {
    /**
    * Used by ConnectedLensModule to specify the session share type.
    
    * @see Used By: {@link ConnectedLensModule#shareSession}
    
    * @example
    * ```js
    * // @input Asset.ConnectedLensModule connectedLensModule
    
    * function shareSessionInvite() {
    *     script.connectedLensModule.shareSession(ConnectedLensModule.SessionShareType.Invitation, onSessionSharedInvite);
    * }
    
    * function shareSessionSnapcode() {
    *     script.connectedLensModule.shareSession(ConnectedLensModule.SessionShareType.Snapcode, onSessionSharedSnapcode);
    * }
    * ```
    */
    enum SessionShareType {
        /**
        * Share a session by inviting your friends, launching the "send to" screen.
        */
        Invitation,
        /**
        * Share session via Snapcode that your friends can scan.
        */
        Snapcode
    }

}

declare namespace ConnectedLensModule {
    /**
    * Provides information about a user in a Connected Lens session.
    
    * @see Used By: {@link BaseMultiplayerSessionOptions#onMessageReceived}, {@link BaseMultiplayerSessionOptions#onRealtimeStoreCreated}, {@link BaseMultiplayerSessionOptions#onRealtimeStoreOwnershipUpdated}, {@link BaseMultiplayerSessionOptions#onUserJoinedSession}, {@link BaseMultiplayerSessionOptions#onUserLeftSession}, {@link ConnectedLensModule.ConnectionInfo#hostUserInfo}, {@link ConnectedLensModule.ConnectionInfo#localUserInfo}, {@link ConnectedLensModule.HostUpdateInfo#userInfo}, {@link ConnectedLensModule.RealtimeStoreCreationInfo#ownerInfo}, {@link ConnectedLensModule.RealtimeStoreDeleteInfo#deleterInfo}, {@link ConnectedLensModule.RealtimeStoreKeyRemovalInfo#removerInfo}, {@link ConnectedLensModule.RealtimeStoreUpdateInfo#updaterInfo}, {@link MultiplayerSession#getLocalUserInfo}
    */
    class UserInfo extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * @readonly
        */
        connectionId: string
        
        /**
        * The current display name of the user.
        
        * @readonly
        */
        displayName: string
        
        /**
        * Provides the server timestamp (in milliseconds) that the user joined the session.
        
        * @readonly
        */
        joinServerTimeMilliseconds: number
        
        /**
        * A unique identifier for each participant of connected lens experience. It is unique per lens for each user.
        
        * @readonly
        */
        userId: string
        
    }

}

/**
* Settings for configuring a Connected Lens session.

* @see Used By: {@link ConnectedLensModule#createSession}
* @see Returned By: {@link ConnectedLensSessionOptions.create}

* @example
* ```js
* // @input Asset.ConnectedLensModule connectedLensModule
* function createSession() {
*     var options = ConnectedLensSessionOptions.create();

*     options.onSessionCreated = onSessionCreated;
*     options.onConnected = onConnected;
*     options.onDisconnected = onDisconnected;
*     options.onMessageReceived = onMessageReceived;
*     options.onUserJoinedSession = onUserJoinedSession;
*     options.onUserLeftSession = onUserLeftSession;
*     options.onError = onError;

*     script.connectedLensModule.createSession(options);
* }```
*/
declare class ConnectedLensSessionOptions extends DirectMultiplayerSessionOptions {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Defines maximum number of receipients that a user of connected lens can select while sharing a connected lens session via Invitation flow.
    */
    maxNumberOfInvitations: number
    
    /**
    * Function called when the session is created. The session creation type in this callback can be used to tell if the session is being created from scratch, or is being received from another user.
    */
    onSessionCreated: (session: MultiplayerSession, sessionCreationType: ConnectedLensSessionOptions.SessionCreationType) => void
    
    /**
    * Create a new ConnectedLensesOptions object.
    */
    static create(): ConnectedLensSessionOptions
    
}

declare namespace ConnectedLensSessionOptions {
    /**
    * Type of the created Connected Lens session.
    
    * @see Used By: {@link ConnectedLensSessionOptions#onSessionCreated}
    
    * @example
    * ```js
    * function onSessionCreated(session, sessionCreationType) {
    *     if (sessionCreationType == ConnectedLensSessionOptions.SessionCreationType.MultiplayerReceiver) {
    *         print("Session was joined via invite");
    *     }
    *     else if (sessionCreationType == ConnectedLensSessionOptions.SessionCreationType.New) {
    *          print("New session was created");
    *     }
    * }
    * ```
    */
    enum SessionCreationType {
        /**
        * New Session was created.
        */
        New,
        /**
        * Session was joined via invite.
        */
        MultiplayerReceiver,
        /**
        * A Connected Lens session with only the current user.
        */
        NewSoloMode
    }

}

/**
* Constraints body motion in configurable ways, for simulating physical objects such as joints and hinges.

* @see Used By: {@link ConstraintComponent#constraint}
* @see Returned By: {@link Physics.Constraint.create}

* @example
* ```js
* // Given two objects, with hangingBox below staticBox
* // apply a constraint on hangingBox so that it hangs on the staticBox
* // Try assigning a box object for each input!

* // @input SceneObject staticBox
* // @input SceneObject hangingBox

* // Setup the boxes to have physics
* var staticBox = script.staticBox;
* var staticBoxBody = staticBox.createComponent("Physics.BodyComponent");
* staticBoxBody.dynamic = false;

* var hangingBox = script.hangingBox;
* var hangingBoxBody = hangingBox.createComponent("Physics.BodyComponent");

* // Create a child object that we can set up as our hinge
* var hingeObj = global.scene.createSceneObject("hinge");
* hingeObj.setParent(hangingBox);

* // First we use the child object as a constraint
* var hingeConstraint = hingeObj.createComponent("Physics.ConstraintComponent");

* // Then we set up the constraint as a hinge
* hingeConstraint.debugDrawEnabled = true;
* hingeConstraint.constraint = Physics.Constraint.create(Physics.ConstraintType.Hinge);

* // Attach our hinge to another Physics body
* hingeConstraint.target = staticBoxBody;

* // Position the hinge
* hingeObj.getTransform().setLocalPosition(new vec3(-7.5, 7.5, 0))

* // Tell the system to recalculate the simulation based on the above parameters
* hingeConstraint.reanchorTarget();
* ```
*/
declare class Constraint extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The type of constraint that is applied.
    
    * @readonly
    */
    constraintType: Physics.ConstraintType
    
}

/**
* Used to apply specific restrictions on how a {@link SceneObject} with {@link BodyComponent} can move.

* @remarks
* This component allows developers to simulate certain types of mechanical connections or joints between objects.

* @see [Constrain Types](https://developers.snap.com/lens-studio/features/physics/physics-examples/physics#constraint-types) guide.

* @see Used By: {@link BodyComponent#removeConstraint}
* @see Returned By: {@link BodyComponent#addPointConstraint}

* @example
* ```js
* // Given two objects, with hangingBox below staticBox
* // apply a constraint on hangingBox so that it hangs on the staticBox
* // Try assigning a box object for each input!

* // @input SceneObject staticBox
* // @input SceneObject hangingBox

* // Setup the boxes to have physics
* var staticBox = script.staticBox;
* var staticBoxBody = staticBox.createComponent("Physics.BodyComponent");
* staticBoxBody.dynamic = false;

* var hangingBox = script.hangingBox;
* var hangingBoxBody = hangingBox.createComponent("Physics.BodyComponent");

* // Create a child object that we can set up as our hinge
* var hingeObj = global.scene.createSceneObject("hinge");
* hingeObj.setParent(hangingBox);

* // First we use the child object as a constraint
* var hingeConstraint = hingeObj.createComponent("Physics.ConstraintComponent");

* // Then we set up the constraint as a hinge
* hingeConstraint.debugDrawEnabled = true;
* hingeConstraint.constraint = Physics.Constraint.create(Physics.ConstraintType.Hinge);

* // Attach our hinge to another Physics body
* hingeConstraint.target = staticBoxBody;

* // Position the hinge
* hingeObj.getTransform().setLocalPosition(new vec3(-7.5, 7.5, 0))

* // Tell the system to recalculate the simulation based on the above parameters
* hingeConstraint.reanchorTarget();
* ```
*/
declare class ConstraintComponent extends Component {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The target is attached to the constraint by a fixed local-space matrix, calculated from the difference between the target's and the constraint's world-space transforms. This local-space matrix is generated on-load, or whenever the target is changed. Call this function to explicitly recalculate it for the current world-space transforms.
    */
    reanchorTarget(): void
    
    /**
    * Dictates constraint type and settings. Note, when setting this field it creates a copy of the constraint, rather than referencing it. So if you intend to modify the constraint after assigning it, you must do so on the component's constraint field, rather than the source constraint object.
    */
    constraint: Constraint
    
    /**
    * Show the constraint with debug-draw.
    */
    debugDrawEnabled: boolean
    
    /**
    * Reference to connected target collider. If null, constraint target is attached to a fixed world transform.
    */
    target: ColliderComponent
    
}

/**
* Contact point between two colliding objects.

* @see {@link ColliderComponent}
* @see {@link CollisionEnterEventArgs}
* @see {@link CollisionExitEventArgs}

* @example
* ```js
* // Print out information about each contact point when the collider enters a collision

* // @input Physics.ColliderComponent collider

* script.collider.onCollisionEnter.add(function(eventArgs) {
*     var collision = eventArgs.collision;
*     var contactCount = collision.contactCount;
*     var contacts = collision.contacts;
*     for (var i = 0; i < contactCount; ++i) {
*         var contact = contacts[i];
*         print("contact[" + i + "]: distance=" + contact.distance + ", impulse=" + contact.impulse);
*     }
* });
* ```
*/
declare class Contact extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Distance along the normal between the hit collider and this collider.
    
    * @readonly
    */
    distance: number
    
    /**
    * Impulse (kg*cm/s) applied along the normal in response to the collision.
    
    * @readonly
    */
    impulse: number
    
    /**
    * Normal on the hit collider.
    
    * @readonly
    */
    normal: vec3
    
    /**
    * Position on the hit collider.
    
    * @readonly
    */
    position: vec3
    
}

/**
* Base class for Texture Providers that crop an input texture.

* @example
* ```js
* //@input Asset.Texture screenCropTexture

* var cropProvider = script.screenCropTexture.control;
* aspect = cropProvider.inputTexture.control.getAspect();

* var cropRect = cropProvider.cropRect;

* var size = cropRect.getSize();
* if (aspect > 1) {
*     size.x = size.x / aspect;
* } else {
*     size.y = size.y * aspect;
* }
* cropRect.setSize(size);
* ```
*/
declare class CropTextureProvider extends TextureProvider {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Input texture to crop.
    */
    inputTexture: Texture
    
}

/**
* The Crypto class in our system serves a similar purpose to the web standard Crypto module, providing high-quality cryptographic operations crucial for secure application development. It offers methods for generating cryptographically secure UUIDs, random bytes, and SHA hashes. This class is designed to meet the stringent security requirements of applications handling authentication and sensitive transactions, complementing our existing framework by adding enhanced security features for robust and reliable cryptographic functionality.

* @example
* ```js
* // Step 1: Generate a state parameter and code verifier for PKCE
* const state = crypto.randomUUID();
* console.log('OAuth2 State:', state);

* const codeVerifierArray = new Uint8Array(32); // Secure random array
* crypto.getRandomValues(codeVerifierArray);
* const codeVerifier = Array.from(codeVerifierArray).map(b => b.toString(16).padStart(2, '0')).join('');
* console.log('Code Verifier:', codeVerifier);

* // Step 2: Create a code challenge from the code verifier (SHA-256 hash, Base64-URL encoded)
* const encoder = new TextEncoder();
* const data = encoder.encode(codeVerifier);

* try {
*   const hashArray = await crypto.subtle.digest('SHA-256', data);
*   const codeChallenge = btoa(String.fromCharCode.apply(null, hashArray))
*                           .replace(/\+/g, '-').replace(/\//g, '_').replace(/=+$/, '');
*   console.log('Code Challenge:', codeChallenge);

*   // Mock Authorization URL for User to Consent
*   const authUrl = `https://authorization.server/auth?response_type=code&client_id=YOUR_CLIENT_ID&scope=YOUR_SCOPES&state=${state}&code_challenge=${codeChallenge}&code_challenge_method=S256`;
*   console.log('Authorization URL:', authUrl);

*   // Note: The user is redirected to the `authUrl` to authorize and generate an authorization code.

*   // Step 3: Assume the authorization server redirects back with a code
*   const authorizationCode = 'example_auth_code'; // Placeholder for authorization server response

*   // Step 4: Exchange authorization code for access token
*   const tokenRequestBody = {
*     grant_type: 'authorization_code',
*     code: authorizationCode,
*     redirect_uri: 'https://your.redirection.uri/callback',
*     client_id: 'YOUR_CLIENT_ID',
*     code_verifier: codeVerifier,
*   };

*   // Mock token exchange request (simulate HTTP request)
*   // In practice, you would send a POST request to the token endpoint
*   console.log('Token Request Body:', JSON.stringify(tokenRequestBody, null, 2));

* } catch (error) {
*   console.error('Error during OAuth2 flow:', error);
* }
* ```
*/
declare class crypto {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Fills a provided Uint8Array with cryptographically secure random values. This method emulates `crypto.getRandomValues()` from the Web Crypto API.
    
    * `typedArray` A Uint8Array to fill with random values.
    
    * __Returns:__ Uint8Array The same array passed in after being filled with random values.
    */
    static getRandomValues(typedArray: Uint8Array): Uint8Array
    
    /**
    * Generates a cryptographically secure UUID. This method mimics `crypto.randomUUID()` from the Web Crypto API.
    
    * __Returns:__ A securely generated UUID string.
    */
    static randomUUID(): string
    
}

declare namespace crypto {
    /**
    * The `Crypto.subtle` read-only property returns an object which can then be used to perform low-level cryptographic operations.
    */
    class subtle {
        
        /** @hidden */
        protected constructor()
        
        /**
        * Generates a cryptographic hash of the given data using the specified algorithm. This method replicates `crypto.subtle.digest()` from the Web Crypto API, suitable for SHA and other algorithms.
        
        * `algorithm` The hash algorithm to use (e.g., "SHA1", "SHA256", "SHA384", "SHA512").
        
        * `data` The data to hash.
        
        * __Returns:__ A Promise that resolves to the hashed output as an Uint8Array.
        */
        static digest(algorithm: string, data: Uint8Array): Promise<Uint8Array>
        
    }

}

/**
* Used with {@link Pass}'s `cullMode` property.
* Determines which faces of a surface are culled (not rendered).

* @see Used By: {@link Pass#cullMode}, {@link PassWrapper#cullMode}

* @example
* ```js
* //@input Component.MeshVisual meshVisual

* var mainPass = script.meshVisual.mainPass;
* mainPass.cullMode = CullMode.Front;
* ```
*/
declare enum CullMode {
    /**
    * Front facing surfaces are not rendered.
    */
    Front,
    /**
    * Back facing surfaces are not rendered.
    */
    Back,
    /**
    * Neither front facing nor back facing surfaces are rendered.
    */
    FrontAndBack
}

/**
* A component managing and tracking a group of Custom Locations with known relative transforms. The {@link Transform} of the Custom Location Group's {@link SceneObject} is updated when any child is successfully tracked.
*/
declare class CustomLocationGroupComponent extends Component {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Provides a hint of the user's position relative to the Custom Location Group's origin and used to optimize tracking when this position is known.
    
    **Note:** You can set this value through the Inspector panel in Lens Studio, or by using this API. When setting by API, it will override any initially trackable location set (child Custom Locations of this group) within Lens Studio. However, the next time the Lens opens again, it will default to what was set in the Inspector panel of Lens Studio.
    */
    hintUserPosition(groupLocalPosition: vec3): void
    
    /**
    * Event fired when a child Custom Location successfully tracks for the first time. Event is fired with the ID of the newly tracking Custom Location.
    
    * @readonly
    */
    onFound: event1<string, void>
    
}

/**
* A cylinder collision shape.

* @see Returned By: {@link Shape.createCylinderShape}
*/
declare class CylinderShape extends Shape {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Lengthwise local axis along which the cylinder is oriented.
    */
    axis: Axis
    
    /**
    * The length of the cylinder.
    */
    length: number
    
    /**
    * The radius of the cylinder.
    */
    radius: number
    
}

/**
* Provides methods to draw primitive visuals for debugging. Accessible through `global.debugRenderSystem`.
*/
declare class DebugRender extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Clears any visual on the debug render system.
    */
    clear(): void
    
    /**
    * Draws a wireframe box.
    */
    drawBox(position: vec3, width: number, height: number, depth: number, color: vec4): void
    
    /**
    * Draw a sequence of unconnected line segments (2 points per line).
    */
    drawBrokenLine(points: vec3[], color: vec4): void
    
    /**
    * Draw a wireframe circle oriented on the XY plane.
    */
    drawCircle(position: vec3, radius: number, color: vec4): void
    
    /**
    * Draw a single line segment.
    */
    drawLine(posA: vec3, posB: vec3, color: vec4): void
    
    /**
    * Draws a solid box mesh.
    */
    drawSolidBox(position: vec3, width: number, height: number, depth: number, color: vec4): void
    
    /**
    * Draws a solid sphere mesh.
    */
    drawSolidSphere(position: vec3, radius: number, color: vec4): void
    
    /**
    * Draws a filled in triangle.
    */
    drawSolidTriangle(vertex1: vec3, vertex2: vec3, vertex3: vec3, color: vec4): void
    
    /**
    * Draws a wireframe sphere as a 3 axis-aligned circle.
    */
    drawSphere(position: vec3, radius: number, color: vec4): void
    
    /**
    * Currently unused.
    */
    isAutoClear: boolean
    
}

/**
* The DeepLinkModule brings the concept of deep linking, familiar from mobile platforms like Android and iOS, to Spectacles. This module allows Lenses to send and receive deep link URIs between Spectacles and the Spectacles App.

* @wearableOnly

* @example
* ```js
* const deepLink = require("LensStudio:DeepLinkModule");

* // Configuration for Google OAuth2
* const clientId = "YOUR_GOOGLE_CLIENT_ID";
* const redirectUri = "YOUR_APP_REDIRECT_URI"; // Make sure this is registered in Google developer console
* const scope = "https://www.googleapis.com/auth/userinfo.profile"; // Adjust scope as needed

* // Register an event handler to handle when a deep link URI is received
* deepLink.onUriReceived.add((event) => {
*     console.log(`Deep link received: ${event.uri}`);

*     // Extract the authorization code using string manipulation
*     const queryString = event.uri.split('?')[1];
*     const params = queryString.split('&');
*     let authorizationCode = null;

*     for (const param of params) {
*         const [key, value] = param.split('=');
*         if (key === 'code') {
*             authorizationCode = decodeURIComponent(value);
*             break;
*         }
*     }

*     if (authorizationCode) {
*         console.log(`Authorization code received: ${authorizationCode}`);

*         // Exchange authorization code for access token
*         exchangeAuthorizationCodeForAccessToken(authorizationCode);
*     } else {
*         console.error("Authorization code not found in URI");
*     }
* });

* // Function to start the OAuth2 flow by opening a deep link to Google's OAuth2 page
* function authenticateWithGoogle() {
*     const oauthUri = `https://accounts.google.com/o/oauth2/v2/auth` +
*                      `?response_type=code` +
*                      `&client_id=${encodeURIComponent(clientId)}` +
*                      `&redirect_uri=${encodeURIComponent(redirectUri)}` +
*                      `&scope=${encodeURIComponent(scope)}`;

*     deepLink.openUri(oauthUri)
*        .then(() => {
*             console.log('Deep link opened successfully');
*         })
*         .catch((error) => {
*             console.error('Failed to open deep link:', error);
*         });
* }

* // Placeholder function for exchanging authorization code for access token
* function exchangeAuthorizationCodeForAccessToken(code: string) {
*     console.log(`Exchanging authorization code for access token: ${code}`);

*     // Implement server-side logic to exchange code for access token
*     // Send a POST request to Google's token endpoint with `code`, `clientId`, `redirectUri`, and `clientSecret`
* }

* // Call this function to begin OAuth2 authentication with Google
* authenticateWithGoogle();
* ```
*/
declare class DeepLinkModule extends Asset {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Initiates a deep link request by sending a specified URI from the Lens to SnapOS, allowing dynamic interaction based on the URIs target action or content.
    
    * @wearableOnly
    */
    openUri(uri: string): Promise<void>
    
    /**
    * An event triggered when a deep link request is received by the Lens from SnapOS, enabling the lens to process and respond to the incoming action or content linked by the URI.
    
    * @readonly
    
    * @wearableOnly
    */
    onUriReceived: event1<DeepLinkUriReceivedArgs, void>
    
}

/**
* Arguments used with the {@link DeepLinkModule#onUriReceived} event.

* @wearableOnly
*/
declare class DeepLinkUriReceivedArgs extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The URI string representing the deep link that has been received.
    
    * @readonly
    
    * @wearableOnly
    */
    uri: string
    
}

/**
* Used for collision meshes that can change shape or form dynamically.
*/
declare class DeformingCollisionMesh extends CollisionMesh {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Creates a buffer for the audio data.

* @see Returned By: {@link DelayBuilder#build}

* @example
* ```js
* // @input int sampleRate
* // @input float delayInSeconds

* var delayBuilder = MachineLearning.createDelayBuilder();
* var delay = delayBuilder.setNumFeatures(1)
*     .setDelay(script.sampleRate * script.delayInSeconds).build();
* ```
*/
declare class Delay extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Process current frame data passed in `inTensor` of shape `inShape`, writes the result (delayed frame) to the `outTensor` and returns the shape of `outTensor`.
    */
    process(inTensor: Float32Array, inShape: vec3, outTensor: Float32Array): vec3
    
    /**
    * Maximum amount of features (channels).
    
    * @readonly
    */
    maxTensorSize: number
    
}

/**
* Builder class for the Delay.

* @see Returned By: {@link DelayBuilder#setDelay}, {@link DelayBuilder#setNumFeatures}, {@link MachineLearning.createDelayBuilder}

* @example
* ```js
* var delayBuilder = MachineLearning.createDelayBuilder();
* ```
*/
declare class DelayBuilder extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Creates new Delay object.
    */
    build(): Delay
    
    /**
    * Set the delay of the Delay object in samples. Delay is equal to `sampleRate * delayInSeconds`.
    */
    setDelay(delay: number): DelayBuilder
    
    /**
    * Specify number of channels. Currently, only mono is supported, so should be set to `1`.
    */
    setNumFeatures(numFeatures: number): DelayBuilder
    
}

/**
* An event that gets triggered after a delay.

* @example
* ```js
* // Wait for 2 seconds before executing a function
* var delayedEvent = script.createEvent("DelayedCallbackEvent");
* delayedEvent.bind(function(eventData)
* {
*     print("delay is over");
* });

* // Start with a 2 second delay
* delayedEvent.reset(2);
* print("delay has started");
* ```
*/
declare class DelayedCallbackEvent extends SceneEvent {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Cancel the callback in progress.
    */
    cancel(): void
    
    /**
    * Returns the total delay time in seconds set on the event.
    */
    getDelayTime(): number
    
    /**
    * Returns the current time in seconds left in the event's delay.
    */
    getTimeLeft(): number
    
    /**
    * Calling this will cause the event to trigger in `time` seconds.
    */
    reset(time: number): void
    
}

/**
* Local estimate of the derivative of the input data along the selected axis. Outputs the derivative of the input features along the window.

* @see Returned By: {@link DeltaBuilder#build}

* @example
* ```js
* var deltaBuilder = MachineLearning.createDeltaBuilder();
* var delta = deltaBuilder.setNumFeatures(numFeatures).setWindowSize(windowSize).build();
* ```
*/
declare class Delta extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Process `inTensor` with shape `inShape` and writes the result to the `outTensor` and returns the shape of `outTensor`.
    */
    process(inTensor: Float32Array, inShape: vec3, outTensor: Float32Array): vec3
    
    /**
    * Maximum amount of features (channels).
    
    * @readonly
    */
    maxTensorSize: number
    
}

/**
* Builder class for Delta.

* @see Returned By: {@link DeltaBuilder#setNumFeatures}, {@link DeltaBuilder#setWindowSize}, {@link MachineLearning.createDeltaBuilder}

* @example
* ```js
* var deltaBuilder = MachineLearning.createDeltaBuilder();
* var delta = deltaBuilder.setNumFeatures(numFeatures).setWindowSize(windowSize).build();
* ```
*/
declare class DeltaBuilder extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Create a new Delta object.
    */
    build(): Delta
    
    /**
    * Specify number of channels. Currently, only mono is supported, so should be set to `1`.
    */
    setNumFeatures(numFeatures: number): DeltaBuilder
    
    /**
    * Set the size of the window.
    */
    setWindowSize(winSize: number): DeltaBuilder
    
}

/**
* Settings for the depth clear option modes on a DepthStencilRenderTargetProvider.

* @see Used By: {@link Camera.DepthStencilRenderTarget#depthClearOption}, {@link DepthStencilRenderTargetProvider#depthClearOption}
*/
declare enum DepthClearOption {
    /**
    * Do not clear the depth buffer, just keep values. Equal to disabled clear depth checkbox in Camera in previous Studio version.
    */
    None,
    /**
    * Clear depth buffer by specific value. Equal to enabled clear depth checkbox in Camera in previous Studio version.
    */
    CustomValue,
    /**
    * Clear depth buffer by values from depth part of input texture. Will use custom value in case of unavailable input texture.
    */
    CustomTexture
}

/**
* The depth frame data as provided by the {@link DepthFrameSession.onNewFrame}.

* @experimental

* @exposesUserData

* @wearableOnly

* @wearableOnly

* @example
* ```js
* const depthModule = require("LensStudio:DepthModule");

* const session = depthModule.createDepthFrameSession();

* session.onNewFrame.add((depthFrameData) => {
*     const depthDeviceCamera = depthFrameData.deviceCamera;

*     // Sample depth for specific pixel
*     const pixelCoordX = 112;
*     const pixelCoordY = 80;
*     const depthFrameArrayIdx = Math.floor(pixelCoordX + pixelCoordY * depthDeviceCamera.resolution.x);
*     const depthValue = depthFrameData.depthFrame[depthFrameArrayIdx];

*     // Back-project depth pixel to device reference space
*     const normalizedCoord = new vec2(pixelCoordX / depthDeviceCamera.resolution.x, pixelCoordY / depthDeviceCamera.resolution.y);
*     const pointInDeviceRef = depthDeviceCamera.unproject(
*         normalizedCoord,
*         depthValue
*     );

*     // Transform point in device reference space to world tracking origin space
*     const worldFromDeviceRef =
*       depthFrameData.toWorldTrackingOriginFromDeviceRef;
*     const pointInWorld = worldFromDeviceRef.multiplyPoint(pointInDeviceRef);
* });

* session.start();
* session.stop();
* ```
*/
declare class DepthFrameData extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The depth frame as a linear Float32Array in centimeters.
    
    * @readonly
    
    * @experimental
    
    * @exposesUserData
    
    * @wearableOnly
    
    * @wearableOnly
    */
    depthFrame: Float32Array
    
    /**
    * Provides information about the depth device camera.
    
    * @readonly
    
    * @experimental
    
    * @exposesUserData
    
    * @wearableOnly
    
    * @wearableOnly
    */
    deviceCamera: DeviceCamera
    
    /**
    * The timestamp of the start of exposure of the frame in seconds.
    
    * @readonly
    
    * @experimental
    
    * @exposesUserData
    
    * @wearableOnly
    
    * @wearableOnly
    */
    timestampSeconds: number
    
    /**
    * The pose of the device reference relative to the frame of reference of the tracked device position.
    
    * @readonly
    
    * @experimental
    
    * @exposesUserData
    
    * @wearableOnly
    
    * @wearableOnly
    */
    toWorldTrackingOriginFromDeviceRef: mat4
    
}

/**
* Used for receiving {@link DepthFrameData} on Spectacles device.

* @see Returned By: {@link DepthModule#createDepthFrameSession}

* @experimental

* @exposesUserData

* @wearableOnly

* @wearableOnly

* @example
* ```js
* const depthModule = require("LensStudio:DepthModule");

* const session = depthModule.createDepthFrameSession();

* session.onNewFrame.add((depthFrameData) => {
*     // Make use of depth frame data
* });

* session.start();
* session.stop();
* ```
*/
declare class DepthFrameSession extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Starts depth estimation for the {@link DepthFrameSession} on Spectacles. The {@link DepthFrameSession.onNewFrame} events will only be triggered after start has been called.
    
    * @experimental
    
    * @exposesUserData
    
    * @wearableOnly
    
    * @wearableOnly
    */
    start(): void
    
    /**
    * Stops depth estimation for the {@link DepthFrameSession} on Spectacles.
    
    * @experimental
    
    * @exposesUserData
    
    * @wearableOnly
    
    * @wearableOnly
    */
    stop(): void
    
    /**
    * Register a callback to be called whenever new {@link DepthFrameData} is available. On Spectacles, the depth update rate is different from the color frame update rate and the render update rate.
    
    * @readonly
    
    * @experimental
    
    * @exposesUserData
    
    * @wearableOnly
    
    * @wearableOnly
    */
    onNewFrame: event1<DepthFrameData, void>
    
}

/**
* Provides access to a {@link DepthFrameSession} on Spectacles device.

* @remarks
* Used for requesting {@link DepthFrameData}.

* @see [Depth Module](https://developers.snap.com/spectacles/about-spectacles-features/apis/depth-module) guide.

* @experimental

* @exposesUserData

* @wearableOnly

* @wearableOnly

* @example
* ```js
* const depthModule = require("LensStudio:DepthModule");

* const session = depthModule.createDepthFrameSession();
* ```
*/
declare class DepthModule extends Asset {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Create a {@link DepthFrameSession}.
    
    * @experimental
    
    * @exposesUserData
    
    * @wearableOnly
    
    * @wearableOnly
    */
    createDepthFrameSession(): DepthFrameSession
    
}

/**
* Writes video feed depth information to the depth buffer, which automatically sets up depth occlusion for 3D visuals.

* @remarks
* Only works in some cases where depth information is supplied by the device.

* @deprecated
*/
declare class DepthSetter extends PostEffectVisual {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Access to a Depth Stencil Render Target that can output depth and stencil values from a Camera in Depth24/Stencil8 format.
*/
declare class DepthStencilRenderTargetProvider extends TextureProvider {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Float value in range [0.0..1.0] used in depth buffer clear operation in "CustomValue" mode. The initial value is 1.0.
    */
    clearDepthValue: number
    
    /**
    * Unsigned int value in range [0..0xFF] used in stencil buffer clear operation in "CustomValue" mode. The initial value is 0.
    */
    clearStencilValue: number
    
    /**
    * Depth buffer clear option.  "None" - depth buffer clear operation will be skipped.  "CustomValue" - depth buffer will be cleared by "clearDepthValue" property value. "CustomTexture" - depth buffer will be cleared by texture from "inputTexture" property, if "inputTexture" is null then depth clear option will fallback to "CustomValue" mode.
    */
    depthClearOption: DepthClearOption
    
    /**
    * Texture with Depth24_Stencil8 format. Depth24 part used in depth clear operation in "CustomTexture" mode.
    */
    inputTexture: Texture
    
    /**
    * Texture with Depth24_Stencil8 format. Stencil8 part used in stencil clear operation in "CustomTexture" mode.
    */
    maskTexture: Texture
    
    /**
    * Enable mipmaps on the current render target.
    */
    mipmapsEnabled: boolean
    
    /**
    * The ouput resolution of the current render target.
    */
    outputResolution: number
    
    /**
    * Custom render target resolution, this property will use if the "outputResolution" property is a "Custom".
    */
    resolution: vec2
    
    /**
    * Stencil buffer clear option.  "None" - stencil buffer clear operation will be skipped.  "CustomValue" - stencil buffer will be cleared by "clearStencilValue" property value. "CustomTexture" - stencil buffer will be cleared by texture from "maskTexture" property, if "maskTexture" is null then the stencil clear option will fallback to "CustomValue" mode.
    */
    stencilClearOption: StencilClearOption
    
}

/**
* Provides depth information of the video feed that the Lens is being applied to when available.
* Can be accessed from `mainPass.baseTex.control` of a Spectacles Depth material.

* @example
* ```js
* // Get a reference to the Spectacles Depth material, set in the Inspector panel
* // @input Asset.Material depthMaterial

* // Bind an event to run every frame
* script.createEvent("UpdateEvent").bind(function()
* {
*     // The point at the center of the screen
*     var screenPosition = new vec2(0.5, 0.5);

*     // Get the depth at the screen position
*     var depth = script.depthMaterial.mainPass.baseTex.control.getDepth(screenPosition);

*     // Print the depth value
*     print (depth);
* });
* ```
*/
declare class DepthTextureProvider extends TextureProvider {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Get the depth at the given `point`.
    
    * @exposesUserData
    */
    sampleDepthAtPoint(point: vec2): number
    
}

/**
* Provides information about the device's camera.

* @see Used By: {@link DepthFrameData#deviceCamera}
* @see Returned By: {@link DeviceInfoSystem#getTrackingCamera}, {@link DeviceInfoSystem#getTrackingCameraForId}

* @example
* ```js
* print(global.deviceInfoSystem.getTrackingCamera().principalPoint);
* ```
*/
declare class DeviceCamera extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Given a point in device reference space, first converts the point to 3d camera space, using extrinsics, and then projects it to produce a point in normalized screen space (origin at top left).
    */
    project(pointInDeviceReferenceNode: vec3): vec2
    
    /**
    * Unprojects the normalized screen space point `normalizedScreenSpacePoint` based on some distance `absoluteDepth` to produce a 3d position in device reference space. On Spectacles, this means that the returned position already includes the offset between the device center and the camera.
    */
    unproject(normalizedScreenSpacePoint: vec2, absoluteDepth: number): vec3
    
    /**
    * Provides the focal length of the device camera in pixels.
    
    * @readonly
    */
    focalLength: vec2
    
    /**
    * Provides the relative offset between a reference point on the device and the camera. On mobile, the reference point is same as the camera, so this transform is identity.
    
    * On Spectacles, the reference point is the device center and the transform converts points in camera space to points relative to device center. Together with pose from DeviceTracking, you can obtain the world positions of points in device camera space.
    
    * For example,
    * ```
    * // @input SceneObject objectWithDeviceTracking
    
    * // Get device camera
    * const deviceCamera = global.deviceInfoSystem.getTrackingCamera();
    * // calculate 3d position in device camera space (using your custom code which uses snapml or a similar technique)
    * const pointInDeviceCamera = ...; // vec3
    * const pointInDeviceRef = deviceCamera.pose.multiplyPoint(pointInDeviceCamera);
    
    * const deviceWorldTransform = script.objectWithDeviceTracking.getTransform().getWorldTransform();
    * const pointInWorld = deviceWorldTransform.multiplyPoint(pointInDeviceRef);
    * // Use pointInWorld. For example, place a scene object at this world position
    * ```
    
    * @readonly
    */
    pose: mat4
    
    /**
    * Provides the principal point of the device camera in pixels with origin at the center of the top-left pixel. Note: Principal point is typically in the center of the image, but it may not always be the case.
    
    * @readonly
    */
    principalPoint: vec2
    
    /**
    * Provides the default resolution of the image returned by the device camera in pixels. The principal point and focal length are calculated relative to this resolution. If you resize the image to a different resolution than the default, then you also need to update the principal points and focal length appropriately.
    
    * @readonly
    */
    resolution: vec2
    
}

/**
* Provides information about the device running the Lens. Accessible through `global.deviceInfoSystem`.

* @example
* ```js
* var osType = global.deviceInfoSystem.getOS();
* var isMobile = (osType == OS.iOS || osType == OS.Android);
* ```
*/
declare class DeviceInfoSystem extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns the operating system type of the device.
    
    * @exposesUserData
    */
    getOS(): OS
    
    /**
    * Provides the tracking camera's {@link DeviceCamera}.
    */
    getTrackingCamera(): DeviceCamera
    
    /**
    * Get the DeviceCamera object for the given camera ID which provides intrinsics/extrinsics of the camera.
    
    * @wearableOnly
    */
    getTrackingCameraForId(cameraId: CameraModule.CameraId): DeviceCamera
    
    /**
    * Returns whether the current Lens is running in CameraKit.
    */
    isCameraKit(): boolean
    
    /**
    * Returns whether the current Lens is running in a desktop computer.
    */
    isDesktop(): boolean
    
    /**
    * Returns whether the current Lens is running in Lens Studio.
    */
    isEditor(): boolean
    
    /**
    * Returns true if the device has access to the internet.
    
    * Example js
    * ```js
    * // @input Component.Text textObject
    
    * script.textObject.text = global.deviceInfoSystem.isInternetAvailable()
    *     ? "Internet is available"
    *     : "No internet";
    * ```
    
    * Example ts
    * ```ts
    * @component
    * export class NewScript extends BaseScriptComponent {
    *   @input textObject: Text;
    
    *   onAwake() {
    *     this.textObject.text = global.deviceInfoSystem.isInternetAvailable()
    *       ? "Internet is available"
    *       : "No internet";
    *   }
    * }
    * ```
    */
    isInternetAvailable(): boolean
    
    /**
    * Returns whether the current Lens is running in a mobile device.
    */
    isMobile(): boolean
    
    /**
    * Returns whether the current Lens is running in a Spectacles device.
    */
    isSpectacles(): boolean
    
    /**
    * Accepts a callback which indicates whether the current device is capable of providing both front and rear camera texture simultaneously.
    */
    supportsDualCamera(callback: (supportsDualCamera: boolean) => void): void
    
    /**
    * Triggered when internet availability changed.
    
    * Example js
    * ```js
    * // @input Component.Text textObject
    
    * global.deviceInfoSystem.onInternetStatusChanged.add(function(eventData) {
    *     script.textObject.text = eventData.isInternetAvailable
    *         ? "UPDATED: Internet is available"
    *         : "UPDATED: No internet";
    * });
    * ```
    
    * Example ts
    * ```ts
    * @component
    * export class NewScript extends BaseScriptComponent {
    *   @input textObject: Text;
    
    *   onAwake() {
    *     this.textObject.text = global.deviceInfoSystem.isInternetAvailable()
    *       ? "Internet is available"
    *       : "No internet";
    
    *     global.deviceInfoSystem.onInternetStatusChanged.add((args) => {
    *       this.textObject.text = args.isInternetAvailable
    *         ? "UPDATED: Internet is available"
    *         : "UPDATED: No internet";
    *     });
    *   }
    * }
    * ```
    
    * @readonly
    
    * @wearableOnly
    */
    onInternetStatusChanged: event1<InternetStatusChangedArgs, void>
    
    /**
    * Specifies the device pixel ratio. Can be used to set rendering at the real screen resolution.
    
    * @readonly
    */
    screenScale: number
    
}

/**
* Used to track a real-world location in a Lens.

* @see [Landmarkers](https://developers.snap.com/lens-studio/features/location-ar/guide) guide.
* @see [Custom Location AR](https://developers.snap.com/lens-studio/features/location-ar/custom-landmarker) guide.

* @example
* ```js
* //@input Component.DeviceLocationTrackingComponent locationTrackingComponent

* // Get whether or not the landmarker is being tracked
* var isLocationTracking = script.locationTrackingComponent.isTracking();

* // Print current status.
* if (isLocationTracking) {
*     print("Location is being tracked");
* } else {
*     print("Location is not being tracked");
* }
* ```
*/
declare class DeviceLocationTrackingComponent extends Component {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns whether the location landmarker is currently being tracked.
    */
    isTracking(): boolean
    
    /**
    * Returns the distance, in meters, to the location. If the distance is unavailable, -1 is returned.
    
    * @readonly
    */
    distanceToLocation: number
    
    /**
    * The location that the tracker is tracking. Useful for dynamically controlling the target location being tracked.
    */
    location: LocationAsset
    
    /**
    * Returns the user's current LocationProximityStatus. Useful for telling if a user is close enough to the location to track it.
    
    * @readonly
    */
    locationProximityStatus: LocationProximityStatus
    
    /**
    * A function that gets called when location data fails to download.
    */
    onLocationDataDownloadFailed: () => void
    
    /**
    * A function that gets called when location data is downloaded.
    */
    onLocationDataDownloaded: () => void
    
    /**
    * A function that gets called when location is found.
    */
    onLocationFound: () => void
    
    /**
    * A function that gets called when location is lost. Note this will also happen when the user flips the camera.
    */
    onLocationLost: () => void
    
}

/**
* Enables a {@link SceneObject} to align with the movements and orientation of the user's device. Provides tracking modes such as `Surface`, `Rotation`, and `World`.

* @remarks

* Usually added to SceneObject with {@link Camera} component.

* If using `Surface` tracking mode, adding this to a SceneObject enables surface tracking for the scene, and moves the
* object to a position and rotation that matches the physical camera's pose in the world. Surface tracking can also be enhanced
* with native AR by enabling the `Use Native AR` option in the Inspector panel, or through script by setting the
* component's {@link SurfaceOptions.enhanceWithNativeAR} property.

* If using `Rotation` tracking mode, adding this to a SceneObject will apply the device's real world rotation to the object.

* If using `World` tracking mode, adding this to a SceneObject enables native AR tracking for the scene, and moves the
* object to a position and rotation that matches the physical camera's pose in the world.

* @see [Tracking Modes](https://developers.snap.com/lens-studio/features/ar-tracking/world/tracking-modes) guide for more information.

* @example
* Set the surface tracking target.
* ```js
* //@input Component.DeviceTracking deviceTrackingComponent
* function setTrackingTarget()
* {
*     if(script.deviceTrackingComponent)
*     {
*         script.deviceTrackingComponent.surfaceTrackingTarget = script.getSceneObject();
*     }
* }
* setTrackingTarget();
* ```
*/
declare class DeviceTracking extends Component {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Calculates a histogram of world mesh surfaces within a sphere at the given world position and radius. Only available when world mesh tracking is supported and enabled.
    */
    calculateWorldMeshHistogram(center: vec3, radius: number): TrackedMeshHistogramResult
    
    /**
    * Creates a TrackedPoint at world position `worldPos` and world rotation `worldRot`.
    */
    createTrackedWorldPoint(worldPos: vec3, worldRot: quat): TrackedPoint
    
    /**
    * Returns the actual DeviceTrackingMode being used. This may be different from the requested DeviceTrackingMode.
    */
    getActualDeviceTrackingMode(): DeviceTrackingMode
    
    /**
    * This capability is no longer available.
    
    * @deprecated
    */
    getDevicePath(): BasicTransform[]
    
    /**
    * This capability is no longer available.
    
    * @deprecated
    */
    getDevicePathIndex(): number
    
    /**
    * Returns the 3D point cloud representing important features visible by the camera.
    
    * @exposesUserData
    */
    getPointCloud(): PointCloud
    
    /**
    * Returns the DeviceTrackingMode currently requested to be used.
    * This may be different from the actual DeviceTrackingMode being used.
    */
    getRequestedDeviceTrackingMode(): DeviceTrackingMode
    
    /**
    * Returns an array of TrackedMeshHitTestResult that intersect with a ray cast from screen position screenPos. Only available when world mesh tracking is supported and enabled.
    */
    hitTestWorldMesh(screenPos: vec2): TrackedMeshHitTestResult[]
    
    /**
    * Returns whether the DeviceTrackingMode is supported.
    */
    isDeviceTrackingModeSupported(mode: DeviceTrackingMode): boolean
    
    /**
    * Returns an array of TrackedMeshHitTestResult that intersect with a ray cast from the world position `from` and continuing through the world position `to`. Only available when world mesh tracking is supported and enabled.
    */
    raycastWorldMesh(from: vec3, to: vec3): TrackedMeshHitTestResult[]
    
    /**
    * Requests that a DeviceTrackingMode be used. This requested change may not happen immediately.
    */
    requestDeviceTrackingMode(val: DeviceTrackingMode): void
    
    /**
    * Resets the World Tracking origin to the point on the surface plane aligned with the screen position `position`.
    * Screen positions are represented in the range ([0-1], [0-1]), (0,0) being the top-left of the screen and (1,1) being the bottom-right.
    */
    resetTracking(position: vec2): void
    
    /**
    * Offsets the default position of the World Tracking surface origin by `offset`.
    * Avoid using a `y` value of zero in `offset`, because it may cause problems with tracking.
    * If used outside of `Initialized` or `TurnOnEvent`, you will need to call `resetTracking()` to apply the offset.
    * Note: calling `resetTracking()` will overwrite the `x` and `z` components of the offset.
    */
    setWorldOriginOffset(offset: vec3): void
    
    /**
    * Used to access rotation tracking settings.
    */
    rotationOptions: RotationOptions
    
    /**
    * Used to access surface tracking settings.
    */
    surfaceOptions: SurfaceOptions
    
    /**
    * Helps to improve surface tracking accuracy while the target `SceneObject` is being moved.
    
    * @deprecated
    */
    surfaceTrackingTarget: SceneObject
    
    /**
    * Returns the WorldOptions object of this component, which can be used to control World Tracking settings.
    */
    worldOptions: WorldOptions
    
    /**
    * Returns the World Tracking Capabilities of the current device.
    
    * @readonly
    */
    worldTrackingCapabilities: WorldTrackingCapabilities
    
}

/**
* Tracking modes used by the {@link DeviceTracking} component to specify what type of tracking to use.

* @see Used By: {@link DeviceTracking#isDeviceTrackingModeSupported}, {@link DeviceTracking#requestDeviceTrackingMode}
* @see Returned By: {@link DeviceTracking#getActualDeviceTrackingMode}, {@link DeviceTracking#getRequestedDeviceTrackingMode}

* @example
* ```js
* // Set the device tracking mode to World
* // @input Component.DeviceTracking deviceTrackingComponent
* script.deviceTrackingComponent.requestDeviceTrackingMode(DeviceTrackingMode.World);
* ```
*/
declare enum DeviceTrackingMode {
    /**
    * Use gyroscope tracking (rotation only)
    */
    Rotation,
    /**
    * Use surface tracking (position and rotation)
    */
    Surface,
    /**
    * Use native tracking (position and rotation)
    */
    World
}

/**
* The module that allows Device Tracking capabilities in a Lens. Used for managing permissions.
*/
declare class DeviceTrackingModule extends Asset {
    
    /** @hidden */
    protected constructor()
    
}

declare namespace Dialog {
    /**
    * Provides answer information in response to `DialogModule.askQuestions()`.
    */
    class Answer extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * The answer to the question.
        
        * @readonly
        */
        answer: string
        
        /**
        * The index of the question this is answering in the questions array.
        
        * @readonly
        */
        questionId: number
        
        /**
        * Status of the question response.
        
        * @readonly
        */
        status: number
        
    }

}

/**
* Provides access to [Question Answering Service](https://developers.snap.com/lens-studio/features/voice-ml/q&a-template-guide) powered by VoiceML.

* @remarks
* Allows to answer questions based on provided text.
*/
declare class DialogModule extends Asset {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Sends a request to ask questions using the DialogModule. `context` is the text the model will use as context for answering the question. `questions` is a list of questions to ask. When answers are ready, `onQuestionsAnswerComplete` will be called with a list of `Dialog.Answer` objects corresponding to the questions in the `questions` list. `onQuestionsAnswerError` will be called if any error occurs.
    */
    askQuestions(context: string, questions: string[], onQuestionsAnswerComplete: (answers: Dialog.Answer[]) => void, onQuestionsAnswerError: (error: number, description: string) => void): void
    
}

/**
* An Audio Component effect that simulates sound attenuation based on the orientation of the transform relative to the {@link AudioListenerComponent}.

* @see Used By: {@link SpatialAudio#directivityEffect}

* @example
* ```js
* // @input Component.AudioComponent audio

* var spatialAudio = script.audio.spatialAudio;
* var directivityEffect = spatialAudio.directivityEffect;
* directivityEffect.enabled = true;
* directivityEffect.shapeDecay = 10;
* directivityEffect.shapeFactor = 0.5;
* ```
*/
declare class DirectivityEffect extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Whether the audio directivity effect is applied or not.
    */
    enabled: boolean
    
    /**
    * The width of main lobe.
    */
    shapeDecay: number
    
    /**
    * A ratio that specifies the shape of pattern directivity from omnidirectional(0) to cardioid (1.0).
    */
    shapeFactor: number
    
}

declare class DirectMultiplayerSessionOptions extends BaseMultiplayerSessionOptions {
    
    /** @hidden */
    protected constructor()
    
}

/**
* An audio effect that simulates sound attenuation based on the distance between the Audio and the {@link AudioListenerComponent}.

* @see Used By: {@link SpatialAudio#distanceEffect}

* @example
* ```js
* // @input Component.AudioComponent audio

* var distanceEffect = script.audio.spatialAudio.distanceEffect;

* distanceEffect.enabled = true;
* distanceEffect.maxDistance = 200;
* distanceEffect.minDistance = 50;
* distanceEffect.type = Audio.DistanceCurveType.Linear;
* ```
*/
declare class DistanceEffect extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * If enabled, the distance effect will be applied.
    */
    enabled: boolean
    
    /**
    * If the distance is higher than this value, the sound cannot be heard at all.
    */
    maxDistance: number
    
    /**
    * If the distance is less than this value, the distance effect is not applied at all.
    */
    minDistance: number
    
    /**
    * Curve type that describes how volume attenuates with distance.
    */
    type: Audio.DistanceCurveType
    
}

/**
* The domain of a {@link ShoppingModule}. You can leave this empty if you are tagging the products from the catalog dynamically, and assets are fetched and sent to the Lens.
*/
declare class DomainInfo {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The description of the domain.
    
    * @readonly
    */
    description: string
    
    /**
    * The name of the domain.
    
    * @readonly
    */
    name: string
    
    /**
    * The states available in the domain.
    
    * @readonly
    */
    states: StateInfo[]
    
}

/**
* Used in {@link Text}'s `dropShadowSettings` property.
* Configures how dropshadow will appear on a Text component.

* @see Used By: {@link Text#dropshadowSettings}

* @example
* ```js
* // @input Component.Text textComponent

* var shadowSettings = script.textComponent.dropshadowSettings;

* shadowSettings.enabled = true;
* shadowSettings.offset = new vec2(0.15, 0.15);
* ```
*/
declare class DropshadowSettings extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Whether dropshadow is enabled on the Text.
    */
    enabled: boolean
    
    /**
    * Settings for how the inside of the dropshadow is drawn.
    */
    fill: TextFill
    
    /**
    * An (x, y) offset controlling where the dropshadow is drawn relative to the Text.
    */
    offset: vec2
    
}

/**
* A resource that is resolved at runtime.

* @see Used By: {@link CameraRollMedia#resource}, {@link RemoteMediaModule#loadResourceAsAudioTrackAsset}, {@link RemoteMediaModule#loadResourceAsBytes}, {@link RemoteMediaModule#loadResourceAsGaussianSplattingAsset}, {@link RemoteMediaModule#loadResourceAsGltfAsset}, {@link RemoteMediaModule#loadResourceAsImageTexture}, {@link RemoteMediaModule#loadResourceAsString}, {@link RemoteMediaModule#loadResourceAsVideoTexture}, {@link UserContextSystem#loadResourceAsSnapchatUser}
* @see Returned By: {@link GltfAsset#getResourceFromExtras}, {@link RemoteApiResponse#asResource}, {@link RemoteServiceHttpResponse#asResource}, {@link RemoteServiceModule#makeResourceFromUrl}
*/
declare class DynamicResource extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Specifies the format for encoding textures, used with Base64.

* @see Used By: {@link Base64.encodeTextureAsync}

* @example
* ```js
* //@input Asset.Texture texture

* //@input int compressionQuality = 0 {"widget" : "combobox", "values" : [{"label" : "MaximumCompression", "value" : "0"}, {"label" : "LowQuality", "value" : "1"}, {"label" : "IntermediateQuality", "value" : "2"}, {"label" : "HighQuality", "value" : "3"}, {"label" : "MaximumQuality", "value" : "4"}]}

* //@input int encodingType = 0 {"widget" : "combobox", "values" : [{"label" : "Png", "value" : "0"}, {"label" : "Jpg", "value" : "1"}]}

* Base64.encodeTextureAsync(script.texture, onSuccess, onFailure, script.compressionQuality, script.encodingType)

* function onSuccess(encodedTexture) {
*     print("Encoded texture: " + encodedTexture)
* }

* function onFailure(error) {
*     print("Error: " + error)
* }
* ```
*/
declare enum EncodingType {
    /**
    * Lossless compression.
    */
    Png,
    /**
    * Lossy compression, usually smaller in size.
    */
    Jpg
}

declare class EventRegistration extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Expression names used with `FaceRenderObjectProvider.getExpressionWeightByName()`
* and returned by `FaceRenderObjectProvider.getExpressionNames()`.

* @example
* ```js
* // @input Asset.RenderMesh faceMesh

* var mouthCloseWeight = script.faceMesh.control.getExpressionWeightByName(Expressions.MouthClose);
* ```
*/
declare class Expressions {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Left eyebrow downward movement
    */
    static BrowsDownLeft: string
    
    /**
    * Right eyebrow downward movement
    */
    static BrowsDownRight: string
    
    /**
    * Between eyebrows upward movement
    */
    static BrowsUpCenter: string
    
    /**
    * Left eyebrow upward movement
    */
    static BrowsUpLeft: string
    
    /**
    * Right eyebrow upward movement
    */
    static BrowsUpRight: string
    
    /**
    * Left cheek and below left eye upward movement
    */
    static CheekSquintLeft: string
    
    /**
    * Right cheek and below right eye upward movement
    */
    static CheekSquintRight: string
    
    /**
    * Left eyelids closing
    */
    static EyeBlinkLeft: string
    
    /**
    * Right eyelids closing
    */
    static EyeBlinkRight: string
    
    /**
    * Left eyelids downward look
    */
    static EyeDownLeft: string
    
    /**
    * Right eyelids downward look
    */
    static EyeDownRight: string
    
    /**
    * Left eyelids looking towards center
    */
    static EyeInLeft: string
    
    /**
    * Right eyelids looking towards center
    */
    static EyeInRight: string
    
    /**
    * Left eyelids opening
    */
    static EyeOpenLeft: string
    
    /**
    * Right eyelids opening
    */
    static EyeOpenRight: string
    
    /**
    * Left eyelids looking away from center
    */
    static EyeOutLeft: string
    
    /**
    * Right eyelids looking away from center
    */
    static EyeOutRight: string
    
    /**
    * Left eye squinting
    */
    static EyeSquintLeft: string
    
    /**
    * Right eye squinting
    */
    static EyeSquintRight: string
    
    /**
    * Left eyelids upward look
    */
    static EyeUpLeft: string
    
    /**
    * Right eyelids upward look
    */
    static EyeUpRight: string
    
    /**
    * Jaw forward movement
    */
    static JawForward: string
    
    /**
    * Jaw leftward movement
    */
    static JawLeft: string
    
    /**
    * Jaw opening
    */
    static JawOpen: string
    
    /**
    * Jaw rightward movement
    */
    static JawRight: string
    
    /**
    * Lips forming open circular shape together
    */
    static LipsFunnel: string
    
    /**
    * Lips compressing together while closed
    */
    static LipsPucker: string
    
    /**
    * Lower lip moving towards and behind upper lip
    */
    static LowerLipClose: string
    
    /**
    * Left lower lip downward movement
    */
    static LowerLipDownLeft: string
    
    /**
    * Right lower lip downward movement
    */
    static LowerLipDownRight: string
    
    /**
    * Lower lip upward movement
    */
    static LowerLipRaise: string
    
    /**
    * Lips moving together
    */
    static MouthClose: string
    
    /**
    * Left mouth corner back and leftward movement
    */
    static MouthDimpleLeft: string
    
    /**
    * Right mouth corner back and rightward movement
    */
    static MouthDimpleRight: string
    
    /**
    * Left mouth corner downward movement
    */
    static MouthFrownLeft: string
    
    /**
    * Right mouth corner downward movement
    */
    static MouthFrownRight: string
    
    /**
    * Both lips leftward movement
    */
    static MouthLeft: string
    
    /**
    * Both lips rightward movement
    */
    static MouthRight: string
    
    /**
    * Left mouth corner upward movement
    */
    static MouthSmileLeft: string
    
    /**
    * Right mouth corner upward movement
    */
    static MouthSmileRight: string
    
    /**
    * Left side of mouth leftward movement
    */
    static MouthStretchLeft: string
    
    /**
    * Right side of mouth rightward movement
    */
    static MouthStretchRight: string
    
    /**
    * Left side of mouth upward movement
    */
    static MouthUpLeft: string
    
    /**
    * Right side of mouth upward movement
    */
    static MouthUpRight: string
    
    /**
    * Both cheeks puffing outward movement
    */
    static Puff: string
    
    /**
    * Left nostril raising
    */
    static SneerLeft: string
    
    /**
    * Right nostril raising
    */
    static SneerRight: string
    
    /**
    * Upper lip moving towards and behind lower lip
    */
    static UpperLipClose: string
    
    /**
    * Upper lip upward movement
    */
    static UpperLipRaise: string
    
    /**
    * Left upper lip upward movement
    */
    static UpperLipUpLeft: string
    
    /**
    * Right upper lip upward movement
    */
    static UpperLipUpRight: string
    
}

/**
* Information about an external music track. Accessible through {@link ExternalMusicModule#getExternalMusicInfo | ExternalMusicModule.getExternalMusicInfo()}.

* @see Used By: {@link ExternalMusicInfo#isSameTrack}
* @see Returned By: {@link ExternalMusicModule#getExternalMusicInfo}, {@link LicensedAudioTrackAsset#getExternalMusicInfo}
*/
declare class ExternalMusicInfo extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns true if this object represents the same music track as the passed in object.
    */
    isSameTrack(other: ExternalMusicInfo): boolean
    
}

/**
* Provides an interface to the `ExternalMusic` feature, and opts the Lens into using the feature when present in the scene.
* @remarks
* When this module is present in a scene, an unbundled and licensed AudioTrack asset must be present in the scene as well. This means the Lens developer should:

* 1. Import a licensed music track from the Asset Library
* 2. On the imported audio asset, make sure `Bundled` is disabled
* 3. Reference the audio asset somewhere in the scene (for example, `@input audioAsset: AudioTrackAsset`)

* When this module is present in a Lens, the client running the Lens (either Snapchat or Lens Studio) will automatically fetch and auto-play the licensed music track included in the Lens.

* This provides the benefits of:
* - The music file is not included in the lens, leading to smaller lens size
* - Special features like {@link LyricsTracker | Lyrics} may be available to the Lens, depending on the audio track
* - On Snapchat, a user can use the built-in music tool to adjust the music the Lens is using, and the Lens can react to that change

* This also adds the following limitations, some of which are due to the inclusion of Licensed Audio Tracks:
* - The Lens cannot control or affect music playback, since it's controlled externally by the user.
* - The Lens must contain a single, unbundled Licensed Audio track. The Lens is not able to play it or any other audio through local methods, such as AudioComponent.
* - If device location is not available, or the track is not available in the users country, the sound in the Lens will be muted. The user will have an ability to remove or replace unavailable tracks.
* - The final audio will be mixed directly into the Lens.

* @example
* ```ts
* const externalMusicModule = require("LensStudio:ExternalMusicModule") as ExternalMusicModule;

* @component
* export class ExternalMusicExample extends BaseScriptComponent {
*     // Make sure an AudioTrack asset is included in the scene. It determines the track that plays.
*     // If you see errors, make sure that "Bundled" is unchecked (disabled) on the AudioTrack.
*     @input audioAsset: AudioTrackAsset;

*     onAwake() {
*         // Get LyricsTracker
*         const lyricsTracker = externalMusicModule.getLyricsTracker();

*         externalMusicModule.onTrackStarted.add(() => {
*             print("External music has started to play");
*         });

*         externalMusicModule.onTrackStopped.add(() => {
*             print("External music has stopped playback");
*         });

*         externalMusicModule.onTrackRemoved.add(() => {
*             print("External music track has been removed");
*         });

*         externalMusicModule.onTrackChanged.add(() => {
*             print("External music track has been changed");
*         });
*     }
* }

* ```
*/
declare class ExternalMusicModule extends Asset {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns the {@link ExternalMusicInfo} object for the current external music track. This can be used to compare two audio tracks during the Lens runtime.
    */
    getExternalMusicInfo(): ExternalMusicInfo
    
    /**
    * Returns the {@link LyricsTracker} for the current external music track.
    */
    getLyricsTracker(): LyricsTracker
    
    /**
    * Returns the sound sync (beats) tracker for the external music track.
    */
    getSoundSyncTracker(): SoundSyncTracker
    
    /**
    * Returns true if an external music track is set.
    
    * @readonly
    */
    isTrackSet: boolean
    
    /**
    * Event that triggers when the external music track changes.
    
    * @readonly
    */
    onTrackChanged: event1<ExternalMusicInfo, void>
    
    /**
    * Event that fires when an external music track is removed.
    
    * @readonly
    */
    onTrackRemoved: event0<void>
    
    /**
    * Event that fires when an external music track starts playing.
    
    * @readonly
    */
    onTrackStarted: event1<ExternalMusicInfo, void>
    
    /**
    * Event that fires when an external music track stops playing.
    
    * @readonly
    */
    onTrackStopped: event0<void>
    
}

/**
* Applies an eye color effect to a face.

* @see [Eye Color](https://developers.snap.com/lens-studio/features/ar-tracking/face/eye-color) guide.

* @example
* ```
* // Prints the eye property `faceIndex`, the face the eye color effect is applied to
* var face = script.getSceneObject().getFirstComponent("Component.EyeColorVisual").faceIndex;

* print("faceIndex = " + face.toString());
* ```
*/
declare class EyeColorVisual extends MaterialMeshVisual {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The index of the face this EyeColorVisual is attached to.
    */
    faceIndex: number
    
    /**
    * The tracking context this effect is being applied to.
    */
    trackingScope: (PersonTrackingScope|TextureTrackingScope|FaceTrackingScope)
    
}

/**
* Texture Provider giving a cropped region of the input texture, calculated based on face position.
* Can be accessed using `Texture.control` on a FaceCropTexture asset.
* For more information, see the [Crop Textures](https://developers.snap.com/lens-studio/features/snap-ml/ml-component/crop-textures) guide.

* @example
* ```js
* //@input Asset.Texture faceCropTexture
* //@input Asset.Texture deviceCameraTexture

* var cropProvider = script.faceCropTexture.control;
* cropProvider.inputTexture = script.deviceCameraTexture;
* cropProvider.faceIndex = 0;
* cropProvider.textureScale = new vec2(1.25, 1.25);
* cropProvider.faceCenterMouthWeight = 0.0;
* ```
*/
declare class FaceCropTextureProvider extends CropTextureProvider {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Ratio of the mouth position on the cropped texture. Value ranges from 0 to 1, with 0 having no effect and 1 centering the image on the mouth.
    */
    faceCenterMouthWeight: number
    
    /**
    * Index of the face being tracked.
    */
    faceIndex: number
    
    /**
    * Scaling of the cropped texture.
    */
    textureScale: vec2
    
    /**
    * The tracking context this effect is being applied to.
    */
    trackingScope: (PersonTrackingScope|TextureTrackingScope|FaceTrackingScope)
    
}

/**
* Triggered when a new face is detected and starts being tracked.

* @example
* ```js
* var event0 = script.createEvent("FaceFoundEvent");
* event0.faceIndex = 0;
* event0.bind(function (eventData)
* {
* 	print("Found face 0");
* });

* var event1 = script.createEvent("FaceFoundEvent");
* event1.faceIndex = 1;
* event1.bind(function (eventData)
* {
* 	print("Found face 1");
* });
* ```
*/
declare class FaceFoundEvent extends FaceTrackingEvent {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Controls the face image picker texture resource.
* Can be accessed through {@link Texture | Texture.control} on a Face Image Picker texture.
* For more information, see the [Face Image Picker Texture guide](https://developers.snap.com/lens-studio/features/ar-tracking/face/face-image-picker-texture).

* @deprecated

* @example
* ```js
* // @input Asset.Texture faceImagePickerTexture

* // Show the face image picker UI
* script.faceImagePickerTexture.control.showImagePicker();

* // Set a callback for when the user picks a different image from the image picker
* script.faceImagePickerTexture.control.setImageChangedCallback(function(){
*     print("face image picker image changed");
* });
* ```
*/
declare class FaceImagePickerTextureProvider extends ImagePickerTextureProvider {
    
    /** @hidden */
    protected constructor()
    
    /**
    * If enabled, the selected image will be cropped to only show the face region.
    
    * @deprecated
    */
    cropToFace: boolean
    
    /**
    * The FaceTextureProvider used to provide the face texture.
    
    * @deprecated
    */
    faceControl: FaceTextureProvider
    
}

/**
* Used with {@link FaceInsetVisual.faceRegion} for setting the face region to draw.

* @see Used By: {@link FaceInsetVisual#faceRegion}

* @example
* ```js
* //@input Component.FaceInsetVisual faceInset
* // Sets the face inset to draw the mouth
* script.faceInset.faceRegion = FaceInsetRegion.Mouth;
* ```
*/
declare enum FaceInsetRegion {
    /**
    * Targets the entire face
    */
    Face,
    /**
    * Targets the left eye
    */
    LeftEye,
    /**
    * Targets the mouth
    */
    Mouth,
    /**
    * Targets the nose
    */
    Nose,
    /**
    * Targets the right eye
    */
    RightEye
}

/**
* Provides a 2D visual of a section of a tracked face, such as `Mouth`, `Nose`, etc.

* @see [Face Inset](https://developers.snap.com/lens-studio/features/ar-tracking/face/face-inset) guide.

* @example
* ```
* // Sets the face inset to use the left eye
* var faceInset = script.getSceneObject().getFirstComponent("Component.FaceInsetVisual");
* faceInset.faceRegion = FaceInsetRegion.LeftEye;
* ```
*/
declare class FaceInsetVisual extends MaterialMeshVisual {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The index of the face this FaceInsetVisual uses.
    */
    faceIndex: number
    
    /**
    * The region of the face this FaceInsetVisual draws.
    */
    faceRegion: FaceInsetRegion
    
    /**
    * Flips the drawn face region horizontally if enabled.
    */
    flipX: boolean
    
    /**
    * Flips the drawn face region vertically if enabled.
    */
    flipY: boolean
    
    /**
    * The amount of alpha fading applied from the border of the face inset inward.
    * This value must be in the range 0-1.
    */
    innerBorderRadius: number
    
    /**
    * The amount of alpha fading applied from the border of the face inset outward.
    * This value must be in the range 0-1.
    */
    outerBorderRadius: number
    
    /**
    * The x and y scaling used to draw the face region.
    * Think of scaling as meaning how many times the face region could fit into the drawing area.
    * Higher values will zoom away from the face region, and lower values will zoom into it.
    * The normal, unzoomed scaling value is (1,1).
    */
    sourceScale: vec2
    
    /**
    * Determines the quality of the face inset's borders.
    * A higher value means better looking borders but lower performance.
    * This value must be greater than 10 and less than 100.
    */
    subdivisionsCount: number
    
    /**
    * The tracking context this effect is being applied to.
    */
    trackingScope: (PersonTrackingScope|TextureTrackingScope|FaceTrackingScope)
    
}

/**
* Triggered when a face can no longer be tracked.  For example, if a face gets blocked from the camera's view, or gets too far away.

* @example
* ```js
* var event = script.createEvent("FaceLostEvent");
* event.faceIndex = 0;
* event.bind(function (eventData)
* {
* 	print("Face tracking lost on face 0");
* });
* ```
*/
declare class FaceLostEvent extends FaceTrackingEvent {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Maps a 2D texture to the user's face.

* @remarks
* The texture appears to be painted on user's skin and contorts with facial movements. Great for full face masks but also can be used for realistic makeup.

* @see [Face Mask](https://developers.snap.com/lens-studio/features/ar-tracking/face/face-mask) guide.

* @example
* ```js
* // Run this in the "Frame Updated" event to switch between drawn faces twice a second
* // Make sure "Use Orig. Face" is enabled on the FaceMask

* //@input Component.FaceMaskVisual faceMask
* //@input Component.Head head

* var numFaces = script.head.getFacesCount();
* script.faceMask.originalFaceIndex = Math.floor(getTime() * 2.0) % numFaces;
* ```
*/
declare class FaceMaskVisual extends MaterialMeshVisual {
    
    /** @hidden */
    protected constructor()
    
    /**
    * A custom mask that will be shown when the detected face's mouth is closed.
    */
    customMaskOnMouthClosed: Texture
    
    /**
    * The index of the face this effect is attached to.
    */
    faceIndex: number
    
    /**
    * Whether to hide the opacity mask when the detected face's mouth is closed.
    */
    hidesMaskOnMouthClosed: boolean
    
    /**
    * If "Use Orig. Face" is enabled for this FaceMaskVisual in the Inspector panel, this property
    * specifies the face index to use for drawing the mask.
    */
    originalFaceIndex: number
    
    /**
    * Whether to swap the opacity mask when the detected face's mouth is closed.
    */
    swapsMaskOnMouthClosed: boolean
    
    teethAlpha: number
    
    /**
    * The tracking context this effect is being applied to.
    */
    trackingScope: (PersonTrackingScope|TextureTrackingScope|FaceTrackingScope)
    
    useOriginalTexCoords: boolean
    
}

/**
* Mesh provider for a Face Mesh. Accessible through the `control` property on a Face Mesh `RenderMesh`.

* @example
* ```js
* // @input Asset.RenderMesh faceMesh

* var weights = script.faceMesh.control.onExpressionWeightsUpdate.add(function(expWeights){
*   // The passed in expressionWeights is a NamedValues: meaning it contains a names and values array. These two arrays correspond to each other.
*   print(expWeights.names[0]); // Prints "EyeBlinkLeft"
*   print(expWeights.values[0]); // Prints strength of EyeBlinkLeft expression
* });
* ```
*/
declare class FaceRenderObjectProvider extends RenderObjectProvider {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns a list of all expression names being tracked.
    
    * @deprecated
    */
    getExpressionNames(): string[]
    
    /**
    * Returns the weight of the expression with the passed in name. See {@link Expressions} for valid expression names.
    
    * @deprecated
    */
    getExpressionWeightByName(expressionName: string): number
    
    /**
    * Returns a Float32Array of all expression weights being tracked.
    
    * @deprecated
    */
    getExpressionWeights(): Float32Array
    
    /**
    * When true, ears will be included in the Face Mesh geometry.
    */
    earGeometryEnabled: boolean
    
    /**
    * When true, a small area in the corners of the eyes will be included in the Face Mesh geometry.
    */
    eyeCornerGeometryEnabled: boolean
    
    /**
    * When true, eyes will be included in the Face Mesh geometry.
    */
    eyeGeometryEnabled: boolean
    
    /**
    * When true, the general face (not including eyes and mouth) will be included in the Face Mesh geometry.
    */
    faceGeometryEnabled: boolean
    
    /**
    * Index of the face this FaceRenderObjectProvider mirrors.
    */
    faceIndex: number
    
    /**
    * When true, the mouth will be included in the Face Mesh geometry.
    */
    mouthGeometryEnabled: boolean
    
    /**
    * An event that will fire each time new expression weights are available.
    
    * @readonly
    */
    onExpressionWeightsUpdate: event1<NamedValues, void>
    
    /**
    * When true, the skull will be included in the Face Mesh geometry.
    */
    skullGeometryEnabled: boolean
    
    /**
    * The tracking context this effect is being applied to.
    */
    trackingScope: (PersonTrackingScope|TextureTrackingScope|FaceTrackingScope)
    
}

/**
* Used to apply deformation effects to specific regions of a tracked face.

* @remarks
* Face Stretch features can be added to a FaceStretchVisual through the Inspector panel in Lens Studio.

* @see [Face Stretch](https://developers.snap.com/lens-studio/features/ar-tracking/face/face-stretch) guide.
* @see [Distort](https://developers.snap.com/lens-studio/features/ar-tracking/face/face-templates/distort) example.

* @example
* ```
* // Changes the intensity of the first face stretch feature
* //@input Component.FaceStretchVisual faceStretch
* var feature = "Feature0";
* var intensity = 2.0;

* script.faceStretch.setFeatureWeight(feature, intensity);
* ```
*/
declare class FaceStretchVisual extends BaseMeshVisual {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Add a feature to the face stretch effect.
    */
    addFeature(name: string): void
    
    /**
    * Remove all features from the face stretch effect.
    */
    clearFeatures(): void
    
    /**
    * Get an array of all the feature names on the face stretch effect.
    */
    getFeatureNames(): string[]
    
    /**
    * Get an array of all the points on the face stretch feature.
    */
    getFeaturePoints(name: string): StretchPoint[]
    
    /**
    * Returns the weight of the face stretch feature named `feature`.
    */
    getFeatureWeight(feature: string): number
    
    /**
    * Remove a feature from the face stretch effect.
    */
    removeFeature(name: string): void
    
    /**
    * Sets the weight of the face stretch feature named `feature` to `intensity`.
    * The intensity must be greater than -0.5 and less than 2.
    */
    setFeatureWeight(feature: string, intensity: number): void
    
    /**
    * Update the the points on the provided face stretch feature.
    */
    updateFeaturePoints(name: string, points: StretchPoint[]): void
    
    /**
    * The index of the face the stretch will be applied to.
    */
    faceIndex: number
    
    /**
    * The tracking context this effect is being applied to.
    */
    trackingScope: (PersonTrackingScope|TextureTrackingScope|FaceTrackingScope)
    
}

/**
* TextureProvider for face textures.
* See the [Face Texture Guide](https://developers.snap.com/lens-studio/features/ar-tracking/face/face-texture) for more information.
* Can be accessed using {@link Texture | Texture.control} on a face texture asset.

* @deprecated

* @example
* ```js
* // Use this in "Frame Updated" to zoom in and out on a face texture over time
* //@input Asset.Texture faceTexture

* var faceTexProvider = script.faceTexture.control;
* var scaleAmount = .75 + Math.sin(getTime())*.25;
* faceTexProvider.scale = new vec2(scaleAmount, scaleAmount);
* ```
*/
declare class FaceTextureProvider extends TextureProvider {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Index of the face to track.
    
    * @deprecated
    */
    faceIndex: number
    
    /**
    * The source texture being drawn.
    * This is useful for controlling which effects are visible on the face texture, based on which camera output texture is being used.
    
    * @deprecated
    */
    inputTexture: Texture
    
    /**
    * The x and y scale used to draw the face within the texture region.
    * A lower scale will be more zoomed in on the face, and a higher scale will be more zoomed out.
    * The default scale is (1, 1).
    
    * @deprecated
    */
    scale: vec2
    
}

/**
* This is the base class for all face tracking events. This event won't actually get triggered itself, so use one of the child classes instead.
*/
declare class FaceTrackingEvent extends SceneEvent {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The index of the face this event is tracking. Change this value to control which face the event tracks.
    */
    faceIndex: number
    
    /**
    * The tracking context this effect is being applied to.
    */
    trackingScope: (PersonTrackingScope|TextureTrackingScope|FaceTrackingScope)
    
}

/**
* The face that a tracker should use. Allows you to to apply the same face index to a group of Components.
*/
declare class FaceTrackingScope extends TrackingScope {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The face to track. The first face is 0, the second is 1, and so on.
    */
    faceIndex: number
    
    /**
    * The parent context that this Tracking Scope is identifying the face from.
    */
    parentScope: TextureTrackingScope
    
}

/**
* Provider for file based Audio Tracks.

* @example
* ```js
* // @input Asset.AudioTrackAsset audioTrack

* var control = script.audioTrack.control;

* if (control.isOfType("Provider.FileAudioTrackProvider")) {
*     control.loops = -1; //set to loop forever
* }
* var audioFrameArray = new Float32Array(control.maxFrameSize)

* script.script.createEvent("UpdateEvent").bind(function(eventData){
*     var frameShape = control.getAudioFrame(audioFrameArray);
*     print(frameShape.x + " samples were read on this update")
*     print(control.position + " current audio track position");
* });
* ```
*/
declare class FileAudioTrackProvider extends AudioTrackProvider {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Writes readSize samples into the passed in audioBuffer Float32Array.
    */
    getAudioBuffer(audioBuffer: Float32Array, readSize: number): vec3
    
    /**
    * Writes current audio frame to the passed in `Float32Array` and returns the frame shape.
    */
    getAudioFrame(audioFrame: Float32Array): vec3
    
    /**
    * The duration of the AudioTrackAsset in seconds.
    
    * @readonly
    */
    duration: number
    
    /**
    * Loop count, if `-1` is provided, the audio track will loop forever.
    */
    loops: number
    
    /**
    * The current position of the AudioTrackAsset in seconds.
    */
    position: number
    
}

/**
* A file track provider of the Licensed Sounds from Asset LIbrary.

* @example
* ```js
* //@input Asset.AudioTrackAsset audioTrack

* var control = script.audioTrack.control;
* var sampleRate = control.sampleRate;
* var audioBuffer = new Float32Array(control.maxFrameSize);
* control.loops = -1;


* script.createEvent("UpdateEvent").bind(function (eventData) {
*    var readSize = eventData.getDeltaTime() * sampleRate;
*    control.getAudioBuffer(audioBuffer, readSize);
* });
* ```
*/
declare class FileLicensedSoundProvider extends AudioTrackProvider {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Writes readSize samples into the passed in audioBuffer Float32Array.
    */
    getAudioBuffer(audioBuffer: Float32Array, readSize: number): vec3
    
    /**
    * Writes current audio frame to the passed in Float32Array and returns the frame shape.
    */
    getAudioFrame(audioFrame: Float32Array): vec3
    
    /**
    * The duration of the AudioTrackAsset in seconds.
    
    * @readonly
    */
    duration: number
    
    /**
    * Loop count, if -1 is provided, the audio track will loop forever.
    */
    loops: number
    
}

/**
* A {@link TextureProvider} for textures originating from files.
*/
declare class FileTextureProvider extends TextureProvider {
    
    /** @hidden */
    protected constructor()
    
}

/**


* @see Used By: {@link SamplerBuilder#setFilteringMode}, {@link SamplerWrapper#filtering}
*/
declare enum FilteringMode {
    Nearest,
    Bilinear,
    Trilinear
}

/**
* Used for collision meshes that remain static and do not change their shape over time.
*/
declare class FixedCollisionMesh extends CollisionMesh {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Fully constrain rotation and translation.

* @see {@link ConstraintComponent}.

* @example
* ```js
* // Create a constraint on an object
* var fixedConstraint = hingeObj.createComponent("Physics.ConstraintComponent");

* // Set up options on that constraint
* fixedConstraint.debugDrawEnabled = true;
* fixedConstraint.constraint = Physics.Constraint.create(Physics.ConstraintType.Fixed);
* ```
*/
declare class FixedConstraint extends Constraint {
    
    /** @hidden */
    protected constructor()
    
}

/**
* The base class for animation tracks using float values.

* @deprecated
*/
declare class FloatAnimationTrack extends AnimationTrack {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Represents an animation track using float value keyframes.

* @deprecated
*/
declare class FloatAnimationTrackKeyFramed extends FloatAnimationTrack {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Adds a key with value `value` at time `time`.
    
    * @deprecated
    */
    addKey(time: number, value: number): void
    
    /**
    * Removes all keys.
    
    * @deprecated
    */
    removeAllKeys(): void
    
    /**
    * Removes key at index `index`.
    
    * @deprecated
    */
    removeKeyAt(index: number): void
    
}

/**
* Represents an animation track using vec3 value keyframes for a bezier curve.

* @deprecated
*/
declare class FloatBezierAnimationTrackKeyFramed extends FloatAnimationTrack {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Adds a key with value `value` at time `time`.
    
    * @deprecated
    */
    addKey(time: number, value: vec3): void
    
    /**
    * Removes all keys.
    
    * @deprecated
    */
    removeAllKeys(): void
    
    /**
    * Removes key at index `index`.
    
    * @deprecated
    */
    removeKeyAt(index: number): void
    
}

/**
* Arguments used with the `InteractionComponent.onFocusEnd` event.

* @example
* ```js
* //@input Component.InteractionComponent interactionComponent

* // Subscribe to the onFocusEnd event
* var onFocusEndEvent = script.interactionComponent.onFocusEnd.add(function(focusEndEventArgs){
*     print("Focus End!");
* });

* // Unsubscribe from the onFocusEnd event
* script.interactionComponent.onFocusEnd.remove(onFocusEndEvent);
* ```
*/
declare class FocusEndEventArgs extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Arguments used with the `InteractionComponent.onFocusStart` event.

* @example
* ```js
* //@input Component.InteractionComponent interactionComponent

* // Subscribe to the onFocusStart event
* var onFocusStartEvent = script.interactionComponent.onFocusStart.add(function(focusStartEventArgs){
*     print("Focus End!");
* });

* // Unsubscribe from the onFocusStart event
* script.interactionComponent.onFocusStart.remove(onFocusStartEvent);
* ```
*/
declare class FocusStartEventArgs extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
}

/**
* A font asset used for rendering text.

* @see {@link Text}.
* @see [Text](https://developers.snap.com/lens-studio/features/text/2d-text) guide.

* @see Used By: {@link Text#font}, {@link Text3D#font}, {@link TextProvider#fontAsset}

* @example
* ```js
* // Assign a Font to a Label
* //@input Component.Label label
* //@input Asset.Font font
* script.label.fontAsset = script.font;
* ```
*/
declare class Font extends Asset {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Mode for setting frustum culling on Pass

* @see Used By: {@link Pass#frustumCullMode}
*/
declare enum FrustumCullMode {
    /**
    * Based on shader property, frustum culling will either be disabled or test with render object's aabb
    */
    Auto,
    /**
    * Enable frustum culling and extend render object's aabb to (1 + value)
    */
    Extend,
    /**
    * Users define the specific AABB which is used for culling test. Assumes frustumCullMin and frustumCullMax are calculated in local space of the object. frustumCullMin is the bottom-left-back and frustumCullMax is the top-right-front of the user defined AABB.
    */
    UserDefinedAABB
}

/**
* Asset that contains [Gaussian Splats](https://developers.snap.com/lens-studio/features/graphics/gaussian-splatting). Used with {@link GaussianSplattingVisual}.

* @see Used By: {@link GaussianSplattingVisual#asset}, {@link RemoteMediaModule#loadResourceAsGaussianSplattingAsset}
*/
declare class GaussianSplattingAsset extends Asset {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The number of frames in the asset. Useful when animating through several Gaussian Splats.
    */
    getNumberOfFrames(): number
    
    /**
    * The number of splats in the asset.
    */
    getSplatCount(): number
    
}

/**
* Allows displaying {@link GaussianSplattingAsset} and play its animation.

* @see [Gaussian Splatting](https://developers.snap.com/lens-studio/features/graphics/gaussian-splatting) guide.
*/
declare class GaussianSplattingVisual extends MaterialMeshVisual {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns whether the visual is currently playing.
    */
    isPlaying(): boolean
    
    /**
    * Pauses the visual.
    */
    pause(): void
    
    /**
    * Plays the visual.
    */
    play(): void
    
    /**
    * The current frame of the Gaussian Splat being rendered.
    */
    activeFrame: number
    
    /**
    * The asset to be rendered.
    */
    asset: GaussianSplattingAsset
    
    /**
    * Whether the visual was set to automatically play.
    */
    autoPlay: boolean
    
    /**
    * Denotes how many key frames this visual is sampled at.
    */
    fps: number
    
}

/**
* Class for storing and retrieving data based on keys.
* Used by {@link PersistentStorageSystem}.
* For more information, see the [Persistent Storage guide](https://developers.snap.com/lens-studio/features/persistent-cloud-storage/persistent-storage).

* @see Used By: {@link BaseMultiplayerSessionOptions#onRealtimeStoreCreated}, {@link BaseMultiplayerSessionOptions#onRealtimeStoreDeleted}, {@link BaseMultiplayerSessionOptions#onRealtimeStoreOwnershipUpdated}, {@link BaseMultiplayerSessionOptions#onRealtimeStoreUpdated}, {@link ConnectedLensModule.RealtimeStoreKeyRemovalInfo#store}, {@link MultiplayerSession#clearRealtimeStoreOwnership}, {@link MultiplayerSession#clearRealtimeStoreOwnership}, {@link MultiplayerSession#createRealtimeStore}, {@link MultiplayerSession#deleteRealtimeStore}, {@link MultiplayerSession#deleteRealtimeStore}, {@link MultiplayerSession#getRealtimeStoreInfo}, {@link MultiplayerSession#requestRealtimeStoreOwnership}, {@link MultiplayerSession#requestRealtimeStoreOwnership}, {@link PersistentStorageSystem#store}, {@link RealtimeStoreCreateOptions#initialStore}
* @see Returned By: {@link GeneralDataStore.create}

* @example
* ```js
* // Get the data store from PersistentStorageSystem
* var store = global.persistentStorageSystem.store;

* // Load score from previous Lens session. Defaults to 0 if data isn't found for this key
* var score = store.getFloat("totalScore");
* print("loaded score: " + score);

* function increaseScore() {
*     // Increase score, then write it to the data store
*     score += 1;
*     store.putFloat("totalScore", score);
*     print("new score: " + score);
* }

* // Increase score on tap event
* script.createEvent("TapEvent").bind(increaseScore);
* ```

* ```js
* // Check Launch Params for a certain string value
* var stringParamName = "testString";
* if (global.launchParams && global.launchParams.has(stringParamName)) {
*     var myString = global.launchParams.getString(stringParamName);
* }
* ```
*/
declare class GeneralDataStore extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Clears all data stored in the General Data Store.
    */
    clear(): void
    
    /**
    * Returns an array with all the keys in the store.
    */
    getAllKeys(): string[]
    
    /**
    * Returns a boolean value stored under the given key, or false if none exists.
    */
    getBool(key: string): boolean
    
    /**
    * Returns a boolean array being stored under the given key, or an empty array if none exists.
    */
    getBoolArray(key: string): boolean[]
    
    /**
    * Returns a double precision floating point number stored under the given key, or 0 if none exists.
    */
    getDouble(key: string): number
    
    /**
    * Returns a floating point value stored under the given key, or 0 if none exists.
    */
    getFloat(key: string): number
    
    getFloat32Array(key: string): Float32Array
    
    getFloat64Array(key: string): Float64Array
    
    /**
    * Returns a floating point array being stored under the given key, or an empty array if none exists.
    */
    getFloatArray(key: string): number[]
    
    /**
    * Returns an integer number stored under the given key, or 0 if none exists.
    */
    getInt(key: string): number
    
    getInt16Array(key: string): Int16Array
    
    getInt32Array(key: string): Int32Array
    
    getInt8Array(key: string): Int8Array
    
    /**
    * Returns an integer array being stored under the given key, or an empty array if none exists.
    */
    getIntArray(key: string): number[]
    
    /**
    * Returns a mat2 value stored under the given key, or a default mat2 if none exists.
    */
    getMat2(key: string): mat2
    
    /**
    * Returns a mat2 array being stored under the given key, or an empty array if none exists.
    */
    getMat2Array(key: string): mat2[]
    
    /**
    * Stores a mat3 value under the given key.
    */
    getMat3(key: string): mat3
    
    /**
    * Returns a mat3 array being stored under the given key, or an empty array if none exists.
    */
    getMat3Array(key: string): mat3[]
    
    /**
    * Returns a mat4 value stored under the given key, or a default mat4 if none exists.
    */
    getMat4(key: string): mat4
    
    /**
    * Returns a mat4 array being stored under the given key, or an empty array if none exists.
    */
    getMat4Array(key: string): mat4[]
    
    /**
    * Returns the maximum total size allowed, in bytes, of all data stored in this General Data Store.
    */
    getMaxSizeInBytes(): number
    
    /**
    * Returns a quat value stored under the given key, or a default quat if none exists.
    */
    getQuat(key: string): quat
    
    /**
    * Returns a quat array being stored under the given key, or an empty array if none exists.
    */
    getQuatArray(key: string): quat[]
    
    /**
    * If `onStoreFull` has been set, this method returns the current total size, in bytes, of all data stored in this General Data Store. Otherwise, `0` is returned.
    */
    getSizeInBytes(): number
    
    /**
    * Returns a string value stored under the given key, or empty string if none exists.
    */
    getString(key: string): string
    
    /**
    * Returns a string array being stored under the given key, or an empty array if none exists.
    */
    getStringArray(key: string): string[]
    
    getUint16Array(key: string): Uint16Array
    
    getUint32Array(key: string): Uint32Array
    
    getUint8Array(key: string): Uint8Array
    
    /**
    * Returns a vec2 value stored under the given key, or a default vec2 if none exists.
    */
    getVec2(key: string): vec2
    
    /**
    * Returns a vec2 array being stored under the given key, or an empty array if none exists.
    */
    getVec2Array(key: string): vec2[]
    
    /**
    * Returns a vec3 value stored under the given key, or a default vec3 if none exists.
    */
    getVec3(key: string): vec3
    
    /**
    * Returns a vec3 array being stored under the given key, or an empty array if none exists.
    */
    getVec3Array(key: string): vec3[]
    
    /**
    * Returns a vec4 value stored under the given key, or a default vec4 if none exists.
    */
    getVec4(key: string): vec4
    
    /**
    * Returns a vec4 array being stored under the given key, or an empty array if none exists.
    */
    getVec4Array(key: string): vec4[]
    
    /**
    * Returns true if a value is being stored under the given key.
    */
    has(key: string): boolean
    
    /**
    * Stores a boolean value under the given key.
    */
    putBool(key: string, value: boolean): void
    
    /**
    * Stores a boolean array under the given key.
    */
    putBoolArray(key: string, value: boolean[]): void
    
    /**
    * Stores a double precision floating point number under the given key.
    */
    putDouble(key: string, value: number): void
    
    /**
    * Stores a floating point value under the given key.
    */
    putFloat(key: string, value: number): void
    
    putFloat32Array(key: string, value: Float32Array): void
    
    putFloat64Array(key: string, value: Float64Array): void
    
    /**
    * Stores a floating point array under the given key.
    */
    putFloatArray(key: string, value: number[]): void
    
    /**
    * Stores an integer number value under the given key.
    */
    putInt(key: string, value: number): void
    
    putInt16Array(key: string, value: Int16Array): void
    
    putInt32Array(key: string, value: Int32Array): void
    
    putInt8Array(key: string, value: Int8Array): void
    
    /**
    * Stores an integer array under the given key.
    */
    putIntArray(key: string, value: number[]): void
    
    /**
    * Stores a mat2 value under the given key.
    */
    putMat2(key: string, value: mat2): void
    
    /**
    * Stores a mat2 array under the given key.
    */
    putMat2Array(key: string, value: mat2[]): void
    
    /**
    * Stores a mat3 value under the given key.
    */
    putMat3(key: string, value: mat3): void
    
    /**
    * Stores a mat3 array under the given key.
    */
    putMat3Array(key: string, value: mat3[]): void
    
    /**
    * Stores a mat4 value under the given key.
    */
    putMat4(key: string, value: mat4): void
    
    /**
    * Stores a mat4 array under the given key.
    */
    putMat4Array(key: string, value: mat4[]): void
    
    /**
    * Stores a quat value under the given key.
    */
    putQuat(key: string, value: quat): void
    
    /**
    * Stores a quat array under the given key.
    */
    putQuatArray(key: string, value: quat[]): void
    
    /**
    * Stores a string value under the given key.
    */
    putString(key: string, value: string): void
    
    /**
    * Stores a string array under the given key.
    */
    putStringArray(key: string, value: string[]): void
    
    putUint16Array(key: string, value: Uint16Array): void
    
    putUint32Array(key: string, value: Uint32Array): void
    
    putUint8Array(key: string, value: Uint8Array): void
    
    /**
    * Stores a vec2 value under the given key.
    */
    putVec2(key: string, value: vec2): void
    
    /**
    * Stores a vec2 array under the given key.
    */
    putVec2Array(key: string, value: vec2[]): void
    
    /**
    * Stores a vec3 value under the given key.
    */
    putVec3(key: string, value: vec3): void
    
    /**
    * Stores a vec3 array under the given key.
    */
    putVec3Array(key: string, value: vec3[]): void
    
    /**
    * Stores a vec4 value under the given key.
    */
    putVec4(key: string, value: vec4): void
    
    /**
    * Stores a vec4 array under the given key.
    */
    putVec4Array(key: string, value: vec4[]): void
    
    /**
    * Removes the value being stored under the given key. If no value exists under the key, nothing will happen.
    */
    remove(key: string): void
    
    /**
    * Callback function that gets called when the allowed storage limit has been passed.
    * The store won't be saved if it is full, so if this is called make sure to remove data until back under the limit.
    */
    onStoreFull: () => void
    
    /**
    * Creates a General Data Store.
    */
    static create(): GeneralDataStore
    
}

/**
* Namespace for location functionality.
*/
declare class GeoLocation {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Creates a new `LocationService`.
    */
    static createLocationService(): LocationService
    
    /**
    * Returns the GPS position of a custom location.
    */
    static getGeoPositionForLocation(location: LocationAsset): Promise<GeoPosition>
    
    /**
    * Calculates heading based on north aligned device orientation.
    */
    static getNorthAlignedHeading(northAlignedOrientation: quat): number
    
}

/**
* Enumeration of supported GPS location accuracy.

* > **Spectacles**: Enumeration of supported GPS location accuracy/settings for Spectacles. Location will be provided by several location sources and/or providers, each accuracy mode will provide a different configuration regarding update frequency and accuracy.

* @see Used By: {@link LocationService#accuracy}
*/
declare enum GeoLocationAccuracy {
    /**
    * Used for guiding the user. Generally accurate up to 5 meters.
    
    * > **Spectacles** : Used for guiding the user in navigation use cases. Accuracy is dependent on the environment (rural / urban) but generally accurate up to 20 meters. This configuration enables locations with location type {@link GeoPosition#locationSource | 'FUSED_LOCATION'} which is available at rendering rate frequency when reception is good. Otherwise, location updates are provided every 10 seconds.
    */
    Navigation,
    /**
    * Best possible accuracy without navigation requirement. Generally accurate up to 5 meters.
    
    * > **Spectacles** : Good accuracy without navigation requirement. Generally accurate up to 30 meters depending on the environment (rural/urban). Use for use cases where navigation is not a requirement and lower waiting times are expected. Location updates provided once per second under good conditions, otherwise one location every 10 seconds.
    */
    High,
    /**
    * Generally accurate up to 10 meters. Power efficient option.
    
    * > **Spectacles**: Generally accurate up to 30 meters depending on the environment (rural/urban). One location update is provided every 15 seconds. Power efficient option.
    */
    Medium,
    /**
    * Generally accurate up to 100 meters. The most power efficient option.
    
    * > **Spectacles**: Generally accurate up to 100 meters depending on the environment (rural/urban). One location update is provided every 30 seconds. The most power efficient option.
    */
    Low
}

/**
* The location of the device.

* @see Used By: {@link LocationService#getCurrentPosition}
* @see Returned By: {@link GeoPosition.create}
*/
declare class GeoPosition extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Represents the position's elevation from sea level in meters. Value will be zero if not available.
    */
    altitude: number
    
    /**
    * Represents the direction towards which the device is facing. This value, specified in degrees, indicates how far off from heading true north the device is. 0 degrees represents true north, and the direction is determined clockwise. The recommended alternative to this field is to use {@link LocationService#onNorthAlignedOrientationUpdate | onNorthAlignedOrientationUpdate} which provides more accurate and frequent updates.
    
    * > **Spectacles** Not supported, use {@link LocationService#onNorthAlignedOrientationUpdate | onNorthAlignedOrientationUpdate} instead
    
    * @readonly
    */
    heading: number
    
    /**
    * The accuracy of the latitude and longitude properties, expressed in meters.
    */
    horizontalAccuracy: number
    
    /**
    * Indicates whether the device is able to provide heading information.
    
    * > **Spectacles** Not supported, use {@link LocationService#onNorthAlignedOrientationUpdate | onNorthAlignedOrientationUpdate} instead
    
    * @readonly
    */
    isHeadingAvailable: boolean
    
    /**
    * The position's latitude in decimal degrees.
    */
    latitude: number
    
    /**
    * Represents the location source of the provided location
    
    * | Value | Description |
    * | ----- | ----------- |
    * | NOT_AVAILABLE | Unknown source. Non-wearables users will always receive this option |
    * | GNSS_RECEIVER | Uses built-in antenna to acquire a location. Not expected to work indoors or in challenging scenarios where reception is poor. |
    * | WIFI_POSITIONING_SYSTEM | Provides rough location in indoor and challenging scenarios. Useful for urban environments with no GPS reception.
    * | FUSED_LOCATION | Provides a fused location between several location sources. Accuracy is dependent on the accuracy of the sources. Update frequency coincides with rendering rate. Ideal for navigation scenarios. Enable this source by selecting {@link GeoLocationAccuracy#Navigation | Navigation Location Accuracy}.|
    */
    locationSource: string
    
    /**
    * The position's longitude in decimal degrees.
    */
    longitude: number
    
    /**
    * Represents the date and time when the location coordinates were acquired.
    */
    timestamp: Date
    
    /**
    * The accuracy of the altitude property, expressed in meters.
    */
    verticalAccuracy: number
    
    /**
    * Create a new GeoPosition.
    */
    static create(): GeoPosition
    
}

/**
* Allows to detect hand gestures using machine learning algorithms.

* @see [Gesture Module](https://developers.snap.com/spectacles/about-spectacles-features/apis/gesture-module) guide.

* @wearableOnly

* @example
* ```ts
* @component
* export class PinchExample extends BaseScriptComponent {
*   private gestureModule: GestureModule = require('LensStudio:GestureModule');
*   onAwake() {
*     this.gestureModule
*       .getPinchDownEvent(GestureModule.HandType.Right)
*       .add((pinchDownArgs: PinchDownArgs) => {
*         print('Right Hand Pinch Down');
*       });

*     this.gestureModule
*       .getPinchUpEvent(GestureModule.HandType.Right)
*       .add((pinchUpArgs: PinchUpArgs) => {
*         print('Right Hand Pinch Up');
*       });

*     this.gestureModule
*       .getFilteredPinchDownEvent(GestureModule.Handtype.Right)
*       .add((pinchDownArgs: PinchDownArgs) => {
*         print('Right Hand filtered Pinch Down');
*       });

*     this.gestureModule
*       .getFilteredPinchUpEvent(GestureModule.Handtype.Right)
*       .add((pinchUpArgs: PinchUpArgs) => {
*         print('Right Hand filtered Pinch Up');
*       });

*     this.gestureModule
*         .getPinchStrengthEvent(GestureModule.HandType.Right)
*         .add((pinchStrengthArgs: PinchStrengthArgs) => {
*             print('Right Hand Pinch Strength: ' + pinchStrengthArgs.strength);
*         });

*     this.gestureModule
*         .getGrabBeginEvent(GestureModule.HandType.Right)
*         .add((grabBeginArgs: GrabBeginArgs) => {
*             print('Right Hand Grab Begin');
*         });
*   }
* }
* ```
*/
declare class GestureModule extends Asset {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Triggered when the thumb and index fingers of the hand in view are pinched together. Compared to `getPinchDownEvent`, this event is more robust when moving the hand.
    
    * @wearableOnly
    */
    getFilteredPinchDownEvent(handType: GestureModule.HandType): event1<PinchDownArgs, void>
    
    /**
    * Triggered when the thumb and index fingers of the hand in view are separated after being pinched together. Compared to `getPinchUpEvent`, this event is more robust when moving the hand.
    
    * @wearableOnly
    */
    getFilteredPinchUpEvent(handType: GestureModule.HandType): event1<PinchUpArgs, void>
    
    /**
    * Triggered when the hand in view starts performing a grab pose, enabling interactions such as grabbing virtual objects or making a fist.
    
    * @wearableOnly
    */
    getGrabBeginEvent(handType: GestureModule.HandType): event1<GrabBeginArgs, void>
    
    /**
    * Triggered when the hand in view ends performing a grab pose and opens the hand, disabling interactions such as grabbing virtual objects or making a fist.
    
    * @wearableOnly
    */
    getGrabEndEvent(handType: GestureModule.HandType): event1<GrabEndArgs, void>
    
    /**
    * Triggered when a phone is detected in a hand. Note: Only smartphone-like objects are detected.
    
    * Details:
    * The event indicates that the state of a hand not holding a phone has changed to a hand holding a phone. If a Lens contains a `GestureModel` and has subscribed to `getIsPhoneInHandBeginEvent`, an initial event is always sent at the start of the Lens if a hand already holds a phone.
    
    * For technical reasons, if a Lens does not initially subscribe to `getIsPhoneInHandBeginEvent`, but subscribes at a later time, no initial event will be sent after subscription.
    
    * @wearableOnly
    */
    getIsPhoneInHandBeginEvent(handType: GestureModule.HandType): event1<IsPhoneInHandBeginArgs, void>
    
    /**
    * Triggered when a phone is no longer detected in a hand. Note: Only smartphone-like objects are considered.
    
    * Details:
    * The event indicates that the state of a hand holding a phone has changed to a hand not holding a phone. If a Lens contains a `GestureModel` and has subscribed to `getIsPhoneInHandEndEvent`, an initial event is always sent at the start of the Lens if a hand does not hold a phone.
    
    * For technical reasons, if a Lens does not initially subscribe to `getIsPhoneInHandEndEvent`, but subscribes at a later time, no initial event will be sent after subscription.
    
    * @wearableOnly
    */
    getIsPhoneInHandEndEvent(handType: GestureModule.HandType): event1<IsPhoneInHandEndArgs, void>
    
    /**
    * Triggered when the index finger of one hand taps the palm of the opposite hand.
    * Specifically, a right-hand palm tap is recognized when the right index finger touches the left palm, and a left-hand palm tap when the left index finger touches the right palm.
    
    * @wearableOnly
    */
    getPalmTapDownEvent(handType: GestureModule.HandType): event1<PalmTapDownArgs, void>
    
    /**
    * Triggered when the left index finger from one hand lifts from the palm on the opposite hand after touching. Currently, only the palm tap to the left hand is supported.
    
    * @wearableOnly
    */
    getPalmTapUpEvent(handType: GestureModule.HandType): event1<PalmTapUpArgs, void>
    
    /**
    * Triggered when the thumb and index fingers of the hand in view are pinched together.
    
    * @wearableOnly
    */
    getPinchDownEvent(handType: GestureModule.HandType): event1<PinchDownArgs, void>
    
    /**
    * Get the strength of a pinch between the thumb and index fingers of the hand in view.
    
    * @wearableOnly
    */
    getPinchStrengthEvent(handType: GestureModule.HandType): event1<PinchStrengthArgs, void>
    
    /**
    * Triggered when the thumb and index fingers of the hand in view are separated after being pinched together.
    
    * @wearableOnly
    */
    getPinchUpEvent(handType: GestureModule.HandType): event1<PinchUpArgs, void>
    
    /**
    * Triggered when the user has an intent to target a digital content in space.
    
    * @wearableOnly
    */
    getTargetingDataEvent(handType: GestureModule.HandType): event1<TargetingDataArgs, void>
    
}

declare namespace GestureModule {
    /**
    * The hand used in gesture detection with `GestureModule`.
    
    * @see Used By: {@link GestureModule#getFilteredPinchDownEvent}, {@link GestureModule#getFilteredPinchUpEvent}, {@link GestureModule#getGrabBeginEvent}, {@link GestureModule#getGrabEndEvent}, {@link GestureModule#getIsPhoneInHandBeginEvent}, {@link GestureModule#getIsPhoneInHandEndEvent}, {@link GestureModule#getPalmTapDownEvent}, {@link GestureModule#getPalmTapUpEvent}, {@link GestureModule#getPinchDownEvent}, {@link GestureModule#getPinchStrengthEvent}, {@link GestureModule#getPinchUpEvent}, {@link GestureModule#getTargetingDataEvent}
    
    * @wearableOnly
    */
    enum HandType {
        /**
        * The left hand.
        
        * @wearableOnly
        */
        Left,
        /**
        * The right hand.
        
        * @wearableOnly
        */
        Right
    }

}

/**
* @wearableOnly
*/
declare class GesturesDataArgs extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Represents a GLTF 3D Model.

* @see Used By: {@link RemoteMediaModule#loadResourceAsGltfAsset}

* @example
* ```
* // @input Asset.BitmojiModule bitmojiModule
* // @input Asset.RemoteMediaModule remoteMediaModule
* // @input Asset.Material pbrMaterialHolder

* script.bitmojiModule.requestBitmoji3DResource(
*     function (bitmoji3DResource) {
*       script.remoteMediaModule.loadResourceAsGltfAsset(
*         bitmoji3DResource,
*         onDownloaded,
*         onFail
*       )
*     }
*   )

* function onDownloaded (gltfAsset){
*   var root = scene.createSceneObject("BitmojiAvatar");
*   var gltfSettings = GltfSettings.create();
*   gltfSettings.convertMetersToCentimeters = true;
*   var avatar = gltfAsset.tryInstantiateWithSetting(root, script.pbrMaterialHolder, gltfSettings);
* }

* function onFail (e){
*     print(e);
* }
* ```
*/
declare class GltfAsset extends Asset {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Use this method to get a dynamic resource for the url from the gltf extras.
    */
    getResourceFromExtras(url: string): DynamicResource
    
    /**
    * Try instantiating an object from the GLTF asset.
    */
    tryInstantiate(parent: SceneObject, material: Material): SceneObject
    
    /**
    * Asynchronously try instantiating an object from the GLTF asset. Useful to prevent frame drops when loading multiple assets simultaenously as to not block the the thread.
    */
    tryInstantiateAsync(parent: SceneObject, material: Material, onSuccess: (sceneObject: SceneObject) => void, onFailure: (error: string) => void, onProgress: (progress: number) => void, gltfSettings?: GltfSettings): void
    
    /**
    * Try instantiating an object from the GLTF asset with supplied GltfSetting
    */
    tryInstantiateWithSetting(parent: SceneObject, material: Material, gltfSettings: GltfSettings): SceneObject
    
    /**
    * @readonly
    */
    extras: string
    
}

/**
* Settings for importing a glTF Asset.
* Use this with {@link GltfAsset} component's `tryInstantiateWithSetting` method.

* @see Used By: {@link GltfAsset#tryInstantiateAsync}, {@link GltfAsset#tryInstantiateWithSetting}
* @see Returned By: {@link GltfSettings.create}
*/
declare class GltfSettings extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Units for all linear distances in glTF are meters and in LensStudio are centimeters, enable this option if you want to automatically scale from meters into centimeters when importing the glTF file.
    */
    convertMetersToCentimeters: boolean
    
    /**
    * Whether the GLB Loader should optimize geometry.
    */
    optimizeGeometry: boolean
    
    /**
    * Controls whether the triangle order is retained in Gltf asset.
    */
    storeTriangleOrder: boolean
    
    static create(): GltfSettings
    
}

/**
* The arguments of the GrabBegin event on `GestureModule`. Currently unused.

* @wearableOnly
*/
declare class GrabBeginArgs extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
}

/**
* The arguments of the GrabEnd event on `GestureModule`. Currently unused.

* @wearableOnly
*/
declare class GrabEndArgs extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
}

/**
* This class has been **Deprecated**. Please instead use the {@link DeviceTracking} component with Tracking Mode set to Rotation.
* See the [Tracking Modes](https://developers.snap.com/lens-studio/features/ar-tracking/world/tracking-modes) guide for more information.

* Applies the device's gyroscope rotation to the SceneObject it is attached to.

* @deprecated

* @example
* ```
* //@input Component.Gyroscope gyro

* script.gyro.invertOnFrontCamera = true;
* script.gyro.invertRotation = true;
* ```
*/
declare class Gyroscope extends Component {
    
    /** @hidden */
    protected constructor()
    
    /**
    * If enabled, the Gyroscope's rotation will be inverted when the front facing camera is active.
    
    * @deprecated
    */
    invertOnFrontCamera: boolean
    
    /**
    * If enabled, the Gyroscope's rotation will be inverted.
    
    * @deprecated
    */
    invertRotation: boolean
    
}

/**
* Hair asset converted from an FBX containing splines to be used with {@link HairVisual}.
*/
declare class HairDataAsset extends Asset {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Used to simulate and render hairstyles with realistic lighting and physics.

* @see [Hair Component](https://developers.snap.com/lens-studio/features/ar-tracking/face/hair-simulation) guide.
* @see [Hair Simulation](https://developers.snap.com/lens-studio/features/ar-tracking/face/face-templates/hair-simulation) example.

* @see Used By: {@link HairVisual#onInitialized}

* @example
* ```javascript
* //@input Component.HairVisual hair

* var width = 1;
* var wind = new vec3(5,0,0);

* script.hair.onInitialized = InitializeHair;

* function InitializeHair(hv){
*     //change hair width
*     hv.strandWidth = width;

*     //add wind to hair
*     hv.windEnabled = true;
*     hv.windForce = wind;
* }
* ```
*/
declare class HairVisual extends BaseMeshVisual {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Adds a collider to the simulation.
    */
    addCollider(colliderComponent: ColliderComponent): void
    
    /**
    * Removes all hair colliders from the hair simulation.
    */
    clearColliders(): void
    
    /**
    * Returns `true` if hair simulation is supported by the device.
    */
    isHardwareSupported(): boolean
    
    /**
    * Returns `true` if the hair resources are initialized.
    */
    isInitialized(): boolean
    
    /**
    * Removes a collider from the simulation by index and returns it.
    */
    removeColliderByIndex(index: number): ColliderComponent
    
    /**
    * Resets the simulation. May be called if the object with the hair simulation is toggled between enabled and disabled.
    */
    resetSimulation(): void
    
    /**
    * Density of each clump, higher number would result in "messier" looking hair.
    */
    clumpDensity: number
    
    /**
    * Offset amount of hair roots.
    */
    clumpRadius: number
    
    /**
    * Offset amount of hair tips.
    */
    clumpTipScale: number
    
    /**
    * List of all colliders assigned to the hair simulation of the current hair visual.
    */
    colliders: ColliderComponent[]
    
    /**
    * Determines if collision is enabled for hair.
    */
    collisionEnabled: boolean
    
    /**
    * Determines amount of friction for collision between hair strands.
    */
    collisionFriction: number
    
    /**
    * Determines how much position offset collision will cause.
    */
    collisionOffset: number
    
    /**
    * Determines how stiff the collision is.
    */
    collisionStiffness: number
    
    /**
    * Higher value of damp will cause hair to have less free movement.
    */
    damp: number
    
    /**
    * Amount of density created to thicken hair.
    */
    density: number
    
    /**
    * @deprecated
    */
    fallbackHairMaterial: Material
    
    /**
    * This enables fallback mode which turns off physics simulations; it is recommended to enable fallback mode on lower-end devices. It will be enabled automatically if device doesn't support hair simulation feature.
    */
    fallbackModeEnabled: boolean
    
    /**
    * Amount of friction for hair strands.
    */
    friction: number
    
    /**
    * Gravity force that gets added for hair's physics simulation.
    */
    gravity: vec3
    
    /**
    * Material used by this simulation to render hair.
    */
    hairMaterial: Material
    
    /**
    * Determines the amount of vertices on each strand. `0` means using original number of the strands. `1` is the smallest resolution for hair. Keep in mind when you change this property, the hair geometry will be regenerated.
    */
    hairResolution: number
    
    /**
    * Offset of hair density.
    */
    noise: number
    
    /**
    * This function gets called when the hair visual is initialized in the Lens.
    */
    onInitialized: (hairVisual: HairVisual) => void
    
    /**
    * @deprecated
    */
    primaryHairMaterial: Material
    
    /**
    * If enabled, hair strands will collide with themselves.
    */
    selfCollisionEnabled: boolean
    
    /**
    * Friction of collisions between hair strands.
    */
    selfCollisionFriction: number
    
    /**
    * Radius limit of collisions between hair strands.
    */
    selfCollisionRadius: number
    
    /**
    * Determines how stiff collision between hair strands are.
    */
    selfCollisionStiffness: number
    
    /**
    * Higher value of stiffness will make hair move less.
    */
    stiffness: number
    
    /**
    * The taper size of each hair towards the tip.
    */
    strandTaper: number
    
    /**
    * The Width of each hair strand.
    */
    strandWidth: number
    
    /**
    * If checked will add wind force to hair simulation.
    */
    windEnabled: boolean
    
    /**
    * Amount of wind force added to hair simulation.
    */
    windForce: vec3
    
}

/**
* Provides additional data for the tracked hand. You can figure out whether the tracked hand is the left hand by accessing the *isLeft* property [true/false], as well as the probability of this data through the  *isLeftProbability*  property [0-1].

* @example
* ```javascript
* //@input Component.ObjectTracking handTracking

* print(handTracking.objectSpecificData.isLeft)
* print(handTracking.objectSpecificData.isLeftProbability)
* ```
*/
declare class HandSpecificData extends ObjectSpecificData {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Enables Hand Tracking 3D for the {@link ObjectTracking3D} component.

* @see [3D Body and Hand Tracking](https://developers.snap.com/lens-studio/features/ar-tracking/body/object-tracking-3d) guide.
*/
declare class HandTracking3DAsset extends Object3DAsset {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Triggers haptic feedback on the device. (iOS Devices only)
*/
declare class HapticFeedbackSystem extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Trigger a haptic feedback.
    */
    hapticFeedback(type: HapticFeedbackType): void
    
}

/**
* The method in which haptic feedback is provided. Use with the `HapticFeedbackSystem`.

* @see Used By: {@link HapticFeedbackSystem#hapticFeedback}
*/
declare enum HapticFeedbackType {
    /**
    * Taptic engine based haptic feedback. Available only on some devices.
    */
    TapticEngine,
    /**
    * Vibration based haptic feedback.
    */
    Vibration
}

/**
* Used to move and rotate {@link SceneObject}s in sync with the user's head movements.

* @see [Head Attached 3D Objects](https://developers.snap.com/lens-studio/features/ar-tracking/face/head-attached-3d-objects) guide.

* @example
* ```js
* // Switch to the next available face
* //@input Component.Head head
* var nextFaceIndex = script.head.faceIndex + 1;
* if(nextFaceIndex >= script.head.getFacesCount())
* nextFaceIndex = 0;
* script.head.faceIndex = nextFaceIndex;
* ```

* ```js
* // Change the attachment point type to HeadCenter
* //@input Component.Head head
* script.head.setAttachmentPointType(AttachmentPointType.HeadCenter);
* ```
*/
declare class Head extends Component {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns the total number of faces currently being tracked.
    
    * @deprecated
    */
    getFacesCount(): number
    
    /**
    * Returns the screen position of the face landmark at the passed in index.
    
    * @deprecated
    */
    getLandmark(index: number): vec2
    
    /**
    * Returns the number of face landmarks being tracked.
    
    * @deprecated
    */
    getLandmarkCount(): number
    
    /**
    * Returns a list of screen positions of all tracked landmarks.
    
    * @deprecated
    */
    getLandmarks(): vec2[]
    
    /**
    * Changes the attachment point type used to anchor this object to a face.
    */
    setAttachmentPointType(attachmentPointType: AttachmentPointType): void
    
    /**
    * The index of the face this head is attached to.
    */
    faceIndex: number
    
    /**
    * Triggered when the landmarks on the head are updated.
    
    * @readonly
    */
    onLandmarksUpdate: event1<vec2[], void>
    
    /**
    * The tracking context this effect is being applied to.
    */
    trackingScope: (PersonTrackingScope|TextureTrackingScope|FaceTrackingScope)
    
}

/**
* Headers for the Fetch API in {@link RemoteServiceModule}. Allows you to perform actions on HTTP request and response headers, like retrieving, setting, adding to, and removing headers.

* You can retrieve a Headers object via the {@link Request.headers} and {@link Response.headers} properties, and create a new Headers object using the Headers() constructor.

* @see Used By: {@link Request#headers}, {@link Response#headers}

* @wearableOnly

* @example
* ```js
* let myHeaders = new Headers();

* myHeaders.append("Content-Type", "text/xml");
* myHeaders.get("Content-Type"); // should return 'text/xml'
* ```
*/
declare class Headers extends ScriptObject {
    /**
    * Construct a new, empty Headers object.
    
    * @wearableOnly
    */
    constructor()
    
    /**
    * Append a new value onto an existing header inside a Headers object, or adds the header if it does not already exist.
    
    * The difference between set() and append() is that if the specified header already exists and accepts multiple values, set() will overwrite the existing value with the new one, whereas append() will append the new value onto the end of the set of values.
    
    * ```
    * myHeaders.append("Accept-Encoding", "deflate");
    * myHeaders.append("Accept-Encoding", "gzip");
    * myHeaders.get("Accept-Encoding"); // Returns 'deflate, gzip'
    * ```
    
    * @wearableOnly
    */
    append(name: string, value: string): void
    
    /**
    * Deletes a header from the Headers object.
    
    * ```
    * myHeaders.append("Content-Type", "image/jpeg");
    * myHeaders.get("Content-Type"); // Returns 'image/jpeg'
    * myHeaders.delete("Content-Type");
    * myHeaders.get("Content-Type"); // Returns null, as it has been deleted
    * ```
    
    * @wearableOnly
    */
    delete(name: string): void
    
    /**
    * Returns an iterator allowing to go through all key/value pairs contained in this object. Both the key and value of each pair are String objects.
    
    * ```
    * // Print the key/value pairs
    * for (const pair of myHeaders.entries()) {
    *     print(`${pair[0]}: ${pair[1]}`);
    * }
    * ```
    
    * @wearableOnly
    */
    entries(): string[][]
    
    /**
    * Returns a comma-separated string of all the values of a header within a Headers object with a given name. If the requested header doesn't exist in the Headers object, returns null.
    
    * ```
    * myHeaders.append("Accept-Encoding", "deflate");
    * myHeaders.append("Accept-Encoding", "gzip");
    * myHeaders.get("Accept-Encoding"); // Returns "deflate, gzip"
    * ```
    
    * @wearableOnly
    */
    get(name: string): string
    
    /**
    * Returns a boolean stating whether the Headers object contains the given header.
    
    * @wearableOnly
    */
    has(name: string): boolean
    
    /**
    * Returns an iterator allowing you to go through all keys contained in the Headers. The keys are String objects.
    
    * ```
    * // Print the keys
    * for (const key of myHeaders.keys()) {
    *   print(key);
    * }
    * ```
    
    * @wearableOnly
    */
    keys(): string[]
    
    /**
    * Sets a new value for an existing header inside a Headers object, or adds the header if it does not already exist.
    
    * The difference between set() and Headers.append is that if the specified header already exists and accepts multiple values, set() overwrites the existing value with the new one, whereas Headers.append appends the new value to the end of the set of values.
    
    * ```
    * myHeaders.set("Accept-Encoding", "deflate");
    * myHeaders.set("Accept-Encoding", "gzip");
    * myHeaders.get("Accept-Encoding"); // Returns 'gzip'
    * ```
    
    * @wearableOnly
    */
    set(name: string, value: string): void
    
    /**
    * Returns an iterator allowing you to go through all values contained in the Headers. The values are String objects.
    
    * ```
    * // Print the values
    * for (const value of myHeaders.values()) {
    *   print(value);
    * }
    * ```
    
    * @wearableOnly
    */
    values(): string[]
    
}

/**
* Constraints translation and a single axis of rotation.  See also: {@link ConstraintComponent}.

* @example
* ```js
* // Create a constraint on an object
* var hingeConstraint = hingeObj.createComponent("Physics.ConstraintComponent");

* // Set up options on that constraint
* hingeConstraint.debugDrawEnabled = true;
* hingeConstraint.constraint = Physics.Constraint.create(Physics.ConstraintType.Hinge);
* ```
*/
declare class HingeConstraint extends Constraint {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Settings limiting the behaviour of hinge constraint in respect to the applied forces.
    */
    limitSettings: HingeLimitSettings
    
    /**
    * Settings describing the independent force (motor) applied to the constraint.
    */
    motorSettings: HingeMotorSettings
    
}

/**
* Settings that describe how the position of the object should be limited in respect to the applied forces.

* @see Used By: {@link HingeConstraint#limitSettings}

* @example
* ```js
* //@input Physics.ConstraintComponent constraintComponent

* let constraint = script.constraintComponent.constraint;

* let limitSettings = constraint.limitSettings;
* limitSettings.enabled = true;
* limitSettings.low = 0;
* limitSettings.high = Math.PI;
* limitSettings.bias = 3;
* limitSettings.relaxation = 2;
* constraint.limitSettings = limitSettings;
* ```
*/
declare class HingeLimitSettings {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Controls how strictly constraint respects the limits.
    */
    bias: number
    
    /**
    * Enable or disable constraint limits.
    */
    enabled: boolean
    
    /**
    * Maximum allowed angle (in radians), relative to the constraint resting state.
    */
    high: number
    
    /**
    * Minimum allowed angle (in radians), relative to the constraint resting state.
    */
    low: number
    
    /**
    * Controls how strictly the limit is enforced. Greater values relate to more "bouncy" behaviour of the constraint.
    */
    relaxation: number
    
}

/**
* Settings describing the independent force (motor) applied to the constraint. Motor within a hinge constraint is used to create controlled, powered movement along the hinge's axis, allowing objects to rotate automatically or maintain a particular speed or position.

* @see Used By: {@link HingeConstraint#motorSettings}

* @example
* ```js
* //@input Physics.ConstraintComponent constraintComponent

* let constraint = script.constraintComponent.constraint;

* let motorSettings = constraint.motorSettings;
* motorSettings.enabled = true;
* motorSettings.maxImpulse = 1;
* motorSettings.targetType = HingeMotorType.AngleTarget;
* motorSettings.targetValue = Math.PI/2;
* motorSettings = motorSettings;
* ```
*/
declare class HingeMotorSettings {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Enable or disable motor settings.
    */
    enabled: boolean
    
    /**
    * Maximum force allowed to be applied to constraint.
    */
    maxImpulse: number
    
    /**
    * A type of the applied motor.
    */
    targetType: HingeMotorType
    
    /**
    * Target value of the motor depending on type: speed or angle.
    */
    targetValue: number
    
}

/**
* Enum that defines motor type.

* @see Used By: {@link HingeMotorSettings#targetType}

* @example
* ```js
* //@input Physics.ConstraintComponent constraintComponent

* let constraint = script.constraintComponent.constraint;

* let motorSettings = constraint.motorSettings;
* motorSettings.enabled = true;
* motorSettings.maxImpulse = 10;
* motorSettings.targetType = HingeMotorType.AngleTarget;
* motorSettings.targetValue = Math.PI/2;
* motorSettings = motorSettings;
* ```
*/
declare enum HingeMotorType {
    /**
    * A motor that attempts to rotate to or hold a specific angle or position along the hinge axis.
    */
    AngleTarget,
    /**
    * A motor that applies torque to reach or maintain a target speed (angular velocity).
    */
    VelocityTarget
}

/**
* Used to display text hints in a Lens.

* @see [Scripting Hints Guide](https://developers.snap.com/lens-studio/essential-skills/adding-interactivity/additional-examples/scripting-hints).

* @remarks
* If you only need to show one hint on Lens start up, you can [configure your project](https://developers.snap.com/lens-studio/publishing/configuring/lens-hints) to display the hint without scripting it.


* | Hint ID                                   | Hint Message                          |
* |------------------------------------------|---------------------------------------|
* | "lens_hint_blow_a_kiss"                  | "Blow A Kiss"                         |
* | "lens_hint_come_closer"                  | "Come Closer"                         |
* | "lens_hint_do_not_smile"                 | "Do Not Smile"                        |
* | "lens_hint_do_not_try_with_a_friend"     | "Do Not Try With A Friend"            |
* | "lens_hint_find_face"                    | "Find Face"                           |
* | "lens_hint_keep_raising_your_eyebrows"   | "Keep Raising Your Eyebrows"          |
* | "lens_hint_kiss"                         | "Kiss"                                |
* | "lens_hint_kiss_again"                   | "Kiss Again"                          |
* | "lens_hint_look_around"                  | "Look Around"                         |
* | "lens_hint_look_down"                    | "Look Down"                           |
* | "lens_hint_look_left"                    | "Look Left"                           |
* | "lens_hint_look_right"                   | "Look Right"                          |
* | "lens_hint_look_up"                      | "Look Up"                             |
* | "lens_hint_make_some_noise"              | "Make Some Noise!"                    |
* | "lens_hint_nod_your_head"                | "Nod Your Head"                       |
* | "lens_hint_now_kiss"                     | "Now Kiss"                            |
* | "lens_hint_now_open_your_mouth"          | "Now Open Your Mouth"                 |
* | "lens_hint_now_raise_your_eyebrows"      | "Now Raise Your Eyebrows"             |
* | "lens_hint_now_smile"                    | "Now Smile"                           |
* | "lens_hint_open_your_mouth"              | "Open Your Mouth"                     |
* | "lens_hint_open_your_mouth_again"        | "Open Your Mouth Again"               |
* | "lens_hint_raise_eyebrows_or_open_mouth" | "Raise Your Eyebrows / Or / Open Your Mouth" |
* | "lens_hint_raise_your_eyebrows"          | "Raise Your Eyebrows"                 |
* | "lens_hint_raise_your_eyebrows_again"    | "Raise Your Eyebrows Again"           |
* | "lens_hint_smile"                        | "Smile"                               |
* | "lens_hint_smile_again"                  | "Smile Again"                         |
* | "lens_hint_swap_camera"                  | "Swap Camera"                         |
* | "lens_hint_tap"                          | "Tap!"                                |
* | "lens_hint_tap_a_surface"                | "Tap A Surface"                       |
* | "lens_hint_tap_ground"                   | "Tap The Ground"                      |
* | "lens_hint_tap_ground_to_place"          | "Tap Ground To Place"                 |
* | "lens_hint_tap_surface_to_place"         | "Tap Surface To Place"                |
* | "lens_hint_try_friend"                   | "Try It With A Friend"                |
* | "lens_hint_try_rear_camera"              | "Try It With Your Rear Camera"        |
* | "lens_hint_turn_around"                  | "Turn Around"                         |
* | "lens_hint_walk_through_the_door"        | "Walk Through The Door"               |

* @example
* ```js
* // Create the hints component
* var hintsComponent = script.getSceneObject().createComponent("Component.HintsComponent");

* // Show "Smile" hint
* hintsComponent.showHint("lens_hint_smile", -1);

* // Wait for smile...
* script.createEvent("SmileStartedEvent").bind(function()
* {
*     // Hide the "Smile" hint
*     hintsComponent.hideHint("lens_hint_smile");
* });
* ```
*/
declare class HintsComponent extends Component {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Hides the hint with id `hintID`.
    */
    hideHint(hintID: string): boolean
    
    /**
    * Shows the hint with id `hintID` for a duration of `duration` seconds. Use a duration of -1 to keep the hint onscreen forever.
    */
    showHint(hintID: string, duration: number): boolean
    
}

/**
* Class responsible for detecting intersections between a virtual ray and real-world surfaces.

* @see Returned By: {@link WorldQueryModule#createHitTestSession}, {@link WorldQueryModule#createHitTestSessionWithOptions}

* @wearableOnly

* @example
* ```
* const worldQuery = require("LensStudio:WorldQueryModule")
* // Create a session with default options (currently, filtering disabled by default)
* var hitTestSession = worldQuery.createHitTestSession();

* // Create a second session with a smoothing filter applied to the hit test result
* var options = HitTestSessionOptions.create();
* options.filter = true;
* var hitTestSessionWithOptions = worldQuery.createHitTestSessionWithOptions(options);

* // Depth computation is started once a session is started
* // Multiple sessions access the same depth data, thus there is no additional cost
* hitTestSession.start()
* hitTestSessionWithOptions.start()

* hitTestSession.hitTest(rayStart, rayEnd, onHitTestResult1);
* hitTestSessionWithOptions.hitTest(rayStart, rayEnd, onHitTestResult2);

* // Depth computation stops once all hit test sessions are stopped
* hitTestSession.stop()
* hitTestSessionWithOptions.stop()
* ```
*/
declare class HitTestSession extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Perform a hit test.
    
    * @wearableOnly
    */
    hitTest(rayStart: vec3, rayEnd: vec3, hitCallback: (hit: WorldQueryHitTestResult) => void): void
    
    /**
    * Reset the session.
    
    * @wearableOnly
    */
    reset(): void
    
    /**
    * Start the session. Depth computation is started once a session is started. Multiple sessions access the same depth data, thus there is no additional cost.
    
    * @wearableOnly
    */
    start(): void
    
    /**
    * Stop the session. Depth computation stops once all hit test sessions are stopped.
    
    * @wearableOnly
    */
    stop(): void
    
}

/**
* Options for configuring a HitTestSession.

* @see Used By: {@link WorldQueryModule#createHitTestSessionWithOptions}
* @see Returned By: {@link HitTestSessionOptions.create}

* @wearableOnly

* @example
* ```
* const worldQuery = require("LensStudio:WorldQueryModule");
* var options = HitTestSessionOptions.create();
* options.filter = true;
* var hitTestSessionWithOptions = worldQuery.createHitTestSessionWithOptions(options);
* ```
*/
declare class HitTestSessionOptions extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * If true - a double exponential filter is applied to filter/smooth over multiple hit test results.
    * By default the filter is set to `false`.
    
    * @wearableOnly
    */
    filter: boolean
    
    /**
    * Create a new HitTestSessionOptions object.
    
    * @wearableOnly
    */
    static create(): HitTestSessionOptions
    
}

/**
* Used by the `horizontalAlignment` property in {@link MeshVisual}.
* When a {@link ScreenTransform} is attached to the same SceneObject, this determines how the mesh will be positioned horizontally.

* @see Used By: {@link BaseMeshVisual#horizontalAlignment}, {@link TransformerBuilder#setHorizontalAlignment}

* @example
* ```js
* // @input Component.MeshVisual meshVisual
* script.meshVisual.horizontalAlignment = HorizontalAlignment.Left;
* ```
*/
declare enum HorizontalAlignment {
    /**
    * The mesh will be aligned to the left side.
    */
    Left,
    /**
    * The mesh will be centered.
    */
    Center,
    /**
    * The mesh will be aligned to the right side.
    */
    Right
}

/**
* Options for wrapping text horizontally.
* Used by {@link Text}'s component's `horizontalOverflow` property.

* @see Used By: {@link Text#horizontalOverflow}, {@link Text3D#horizontalOverflow}

* @example
* ```js
* // @input Component.Text textComponent

* script.textComponent.horizontalOverflow = HorizontalOverflow.Wrap;
* ```
*/
declare enum HorizontalOverflow {
    /**
    * Text will continue drawing past horizontal boundaries.
    */
    Overflow,
    /**
    * Text is clipped to the width of horizontal boundaries.
    */
    Truncate,
    /**
    * Text wraps when reaching horizontal boundaries and continues on the next line.
    */
    Wrap,
    /**
    * Text will shrink to fit within the horizontal boundaries.
    */
    Shrink,
    /**
    * When text exceeds the available space an ellipsis (...) will be added at the end.
    */
    Ellipsis,
    /**
    * When text exceeds the available space in the front, an ellipsis (...) will be added at the front.
    */
    EllipsisFront,
    /**
    * Text is clipped in the front to the width of horizontal boundaries.
    */
    TruncateFront
}

/**
* Triggered when hovering state has ended on an {@link InteractionComponent}.
*/
declare class HoverEndEvent extends SceneObjectEvent {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Get the screen space of the hover position.
    */
    getHoverPosition(): vec2
    
}

/**
* The arguments passed in by {@link HoverEndEvent}.
*/
declare class HoverEndEventArgs extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The position of the event.
    
    * @readonly
    */
    position: vec2
    
}

/**
* Triggered when a mouse hover event occurs. Only triggered in the `Preview` panel of Lens studio. Useful when working with Spectacles, where you can simulate the use of your hand to hover over an object. Does not get triggered on mobile.
*/
declare class HoverEvent extends SceneObjectEvent {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The normalized screen position of the hover event.
    */
    getHoverPosition(): vec2
    
}

/**
* Triggered when hovering is occuring on an {@link InteractionComponent}.
*/
declare class HoverEventArgs extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * @readonly
    */
    position: vec2
    
}

/**
* Triggered when hovering state has started on an {@link InteractionComponent}.
*/
declare class HoverStartEvent extends SceneObjectEvent {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Get the screen space of the hover position.
    */
    getHoverPosition(): vec2
    
}

/**
* The arguments passed in by {@link HoverStartEvent}.
*/
declare class HoverStartEventArgs extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The position of the event.
    
    * @readonly
    */
    position: vec2
    
}

/**
* The base class for parameter objects passed into event callbacks.
*/
declare class IEventParameters extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Used to display 2D textures within a scene.

* @remarks
* Commonly used with {@link ScreenTransform} for drawing images on the screen.

* See the [Image](https://developers.snap.com/lens-studio/assets-pipeline/2d/image) guide.

* @example
* ```js
* //@input Component.Image image
* //@input Asset.Texture texture

* // Set the Image component's texture
* script.image.mainPass.baseTex = script.texture;

* // Set the Image's pivot point
* script.image.pivot = new vec2(1, 1);
* ```
*/
declare class Image extends MaterialMeshVisual {
    
    /** @hidden */
    protected constructor()
    
    /**
    * If enabled, the drawn image will be flipped horizontally.
    */
    flipX: boolean
    
    /**
    * If enabled, the drawn image will be flipped vertically.
    */
    flipY: boolean
    
    /**
    * The location of the Image's pivot point relative to its boundaries.
    * Where (-1, -1) is the bottom left corner, (0, 0) is the center, and (1, 1) is the top right corner of the Image.
    */
    pivot: vec2
    
    /**
    * Rotate the image without manipulating the transforms.
    */
    rotationAngle: number
    
}

/**
* Spectacles: ImageFrame contains the results of a still image request initiated from the {@link CameraModule}. Still images are high resolution images of the user's current camera stream.

* @exposesUserData

* @wearableOnly

* @example
* ```js
* let cameraModule = require("LensStudio:CameraModule");
* let imageRequest = CameraModule.createImageRequest();

* try {
*     let imageFrame = await cameraModule.requestImage(imageRequest);

*     // Use the texture in some visual
*     script.image.mainPass.baseTex = imageFrame.texture;
*     let timestamp = imageFrame.timestampMillis; // scene-relative time
* } catch (error) {
*     print(`Still image request failed: ${error}`);
* }
* ```
*/
declare class ImageFrame extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The texture of the ImageFrame.
    
    * @readonly
    
    * @exposesUserData
    
    * @wearableOnly
    */
    texture: Texture
    
}

/**
* Controls an image picker texture and UI.
* Can be accessed through {@link Texture | Texture.control} on an Image Picker texture.
* For more information, see the [Image Picker Texture](https://developers.snap.com/lens-studio/4.55.1/references/guides/lens-features/tracking/face/face-effects/face-image-picker-texture) guide.

* @deprecated

* @example
* ```js
* // @input Asset.Texture imagePickerTexture

* // Show the image picker UI
* script.imagePickerTexture.control.showImagePicker();

* // Set a callback for when the user picks a different image from the image picker
* script.imagePickerTexture.control.setImageChangedCallback(function(){
*     print("image picker image changed");
* });
* ```
*/
declare class ImagePickerTextureProvider extends MediaPickerTextureProvider {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Hides the image picker UI.
    
    * @deprecated
    */
    hideImagePicker(): void
    
    /**
    * Binds a callback function for when the user selects or changes an image from the picker.
    
    * @deprecated
    */
    setImageChangedCallback(callback: () => void): void
    
    /**
    * Shows the image picker UI.
    
    * @deprecated
    */
    showImagePicker(): void
    
    /**
    * If enabled, the image picker UI will be shown automatically when the Lens starts.
    
    * @deprecated
    */
    autoShowImagePicker: boolean
    
}

/**
* Builds InputPlaceHolders for MLComponent.

* @see Returned By: {@link InputBuilder#setInputTexture}, {@link InputBuilder#setName}, {@link InputBuilder#setSampler}, {@link InputBuilder#setShape}, {@link InputBuilder#setTransformer}, {@link MachineLearning.createInputBuilder}

* @example
* ```js
* //@input vec2 inputSize = {64, 64}
* //@input string inputName = "input"

* var inputChannels = 3;

* var inputBuilder = MachineLearning.createInputBuilder();
* inputBuilder.setName(script.inputName);
* inputBuilder.setShape(new vec3(script.inputSize.x, script.inputSize.y, inputChannels));

* var inputPlaceholder = inputBuilder.build();
* ```
*/
declare class InputBuilder extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Builds and returns a new InputPlaceholder.
    */
    build(): InputPlaceholder
    
    /**
    * Sets the input texture of the InputPlaceholder to be built.
    */
    setInputTexture(texture: Texture): InputBuilder
    
    /**
    * Sets the name of the InputPlaceholder to be built.
    */
    setName(name: string): InputBuilder
    
    /**
    * Sets sampler for input placeholder builder.
    */
    setSampler(sampler: Sampler): InputBuilder
    
    /**
    * Sets the shape of the InputPlaceholder to be built.
    */
    setShape(shape: vec3): InputBuilder
    
    /**
    * Sets the Transformer of the InputPlaceholder to be built.
    */
    setTransformer(transformer: Transformer): InputBuilder
    
}

/**
* Controls input data for a neural network used by an MLComponent.
* For more information, see the [MLComponent Scripting](https://developers.snap.com/lens-studio/features/snap-ml/ml-component/scripting-ml-component) guide.

* @see Returned By: {@link InputBuilder#build}, {@link MLComponent#getInput}

* @example
* ```js
* //@input Component.MLComponent mlComponent
* //@input Asset.Texture inputTexture

* script.mlComponent.onLoadingFinished = onLoadingFinished;

* function onLoadingFinished() {
*     var input = script.mlComponent.getInput(script.inputName);
*     input.texture = script.inputTexture;
* }
* ```
*/
declare class InputPlaceholder extends BasePlaceholder {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Data used as input.
    
    * @readonly
    */
    data: Float32Array
    
    /**
    * Texture used as input.
    */
    texture: Texture
    
}

/**
* The base class for animation tracks using integer values.

* @deprecated
*/
declare class IntAnimationTrack extends AnimationTrack {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Enables invoking touch interactions with a {@link BaseMeshVisual} rendered to specific {@link Camera}.

* @remarks
* Sometimes touch events within lens may collide with Snapchat touch events. To avoid this use Touch Blocking.

* @see [Touch And Interactions](https://developers.snap.com/lens-studio/features/scripting/touch-input) guide.
* @see [Touch Blocking](https://developers.snap.com/lens-studio/features/scripting/touch-input#touch-blocking) guide.

* @see Used By: {@link Text#touchHandler}, {@link Text3D#touchHandler}

* @example
* ```js
* // Pass in a MeshVisual
* // @input Component.BaseMeshVisual myMeshVisual

* // Get a reference to the InteractionComponent
* var interactionComponent = script.getComponent("Component.InteractionComponent");

* // Sets the MeshVisual to handle touches
* interactionComponent.addMeshVisual(script.myMeshVisual);

* // Allow certain touch types to be passed to Snapchat
* interactionComponent.addTouchBlockingException("TouchTypeDoubleTap");
* ```

* ```js
* //@input Component.InteractionComponent interactionComponent

* // Subscribe to the onTap event
* var onTapEvent = script.interactionComponent.onTap.add(function(tapEventArgs){
*     print("onTap!");
* });

* // Unsubscribe from the onTap event
* script.interactionComponent.onTap.remove(onTapEvent);
* ```
*/
declare class InteractionComponent extends Component {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Adds a MeshVisual as a target for interaction detection.
    */
    addMeshVisual(meshVisual: BaseMeshVisual): void
    
    /**
    * Adds a touch type that this component will ignore.
    */
    addTouchBlockingException(exception: string): void
    
    /**
    * Returns the minimum bounding box size used for detecting touches. Value range is from [0-1], relative to screen width.
    */
    getMinimumTouchSize(): number
    
    /**
    * Removes a MeshVisual as a target for interaction detection.
    */
    removeMeshVisual(meshVisual: BaseMeshVisual): void
    
    /**
    * Sets the camera that will be used for interaction detection.
    */
    setCamera(camera: Camera): void
    
    /**
    * Sets the minimum bounding box size used for detecting touches. Value range is from [0-1], relative to screen width.
    */
    setMinimumTouchSize(value: number): void
    
    /**
    * When enabled, interaction events will be invoked only on the"closest" object, where order is defined by distance from camera and camera render order.
    */
    isFilteredByDepth: boolean
    
    /**
    * Returns whether the user is currently gazing at the object on wearable devices, and when the user is touching on mobile devices.
    
    * @readonly
    */
    isFocused: boolean
    
    /**
    * Returns whether the user is currently selecting and gazing at an object on wearable devices, or touching it on mobile devices.
    
    * @readonly
    */
    isSelected: boolean
    
    /**
    * On mobile devices, this is triggered when the user releases their touch. On wearable devices, this is triggered when the user looks away from the current object.
    
    * @readonly
    */
    onFocusEnd: event1<FocusEndEventArgs, void>
    
    /**
    * On mobile devices, this is triggered when the user starts touching. On wearable devices, this is triggered when the user gazes on the current object.
    
    * @readonly
    */
    onFocusStart: event1<FocusStartEventArgs, void>
    
    /**
    * Bind a function to when hovering is occurring.
    
    * @readonly
    */
    onHover: event1<HoverEventArgs, void>
    
    /**
    * Bind a function to when hovering has ended.
    
    * @readonly
    */
    onHoverEnd: event1<HoverEndEventArgs, void>
    
    /**
    * Bind a function to when hovering has started.
    
    * @readonly
    */
    onHoverStart: event1<HoverStartEventArgs, void>
    
    /**
    * On mobile devices, this is triggered when the user releases their touch. On wearable devices, this is triggered when the user stops touching the current object.
    
    * @readonly
    */
    onSelectEnd: event1<SelectEndEventArgs, void>
    
    /**
    * On mobile devices, this is triggered when the user starts touching the current object. On wearable devices, this is triggered when the user touches the touchpad while focusing on an object.
    
    * @readonly
    */
    onSelectStart: event1<SelectStartEventArgs, void>
    
    /**
    * Triggered when the user taps on the screen.
    
    * @readonly
    */
    onTap: event1<TapEventArgs, void>
    
    /**
    * Triggered when a touch event ends.
    
    * @readonly
    */
    onTouchEnd: event1<TouchEndEventArgs, void>
    
    /**
    * Triggered when a touch position on the screen is moved.
    
    * @readonly
    */
    onTouchMove: event1<TouchMoveEventArgs, void>
    
    /**
    * Triggered when a touch event starts--either on the screen or on a touchpad. On wearables with touchpad, the coordinate returned are based on the touchpad and not the screen.
    
    * @readonly
    */
    onTouchStart: event1<TouchStartEventArgs, void>
    
    /**
    * Gets called when the user triggers the primary input on their device. For example touch on touch screens.
    
    * @readonly
    */
    onTriggerPrimary: event1<TriggerPrimaryEventArgs, void>
    
}

/**
* Provides access to the open internet. Available only on Spectacles and Camera Kit.
*/
declare class InternetModule extends Asset {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The createWebSocket() method initiates a WebSocket connection with the given `wss` URL and returns a {@link WebSocket} object that can be used to send and receive messages from the server.
    
    * Syntax
    * ```
    * createWebSocket(url)
    * ```
    
    * `url` Defines the `wss` URL to which to establish the WebSocket connection.
    
    * Example
    
    * ```
    * //@input Asset.InternetModule internetModule
    * var internetModule = script.internetModule;
    
    * // Create WebSocket connection.
    * let socket = script.internetModule.createWebSocket("wss://<some-url>");
    * socket.binaryType = "blob";
    
    * // Listen for the open event
    * socket.onopen = (event) => {
    *     // Socket has opened, send a message back to the server
    *     socket.send("Message 1");
    
    *     // Try sending a binary message
    *     // (the bytes below spell 'Message 2')
    *     const message = [77, 101, 115, 115, 97, 103, 101, 32, 50];
    *     const bytes = new Uint8Array(message);
    *     socket.send(bytes);
    * };
    
    * // Listen for messages
    * socket.onmessage = async (event) => {
    *     if (event.data instanceof Blob) {
    *         // Binary frame, can be retrieved as either Uint8Array or string
    *         let bytes = await event.data.bytes();
    *         let text = await event.data.text();
    
    *         print("Received binary message, printing as text: " + text);
    *     } else {
    *         // Text frame
    *         let text = event.data;
    *         print("Received text message: " + text);
    *     }
    * });
    
    * socket.onclose = (event) => {
    *     if (event.wasClean) {
    *         print("Socket closed cleanly");
    *     } else {
    *         print("Socket closed with error, code: " + event.code);
    *     }
    * };
    
    * socket.onerror = (event) => {
    *     print("Socket error");
    * };
    * ```
    
    * @wearableOnly
    
    * @CameraKit
    */
    createWebSocket(url: string, protocols?: any): WebSocket
    
    /**
    * This function will create a new instance of a WebView with the specified options. Once it has been created, onSuccess will be invoked, which returns the {@link Texture} of the WebView for rendering. This {@link Texture} contains a reference to the {@link WebPageTextureProvider} through its `Control` property. {@link WebPageTextureProvider} can be used for sending events and actions to the WebView. In the event of an error, the `onError` callback is invoked with the error message.
    
    * Arguments:
    * - **options:** The options for a specific WebView.
    * - **onSuccess:** Invoked on successful WebView creation. Provides the Asset.Texture for rendering and control.
    * - **onError:** Invoked on creation failure. Provides an error message.
    
    * _Note: Only 1 callback will be invoked, and only once._
    
    * _Note: After creation, the webview will later invoke onReady on the WebPageTextureProvider.control object to indicate it is ready for handling events and actions._
    
    * Creating a Webview Texture in JavaScript:
    
    * ```js
    * // Create the options
    * var resolution = new vec2(512, 512);
    * var options = InternetModule.createWebViewOptions(resolution);
    
    * // Create the WebView
    * script.internetModule.createWebView(
    * 	options,
    * 	(texture) => {
    * 		script.image.mainPass.baseTex = texture;
    * 		webViewControl = texture.control;
    * 		webViewControl.onReady.add(() => {
    * 			print("onReady");
    * 			webViewControl.loadUrl("https://snap.com");
    * 		});
    * 	},
    * 	(msg) => {
    * 		print("Error:" + msg);
    * 	}
    * );
    * ```
    
    * Creating a Webview Texture in TypeScript:
    
    * ```ts
    * const resolution = new vec2(512,512)
    * const options = InternetModule.createWebViewOptions(resolution)
    
    * this.internetModule.createWebView(
    * 	options,
    * 	(texture: Asset.Texutre) => {
    * 		this.image.mainPass.baseTex = texture
    * 		this.webViewControl = texture.control
    * 		this.webViewControl.onReady.add(() => {
    * 			print("onReady")
    * 			this.webViewControl.loadUrl("https://snap.com")
    * 		})
    * 	},
    * 	(msg: string) => {
    * 		print(`Error: ${msg}`)
    * 	}
    * )
    * ```
    
    * @experimental
    
    * @wearableOnly
    */
    createWebView(options: WebViewOptions, onSuccess: (texture: Texture) => void, onFailure: (errorMessage: string) => void): void
    
    /**
    * The fetch() method starts the process of fetching a resource from the internet, returning a promise that is fulfilled once the response is available.
    
    * The promise resolves to a {@link Response} object representing the response to your request.
    
    * A fetch() promise only rejects in cases of malformed URLs or network errors. A fetch() promise does not reject if the server responds with HTTP status codes that indicate errors. These errors can be checked manually via the Response status properties.
    
    * Syntax
    * ```
    * fetch(resource)
    * fetch(resource, options)
    * ```
    
    * `resource` Defines the URL you wish to fetch, or a {@link Request} object.
    
    * `options` Object containing any custom settings you want to apply to the request. Available options are `body`, `method`, `headers`, `redirect`, and `keepalive`. For more information on these properties see the {@link Request} class. If a {@link Request} was given for `resource`, then these options will entry-wise override the options specified in the {@link Request} object.
    
    * Example
    
    * ```
    * //@input Asset.InternetModule internetModule
    * var internetModule = script.internetModule;
    
    * // For this example assume this URL simply responds with the same body
    * // that it receives.
    * let request = new Request("https://<Your URL>.com", {
    *     method: "POST",
    *     body: JSON.stringify({ user: { name: "user", career: "developer" }}),
    *     headers: {
    *         "Content-Type": "application/json",
    *     },
    * });
    
    * let response = await internetModule.fetch(request, {
    *     body: JSON.stringify({ user: { name: "user", career: "salesman" }})
    * });
    * if (response.status != 200) {
    *     print("Failure: response not successful");
    *     return;
    * }
    
    * let contentTypeHeader = response.headers.get("content-type");
    * if (!contentTypeHeader.includes("application/json")) {
    *    print("Failure: wrong content type in response");
    *    return;
    * }
    
    * let responseJson = await response.json();
    * let username = responseJson.json["user"]["name"];
    * let career = responseJson.json["user"]["career"];
    
    * print(career); // will print "salesman"
    
    * ```
    
    * @wearableOnly
    */
    fetch(request: (Request|string), options?: any): Promise<Response>
    
    /**
    * Perform an http request described by {@link RemoteServiceHttpRequest}.
    
    * The following example demonstrates how to load a Texture using the InternetModule and {@link RemoteMediaModule}.
    * ```js
    * //@input Asset.InternetModule internetModule
    * //@input Asset.RemoteMediaModule remoteMediaModule
    * //@input Component.Image image
    
    * let request = RemoteServiceHttpRequest.create();
    * request.url = "https://docs.snap.com/img/spectacles/spectacles-project-info-settings.png";
    
    * script.internetModule
    *     .performHttpRequest(request, function(response){
    *         const dynamicResource = response.asResource()
    
    *         script.remoteMediaModule
    *             .loadResourceAsImageTexture(dynamicResource, function(texture) {
    
    *             script.image.mainPass.baseTex = texture
    *         },function(error){
    *             print(error)
    *         })
    *     })
    * ```
    
    * @wearableOnly
    
    * @CameraKit
    */
    performHttpRequest(requestOptions: RemoteServiceHttpRequest, onHttpResponse: (response: RemoteServiceHttpResponse) => void): void
    
    /**
    * This function creates a WebViewOptions instance that allows you to configure your webview.
    
    **Resolution:** `vec2` type representing the desired webpage resolution.
    
    * _Note: This is capped at 2048x2048._
    * _Note: Once a webview has been created this can not be changed._
    
    * @experimental
    
    * @wearableOnly
    */
    static createWebViewOptions(resolution: vec2): WebViewOptions
    
}

/**
* Arguments used with the onInternetStatusChanged event.

* @example
* Example js
* ```js
* // @input Component.Text textObject
* global.deviceInfoSystem.onInternetStatusChanged.add(function(eventData) {
*     script.textObject.text = eventData.isInternetAvailable
*         ? "UPDATED: Internet is available"
*         : "UPDATED: No internet";
* });
* ```

* Example ts
* ```ts
* @component
* export class NewScript extends BaseScriptComponent {
*   @input textObject: Text;

*   onAwake() {
*     global.deviceInfoSystem.onInternetStatusChanged.add((args) => {
*       this.textObject.text = args.isInternetAvailable
*         ? "UPDATED: Internet is available"
*         : "UPDATED: No internet";
*     });
*   }
* }
* ```
*/
declare class InternetStatusChangedArgs extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Whether the device has access to the internet.
    
    * @readonly
    
    * @wearableOnly
    */
    isInternetAvailable: boolean
    
}

/**
* Represents an animation track using stepped integer value keyframes.

* @deprecated
*/
declare class IntStepAnimationTrackKeyFramed extends IntAnimationTrack {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Adds a key with value `value` at time `time`.
    
    * @deprecated
    */
    addKey(time: number, value: number): void
    
    /**
    * Removes all keys.
    
    * @deprecated
    */
    removeAllKeys(): void
    
    /**
    * Removes key at index `index`.
    
    * @deprecated
    */
    removeKeyAt(index: number): void
    
}

/**
* Represents an animation track using stepped integer value keyframes.

* @deprecated
*/
declare class IntStepNoLerpAnimationTrackKeyFramed extends IntAnimationTrack {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Adds a key with value `value` at time `time`.
    
    * @deprecated
    */
    addKey(time: number, value: number): void
    
    /**
    * Removes all keys.
    
    * @deprecated
    */
    removeAllKeys(): void
    
    /**
    * Removes key at index `index`.
    
    * @deprecated
    */
    removeKeyAt(index: number): void
    
}

/**
* The arguments of `getIsPhoneInHandBeginEvent` on `GestureModule`. Currently empty.

* @wearableOnly
*/
declare class IsPhoneInHandBeginArgs extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
}

/**
* The arguments of `getIsPhoneInHandEndEvent` on `GestureModule`. Currently empty.

* @wearableOnly
*/
declare class IsPhoneInHandEndArgs extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Triggered when the tracked face ends a kiss.

* @example
* ```js
* var event = script.createEvent("KissFinishedEvent");
* event.faceIndex = 0;
* event.bind(function (eventData)
* {
* 	print("Kiss finished on face 0");
* });
* ```
*/
declare class KissFinishedEvent extends FaceTrackingEvent {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Triggered when the tracked face starts a kiss.

* @example
* ```js
* var event = script.createEvent("KissStartedEvent");
* event.faceIndex = 0;
* event.bind(function (eventData)
* {
* 	print("Kiss just started on face 0");
* });
* ```
*/
declare class KissStartedEvent extends FaceTrackingEvent {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Displays text in the scene.
* This is now deprecated in favor of {@link Text}.
* See the [Text guide](https://developers.snap.com/lens-studio/features/text/2d-text) for more information.

* @deprecated

* @example
* ```js
* // Set a label's text

* //@input Component.Label label

* script.label.text = "My New Text";
* ```
*/
declare class Label extends SpriteVisual {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns the potential width and height of the Label if it were to display the input text.
    
    * @deprecated
    */
    measureText(text: string): vec2
    
    /**
    * The font used to display text.
    
    * @deprecated
    */
    fontAsset: Font
    
    /**
    * The color used for the outline effect.
    
    * @deprecated
    */
    outlineColor: vec4
    
    /**
    * The strength of the outline effect.
    
    * @deprecated
    */
    outlineSize: number
    
    /**
    * The color used for dropshadow.
    
    * @deprecated
    */
    shadowColor: vec4
    
    /**
    * The horizontal and vertical offset used for dropshadow.
    
    * @deprecated
    */
    shadowOffset: vec2
    
    /**
    * The font size being used.
    
    * @deprecated
    */
    size: number
    
    /**
    * The text displayed by the Label.
    
    * @deprecated
    */
    text: string
    
    /**
    * The color used for drawing text.
    
    * @deprecated
    */
    textColor: vec4
    
    /**
    * If enabled, adds a dropshadow to the text.
    
    * @deprecated
    */
    useDropshadow: boolean
    
    /**
    * If enabled, adds an outline around the text.
    
    * @deprecated
    */
    useOutline: boolean
    
}

/**
* This event is triggered at the end of every frame, after normal {@link UpdateEvent} trigger
* but before rendering occurs.

* @example
* ```js
* // Move this SceneObject every frame to match the position of a target SceneObject
* //@input SceneObject target

* var transform = script.getTransform();
* var targetTransform = script.target.getTransform();

* var event = script.createEvent("LateUpdateEvent");
* event.bind(function (eventData)
* {
*     var targetPos = targetTransform.getWorldPosition();
*     transform.setWorldPosition(targetPos);
* });
* ```
*/
declare class LateUpdateEvent extends SceneEvent {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns the time elapsed (in seconds) between the current frame and previous frame.
    */
    getDeltaTime(): number
    
}

/**
* Used to describe a set of layers that an object belongs to or interacts with.

* @see {@link SceneObject} `layer` property
* @see {@link Camera} `renderLayer` property
* @see {@link LightSource} `renderLayer` property.

* @see Used By: {@link Camera#renderLayer}, {@link LayerSet#contains}, {@link LayerSet#except}, {@link LayerSet#intersect}, {@link LayerSet#union}, {@link LightSource#renderLayer}, {@link Physics.Filter#onlyLayers}, {@link Physics.Filter#skipLayers}, {@link SceneObject#layer}
* @see Returned By: {@link LayerSet#except}, {@link LayerSet#intersect}, {@link LayerSet#union}, {@link LayerSet.fromNumber}, {@link LayerSet.makeUnique}

* @example
* Check if a Camera will render this SceneObject

* ```js
* // @input Component.Camera camera

* // Check if the camera will render this SceneObject
* var cameraLayer = script.camera.renderLayer;
* var objectLayer = script.getSceneObject().layer;

* var intersection = cameraLayer.intersect(objectLayer);

* if (intersection.isEmpty()) {
*     print("camera won't render this object");
* } else {
*     print("camera will render this object");
* }
* ```

* Add this SceneObject's layer to a Camera's render layer, so that the camera will render it.

* ```js
* // @input Component.Camera camera

* // Add this SceneObject's layer to a Camera's render layer
* var objectLayer = script.getSceneObject().layer;
* script.camera.renderLayer = script.camera.renderLayer.union(objectLayer);
* ```

* Create a new LayerSet based on a number

* ```js
* // Create a LayerSet based on 0
* var layerSet = LayerSet.fromNumber(0);
* ```
*/
declare class LayerSet {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns true if all layers in the `other` LayerSet are also present in this one.
    */
    contains(other: LayerSet): boolean
    
    /**
    * Returns a new LayerSet that contains layers present in this LayerSet but not present in `other`.
    */
    except(other: LayerSet): LayerSet
    
    /**
    * Returns a new LayerSet that only contains layers present in both this LayerSet and `other`.
    */
    intersect(other: LayerSet): LayerSet
    
    /**
    * Returns true if this LayerSet contains no layers.
    */
    isEmpty(): boolean
    
    /**
    * Returns a string representation of this LayerSet.
    */
    toString(): string
    
    /**
    * Returns a new LayerSet combining this LayerSet and `other`.
    */
    union(other: LayerSet): LayerSet
    
    numbers: number[]
    
    /**
    * Returns a new LayerSet based on the passed in number.
    */
    static fromNumber(layerId: number): LayerSet
    
    /**
    * Creates a new LayerSet that is guaranteed to be empty and not intersect existing layers. This bypasses the normal 32 layer limit in Studio, allowing for around 65,000 unique layers.
    */
    static makeUnique(): LayerSet
    
}

/**
* A leaderboard which can contain scores and information about participating users. Accessible through the `LeaderboardModule` asset.

* @see Used By: {@link LeaderboardModule#getLeaderboard}

* @example
* ```js
* //@input Asset.LeaderboardModule leaderboardModule

* const createOptions = getCreateOptions();
* script.leaderboardModule.getLeaderboard(createOptions, getLeaderboardSuccessCallback, getLeaderboardFailureCallback);

* function getCreateOptions() {
*     const options = Leaderboard.CreateOptions.create();
*     options.name = "leaderboardName";
*     options.ttlSeconds = 64000;
*     options.orderingType = Leaderboard.OrderingType.Descending;

*     return options;
* }

* function getRetrievalOptions() {
*     const retrievalOptions = Leaderboard.RetrievalOptions.create();
*     retrievalOptions.usersType = Leaderboard.UsersType.Friends;
*     retrievalOptions.usersLimit = 3;

*     return retrievalOptions;
* }

* function getLeaderboardSuccessCallback(leaderboard) {
*     print("[Leaderboard] getLeaderbaord success callback");
*     print("[Leaderboard] Leaderboard Name = " + leaderboard.name);
*     print("[Leaderboard] Ordering Type = " + leaderboard.orderingType);

*     const retrievalOptions = getRetrievalOptions();
*     leaderboard.getLeaderboardInfo(retrievalOptions, getInfoSuccessCallback, getInfoFailureCallback);
*     leaderboard.submitScore(100, submitScoreSuccessCallback, submitScoreFailureCallback);
* }

* function getLeaderboardFailureCallback(message) {
*     print("[Leaderboard] getLeaderboard failure callback with message " + message);
* }

* function getInfoSuccessCallback(othersInfo, currentUserInfo) {
*     print("[Leaderboard] getLeaderboardInfo success callback");

*     if (!isNull(currentUserInfo)) {
*         print(`[Leaderboard] Current User info: ${currentUserInfo.snapchatUser.displayName ? currentUserInfo.snapchatUser.displayName : ''} score: ${currentUserInfo.score}`);
*     }
*     othersInfo.forEach((userRecord, idx) => {
*         print(`[Leaderboard] ${idx + 1}. ${userRecord.snapchatUser.displayName ? userRecord.snapchatUser.displayName : ''} score: ${userRecord.score}`);
*     });
* }

* function getInfoFailureCallback(message) {
*     print("[Leaderboard] getLeaderboardInfo failure callback with message " + message);
* }

* function submitScoreSuccessCallback(currentUserInfo) {
*     print("[Leaderboard] submitScore success callback");
*     if (!isNull(currentUserInfo)) {
*         print(`[Leaderboard] Current User info: ${currentUserInfo.snapchatUser.displayName ? currentUserInfo.snapchatUser.displayName : ''} score: ${currentUserInfo.score}`);
*     }
* }

* function submitScoreFailureCallback(message) {
*     print("[Leaderboard] submitScore failure callback with message " + message);
* }
* ```
*/
declare class Leaderboard extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Get information about the leaderboard, such as who is on it.
    */
    getLeaderboardInfo(options: Leaderboard.RetrievalOptions, successCallback: (othersInfo: Leaderboard.UserRecord[], currentUserInfo?: Leaderboard.UserRecord) => void, failureCallback: (status: number) => void): void
    
    /**
    * Submit a score to the leaderboard.
    */
    submitScore(score: number, successCallback: (currentUserInfo: Leaderboard.UserRecord) => void, failureCallback: (status: number) => void): void
    
    /**
    * The name of the leaderboard.
    
    * @readonly
    */
    name: string
    
    /**
    * How the leaderboard should be ordered.
    
    * @readonly
    */
    orderingType: Leaderboard.OrderingType
    
    /**
    * How long entries on the leaderboard should last in seconds. Can be between 0-2 years. Setting `0` will result in default ttl which is 1 year. You must include this parameter when creating a leaderboard.
    
    * @readonly
    */
    ttlSeconds: number
    
}

declare namespace Leaderboard {
    /**
    * The options for the leaderboard to be made.
    
    * @see Used By: {@link LeaderboardModule#getLeaderboard}
    * @see Returned By: {@link Leaderboard.CreateOptions.create}
    */
    class CreateOptions extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * The name of the leaderboard.
        */
        name: string
        
        /**
        * How the leaderboard should be ordered.
        */
        orderingType: Leaderboard.OrderingType
        
        /**
        * How long the leaderboard should last.
        */
        ttlSeconds: number
        
        /**
        * Create the option.
        */
        static create(): Leaderboard.CreateOptions
        
    }

}

declare namespace Leaderboard {
    /**
    * Describes how the leaderboard should be ordered.
    
    * @see Used By: {@link Leaderboard#orderingType}, {@link Leaderboard.CreateOptions#orderingType}
    */
    enum OrderingType {
        /**
        * Results in a leaderboard where higher scores are better.
        */
        Descending,
        /**
        * Results in a leaderboard where lower scores are better.
        */
        Ascending
    }

}

declare namespace Leaderboard {
    /**
    * Describes the context for the leaderboard to be requested.
    
    * @see Used By: {@link Leaderboard#getLeaderboardInfo}
    * @see Returned By: {@link Leaderboard.RetrievalOptions.create}
    */
    class RetrievalOptions extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * The number of users to be retrieved. The number should be >= 0.
        */
        usersLimit: number
        
        /**
        * The type of users to be retrieved.
        */
        usersType: Leaderboard.UsersType
        
        /**
        * Creates the option.
        */
        static create(): Leaderboard.RetrievalOptions
        
    }

}

declare namespace Leaderboard {
    /**
    * Information for a user who submitted a score to the leaderboard.
    
    * @see Used By: {@link Leaderboard#getLeaderboardInfo}, {@link Leaderboard#submitScore}
    */
    class UserRecord extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * The numerical rank of the user in the global leaderboard.
        
        * @readonly
        */
        globalExactRank?: number
        
        /**
        * The percentile rank of the user in the global leaderboard.
        
        * @readonly
        */
        globalRankPercentile: number
        
        /**
        * The exact score of the user.
        
        * @readonly
        */
        score: number
        
        /**
        * The user which submitted the score. Display name and user name will not be available for global users.
        
        * @readonly
        */
        snapchatUser: SnapchatUser
        
    }

}

declare namespace Leaderboard {
    /**
    * The type of user to be retrieved.
    
    * @see Used By: {@link Leaderboard.RetrievalOptions#usersType}
    */
    enum UsersType {
        /**
        * A friend of the Lens viewer.
        */
        Friends,
        /**
        * Both friend and non-friend of the Lens viewer.
        */
        Global
    }

}

/**
* Enables usage of {@link Leaderboard} API within a Lens.

* @see [Leaderboard](https://developers.snap.com/lens-studio/features/user-context/leaderboard) guide.
*/
declare class LeaderboardModule extends Asset {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Gets a handle for a leaderboard.
    */
    getLeaderboard(options: Leaderboard.CreateOptions, successCallback: (leaderboard: Leaderboard) => void, failureCallback: (message: string) => void): void
    
}

/**
* Collider asset generated from a mesh to be used with the {@link HairVisual} as part of the hair simulation.

* @see Used By: {@link LevelsetShape#asset}
*/
declare class LevelsetColliderAsset extends BinAsset {
    
    /** @hidden */
    protected constructor()
    
}

/**
* A levelset collision shape. A levelset is an asset that is generated to approximate a mesh.

* @see Returned By: {@link Shape.createLevelsetShape}
*/
declare class LevelsetShape extends Shape {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Reference to the levelset asset.
    */
    asset: LevelsetColliderAsset
    
}

/**
* A Licensed Sounds audio track from Asset LIbrary.
*/
declare class LicensedAudioTrackAsset extends AudioTrackAsset {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns the {@link ExternalMusicInfo} object for the current external music track. This can be used to compare two audio tracks during the Lens runtime.
    */
    getExternalMusicInfo(): ExternalMusicInfo
    
}

/**
* Acts as a source of light in the scene.

* @see [Light and Shadows](https://developers.snap.com/lens-studio/features/graphics/light-and-shadow) guide.

* @example
* ```
* // Sets the intensity of a light source
* //@input Component.LightSource lightSource

* script.lightSource.intensity = 0.0;
* ```
*/
declare class LightSource extends Component {
    
    /** @hidden */
    protected constructor()
    
    /**
    * If enabled, the LightSource will be automatically positioned based on its orientation relative to any shadow casting meshes in the scene.
    */
    autoLightSourcePosition: boolean
    
    /**
    * If enabled, `shadowFrustumSize` will be automatically updated based on its orientation relative to any shadow casting meshes in the scene.
    */
    autoShadowFrustumSize: boolean
    
    /**
    * If enabled, the LightSource will be able to cast shadows.
    */
    castsShadows: boolean
    
    /**
    * The color of the light.
    */
    color: vec3
    
    /**
    * A color image applied to an imaginary skybox the LightSource will use for color information.
    */
    diffuseEnvmapTexture: Texture
    
    /**
    * A value used to increase the intensity of light information derived from the `diffuseEnvmapTexture` exponentially.
    */
    envmapExposure: number
    
    /**
    * Controls the amount of rotation applied to the `diffuseEnvmapTexture`.
    */
    envmapRotation: number
    
    /**
    * Controls the strength of blurring done to shadows cast by this LightSource.
    
    * @deprecated
    */
    gaussianBlurSigma: number
    
    /**
    * The strength of the light on a scale of 0.0  1.0.
    */
    intensity: number
    
    /**
    * The type of light emitted from this light source.
    */
    lightType: LightType
    
    /**
    * The set of layers this LightSource will affect.
    */
    renderLayer: LayerSet
    
    /**
    * Controls the blurring size used when casting shadows from this LightSource.
    */
    shadowBlurRadius: number
    
    /**
    * Controls the color used when casting shadows from this LightSource.
    */
    shadowColor: vec4
    
    /**
    * The lightness and darkness value of the shadow cast by objects from this light source.
    */
    shadowDensity: number
    
    /**
    * The maximum distance at which shadows will be calculated for this LightSource.
    */
    shadowFrustumFarClipPlane: number
    
    /**
    * The minimum distance at which shadows will be calculated for this LightSource.
    */
    shadowFrustumNearClipPlane: number
    
    /**
    * The simulated distance of the light source from objects to calculate the softness of the shadow.
    */
    shadowFrustumSize: number
    
    /**
    * A color image applied to an imaginary skybox the light source will use for specular and reflection information.
    */
    specularEnvmapTexture: Texture
    
    /**
    * Enable if you would like the LightSource to use information from the `diffuseEnvmapTexture` for light color information.
    */
    useEnvmap: boolean
    
}

/**
* The types of light emission from a {@link LightSource}.

* @see Used By: {@link LightSource#lightType}
*/
declare enum LightType {
    /**
    * A type of light that illuminates from a single point and spreads in a cone shape.
    */
    Point,
    /**
    * A type of light that illuminates uniformly from a given direction, similar to an area light of infinite size that is infinitely far away.
    */
    Directional,
    /**
    * A type of light that illuminates from a single point and spreads in all directions.
    */
    Spot,
    /**
    * A type of light that illuminates the scene equally from all directions, with a fixed intensity.
    */
    Ambient,
    /**
    * A type of light that illuminates based on a provided environment map texture.
    */
    Envmap,
    /**
    * A type of light that illuminates based on estimation of light in the `Device Camera Texture`.
    */
    Estimation
}

/**
* Applies a liquify effect to anything rendered behind it.

* @see [Face Liquify](https://developers.snap.com/lens-studio/features/ar-tracking/face/face-liquify) guide.

* @example
* ```
* intensity = script.getSceneObject().getFirstComponent("Component.LiquifyVisual").intensity;

* radius = script.getSceneObject().getFirstComponent("Component.LiquifyVisual").radius;

* print(intensity.toString());
* print(radius.toString());
* ```
*/
declare class LiquifyVisual extends BaseMeshVisual {
    
    /** @hidden */
    protected constructor()
    
    /**
    * How strong the liquify effect is.
    */
    intensity: number
    
    /**
    * The radius of the liquify effect circle.
    */
    radius: number
    
}

/**


* @see Returned By: {@link Provider#getLoadStatus}
*/
declare enum LoadStatus {
    Idle,
    Loading,
    Loaded
}

/**
* Asset that provides the necessary localized text for your Lens.

* @remarks
* The asset refers to a folder containing files for each language that youve translated your texts to.

* The Lens will automatically use the correct localized string provided by a Localizations Asset based on the Snapchatter's device language.

* @see [Localization](https://developers.snap.com/lens-studio/features/text/localization) guide.
*/
declare class LocalizationsAsset extends Asset {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Helps convert data types to localized string representations. Accessible through `global.localizationSystem`.

* Note that formatted or localized strings may appear differently to users depending on their region.
* The example results given here are representative of a user in the United States, but may appear differently for users in other regions.

* @example
* ```js
* // get today date and time
* var currentDate = new Date();
* print("Today is: " + global.localizationSystem.getDayOfWeek(currentDate));
* print("The current time is: " + global.localizationSystem.getTimeFormatted(currentDate));
* ```

* ```js
* // get tomorrow's date
* var tomorrow = new Date();
* tomorrow.setDate(currentDate.getDate()+1);
* print("Tomorrow's date is: " + global.localizationSystem.getDateFormatted(tomorrow));
* ```

* ```js
* // show image based on locale
* /*
* @typedef LanguageTexturePair
* @property {string} language
* @property {Asset.Texture} texture
* *\/
* //@input Asset.Texture defaultTex

* //@input Component.Image image
* //@input LanguageTexturePair[] pairs

* var lang = global.localizationSystem.getLanguage();
* var lMap = {}

* for (var i = 0; i < script.pairs.length; i++) {
*     lMap[script.pairs[i].language] = script.pairs[i].texture;
* }
* if (lMap[lang] != undefined) {
*     script.image.mainMaterial.mainPass.baseTex = lMap[lang];
* } else {
*     script.image.mainMaterial.mainPass.baseTex = script.defaultTex;
* }
* ```
*/
declare class LocalizationSystem extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns a localized string for the date and time of the passed in `Date` object.
    
    * Example: "Jan 1, 2019 at 12:34 AM"
    */
    getDateAndTimeFormatted(date: Date): string
    
    /**
    * Returns a localized string for the date of the passed in `Date` object.
    
    * Example: "Jan 1, 2019"
    */
    getDateFormatted(date: Date): string
    
    /**
    * Returns a short, localized string for the date of the passed in `Date` object.
    
    * Example: "1/1/19"
    */
    getDateShortFormatted(date: Date): string
    
    /**
    * Returns a localized string for the day of the week of the passed in `Date` object.
    
    * Example: "Tuesday"
    */
    getDayOfWeek(date: Date): string
    
    /**
    * Returns a localized, formatted string representation of the distance in meters passed in.
    
    * Example: "39.4 in" (from 1 passed in)
    */
    getFormattedDistanceFromMeters(meters: number): string
    
    /**
    * Returns a localized, formatted string representation of the number passed in.
    
    * Example: "1,234" (from 1234 passed in)
    */
    getFormattedNumber(number: number): string
    
    /**
    * Returns a localized, formatted string representing the number of seconds passed in.
    
    * Example: "2:06" (from 126 passed in)
    */
    getFormattedSeconds(seconds: number): string
    
    /**
    * Returns a localized, formatted string representation of the celsius temperature passed in.
    
    * Example: "32F" (from 0 passed in)
    */
    getFormattedTemperatureFromCelsius(temperature: number): string
    
    /**
    * Returns a localized, formatted string representation of the fahrenheit temperature passed in.
    
    * Example: "32F" (from 32 passed in)
    */
    getFormattedTemperatureFromFahrenheit(temperature: number): string
    
    /**
    * Returns the language code of the language being used on the device.
    
    * Example: "en" (for English)
    
    * @exposesUserData
    */
    getLanguage(): string
    
    /**
    * Returns a localized string for the month of the passed in `Date` object.
    
    * Example: "January"
    */
    getMonth(date: Date): string
    
    /**
    * Returns a localized string for the time of the passed in `Date` object.
    
    * Example: "12:34 AM"
    */
    getTimeFormatted(date: Date): string
    
    /**
    * The method takes a [localization key](https://developers.snap.com/lens-studio/features/text/localization) and returns the localized string according to device language. Useful for localizing strings before formatting them and assigning them to Text.
    
    * @exposesUserData
    */
    localize(key: string): string
    
    /**
    * Use this property to set the language. Intended for debugging.
    
    * @exposesUserData
    */
    language: string
    
}

/**
* Enables placing a {@link SceneObject} at a real world location provided by {@link LocationAsset} and specified relative position.
*/
declare class LocatedAtComponent extends Component {
    
    /** @hidden */
    protected constructor()
    
    /**
    * @readonly
    */
    distanceToLocation: number
    
    /**
    * The LocationAsset which contains the position this object should be anchored to.
    */
    location: LocationAsset
    
    /**
    * @readonly
    */
    onCanTrack: event0<void>
    
    /**
    * @readonly
    */
    onCannotTrack: event0<void>
    
    /**
    * @readonly
    */
    onError: event0<void>
    
    /**
    * @readonly
    */
    onFound: event0<void>
    
    /**
    * @readonly
    */
    onLost: event0<void>
    
    /**
    * @readonly
    */
    onReady: event0<void>
    
    /**
    * The geo anchored position within a LocationAsset that this object should be anchored to.
    */
    position: vec3
    
    /**
    * @readonly
    */
    proximityStatus: LocationProximityStatus
    
    /**
    * Creates an option object for the `LocatedAtComponent`.
    */
    static createMappingOptions(): MappingOptions
    
    /**
    * Creates a mapping session.
    */
    static createMappingSession(options: MappingOptions): MappingSession
    
}

/**
* Provides a frame of reference in which to localize objects to the real world. Use with {@link LocatedAtComponent}.

* @see Used By: {@link Anchor#location}, {@link DeviceLocationTrackingComponent#location}, {@link GeoLocation.getGeoPositionForLocation}, {@link LocatedAtComponent#location}, {@link LocationCloudStorageModule#retrieveLocation}, {@link LocationCloudStorageModule#storeLocation}, {@link LocationCloudStorageOptions#location}, {@link LocationRenderObjectProvider#location}, {@link LocationTextureProvider#location}, {@link MapModule#longLatToImageRatio}, {@link MappingOptions#location}, {@link MapTextureProvider#location}
* @see Returned By: {@link LocationAsset#adjacentTile}, {@link LocationAsset.fromSerialized}, {@link LocationAsset.getAROrigin}, {@link LocationAsset.getNearby}, {@link LocationAsset.getProxy}
*/
declare class LocationAsset extends Asset {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns the `LocationAsset` Tile that is  specified by the containing tile and the specified `xOffset`, `yOffset` and `zoomOffset`. When zooming in, index offset is relative to the center. When zooming out or not zooming, index offset is relative to the containing tile.
    */
    adjacentTile(xOffset: number, yOffset: number, zoomOffset: number): LocationAsset
    
    /**
    * Gets the Proxy LocationID if the LocationAsset is a proxyLocation; returns null otherwise. This is useful in instances where a callback may return a mix of different location assets, and the creator of the proxy asset has specific handling in mind for particular proxy locations.
    */
    getProxyId(): string | undefined
    
    /**
    * Deserialize the provided serialized LocationAsset.
    
    * @exposesUserData
    
    * @wearableOnly
    */
    toSerialized(): string
    
    /**
    * Serialize the LocationAsset to a string representation. 
    
    * @exposesUserData
    
    * @wearableOnly
    */
    static fromSerialized(serialized: string): LocationAsset
    
    /**
    * Gets the origin position of the `LocationAsset`.
    */
    static getAROrigin(): LocationAsset
    
    /**
    * Get an {@link Anchor} based on the given GPS coordinate.
    */
    static getGeoAnchoredPosition(longitude: number, latitude: number): Anchor
    
    /**
    * Gets nearby LocationAsset tiles, where each argument represents the x,y,z position relative to the current LocationAsset.
    */
    static getNearby(xOffset: number, yOffset: number, zoomOffset: number): LocationAsset
    
    /**
    * Get a Proxy LocationAsset with the proxyId embedded in the locationId. proxyId must not be null, must start with an alpha char, and consist only of alphanumerics or ".".
    */
    static getProxy(proxyId: string): LocationAsset
    
}

/**
* Provides access to location cloud storage depending upon the {@link LocationCloudStorageOptions}.

* @see Used By: {@link MappingOptions#locationCloudStorageModule}

* @example
* 
*/
declare class LocationCloudStorageModule extends Asset {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Request discovery of LocationCloudStores based on the user location.
    */
    getNearbyLocationStores(options: LocationCloudStorageOptions): void
    
    /**
    * Retrieves the specified `LocationAsset`.
    */
    retrieveLocation(persistedLocationId: string, onRetrievedLocation: (location: LocationAsset) => void, onError: (error: string) => void): void
    
    /**
    * Stores the specified `LocationAsset`.
    */
    storeLocation(location: LocationAsset, onStoredLocation: (persistedLocationId: string) => void, onError: (error: string) => void): void
    
    /**
    * The active session used in the CloudStorageModule.
    
    * @wearableOnly
    */
    session: MultiplayerSession
    
}

/**
* Used to configure LocationCloudStorage module with various options.

* @see Used By: {@link LocationCloudStorageModule#getNearbyLocationStores}
* @see Returned By: {@link LocationCloudStorageOptions.create}
*/
declare class LocationCloudStorageOptions extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Tag to represent a collection of objects/entities
    */
    collection: string
    
    /**
    * Provides a frame of reference in which to localize objects to the real world. Could be any location type.
    */
    location: LocationAsset
    
    /**
    * Event raised on discovery of nearby cloud stores.
    
    * @readonly
    */
    onDiscoveredNearby: event2<LocationAsset, LocationCloudStore, void>
    
    /**
    * Error event raised when failing to instantiate a location cloud store associated with that location asset.
    
    * @readonly
    */
    onError: event3<LocationAsset, string, string, void>
    
    /**
    * Create options to provide to LocationCloudStorageModule
    */
    static create(): LocationCloudStorageOptions
    
}

/**
* Instance of location cloud store which has a similar interface as cloud store.
*/
declare class LocationCloudStore extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Deletes a value on the location cloud store. Note: when updating a value, the scope and collection must match that of the original value.
    */
    deleteValue(key: string, readOptions: CloudStorageReadOptions, onDeleted: () => void, onError: (code: string, description: string) => void): void
    
    /**
    * Gets a value on the location cloud store. Note: when updating a value, the scope and collection must match that of the original value.
    */
    getValue(key: string, readOptions: CloudStorageReadOptions, onRetrieved: (key: string, value: (mat4|mat3|mat2|vec4|vec2|vec3|boolean|quat|number|string), collection: string) => void, onError: (code: string, description: string) => void): void
    
    /**
    * List values on the location cloud store. Note: when updating a value, the scope and collection must match that of the original value.
    */
    listValues(listOptions: CloudStorageListOptions, onRetrieved: (values: (mat4|mat3|mat2|vec4|vec2|vec3|boolean|quat|number|string)[][], cursor: string, collection: string) => void, onError: (code: string, description: string) => void): void
    
    /**
    * Create/update a value on the location cloud store. Note: when updating a value, the scope and collection must match that of the original value.
    */
    setValue(key: string, value: (mat4|mat3|mat2|vec4|vec2|vec3|boolean|quat|number|string), writeOptions: CloudStorageWriteOptions, onSaved: () => void, onError: (code: string, description: string) => void): void
    
}

/**
* Used by {@link DeviceLocationTrackingComponent} to indicate the user's distance from the landmarker location.
* See the [Landmarker guide](https://developers.snap.com/lens-studio/features/location-ar/guide) for more information.

* @see Used By: {@link DeviceLocationTrackingComponent#locationProximityStatus}, {@link LocatedAtComponent#proximityStatus}

* @example
* ```js
* //@input Component.DeviceLocationTrackingComponent locationTrackingComponent

* if (script.locationTrackingComponent.locationProximityStatus == LocationProximityStatus.WithinRange) {
*     print("user is in range to see the landmarker");
* } else {
*     print("user's location is unknown or user is too far away");
* }
* ```
*/
declare enum LocationProximityStatus {
    /**
    * User's distance cannot be determined or has not been determined yet.
    */
    Unknown,
    /**
    * User is close enough to the landmarker location to begin tracking.
    */
    WithinRange,
    /**
    * User is too far away from the landmarker location to track it.
    */
    OutOfRange
}

/**
* Provides access to a location's Mesh--such as when working with City Scale AR. Usually used in conjunction with `LocationTextureProvider`.

* @example
* ```js
* // @input Asset.Material material

* // This is an example of displaying one tile(center tile which is 0, 0) of nearby area. 9 tiles are supported, including:

* // center tile: LocationAsset.getNearby(0, 0, 0)

* // east tile of center: LocationAsset.getNearby(1, 0, 0)
* // west tile of center: LocationAsset.getNearby(-1, 0, 0)
* // south tile of center: LocationAsset.getNearby(0, 1, 0)
* // north tile of center: LocationAsset.getNearby(0, -1, 0)

* // southEast tile of center: LocationAsset.getNearby(1, 1, 0)
* // southWest tile of center: LocationAsset.getNearby(-1, 1, 0)
* // northEast tile of center: LocationAsset.getNearby(1, -1, 0)
* // northWest tile of center: LocationAsset.getNearby(-1, -1, 0)

* // Get the tile location asset of current position
* var centerTile_0_0 = LocationAsset.getNearby(0, 0, 0);

* // Create a Location Texture
* var locationTexture = LocationTextureProvider.create();
* locationTexture.control.location = centerTile_0_0;

* // Create a Location mesh
* var locationMesh = LocationRenderObjectProvider.create();
* locationMesh.control.location = centerTile_0_0;

* // Create a LocatedAtComponent to put location mesh in correct place
* var centerPin = global.scene.createSceneObject("center_tile_0_0");
* centerPin.setParent(script.getSceneObject());
* var locatedAt = centerPin.createComponent("Component.LocatedAtComponent");
* locatedAt.location = centerTile_0_0;

* // Create render mesh to display
* var mesh = global.scene.createSceneObject("mesh");
* mesh.setParent(centerPin);
* var renderMeshVisual = mesh.createComponent("Component.RenderMeshVisual");

* // Assign texture
* var locationMaterial = script.material.clone();
* renderMeshVisual.clearMaterials();
* renderMeshVisual.addMaterial(locationMaterial);
* renderMeshVisual.mainPass["baseTex"] = locationTexture;

* renderMeshVisual.mesh = locationMesh;
* ```
*/
declare class LocationRenderObjectProvider extends RenderObjectProvider {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The Render Object's `LocationAsset` nearby tile.
    */
    location: LocationAsset
    
    /**
    * @readonly
    */
    onLocationDataDownloadFailed: event0<void>
    
    /**
    * @readonly
    */
    onLocationDataDownloaded: event0<void>
    
    /**
    * Creates the location mesh.
    */
    static create(): RenderMesh
    
}

/**
* The LocationService allows the user to provide their location to Lens applications if they so desire. For privacy reasons, the user is asked for permission to report location information.

* > **Spectacles**: To use location services, users must be logged in and paired with their Snapchat account, and their location permission must be enabled. Users are also expected to be connected to the internet. To access the location API on Spectacles, refer to the [Spectacles Location](https://developers.snap.com/spectacles/about-spectacles-features/apis/location) documentation for examples and extra setup instructions regarding permissions.

* @see Returned By: {@link GeoLocation.createLocationService}

* @example
* ```js
* // Instantiate LocationService and obtain regular location and heading updates
* let locationService = undefined;
* let timestampLastLocation = undefined;

* const repeatUpdateUserLocation = script.createEvent('DelayedCallbackEvent');

* function createAndLogLocationAndHeading() {

*     //Create location handler
*     locationService = GeoLocation.createLocationService();

*     //Set the accuracy
*     locationService.accuracy = GeoLocationAccuracy.Navigation;

*     // Acquire heading orientation updates
*     var onOrientationUpdate = function (northAlignedOrientation) {
*         //Providing 3DoF north aligned rotation in quaternion form
*         let heading = GeoLocation.getNorthAlignedHeading(northAlignedOrientation);
*         print("Heading orientation: " + heading.toFixed(3))
*         // Convert to a 2DoF rotation for plane rendering purposes
*         var rotation = heading * Math.PI / 180;
*         script.screenTransform.rotation = (quat.fromEulerAngles(0, 0, rotation));
*     };
*     locationService.onNorthAlignedOrientationUpdate.add(onOrientationUpdate);

*     //Acquire next location immediately with zero delay
*     repeatUpdateUserLocation.reset(0.0);
* }

* script.createEvent('OnStartEvent').bind(() => {
*     createAndLogLocationAndHeading();
* });

* repeatUpdateUserLocation.bind(() => {
*     //Get users location
*     locationService.getCurrentPosition(
*         function (geoPosition) {
*         //Check if location coordinates have been updated based on timestamp
*         let geoPositionTimestampMs = geoPosition.timestamp.getTime();
*         if (timestampLastLocation != geoPositionTimestampMs) {
*             script.latitude = geoPosition.latitude;
*             script.longitude = geoPosition.longitude;
*             if (geoPosition.altitude != 0) {
*                 script.altitude = geoPosition.altitude;
*             }
*             script.horizontalAccuracy = geoPosition.horizontalAccuracy;
*             script.verticalAccuracy = geoPosition.vertical;
*             print('location source: ' + geoPosition.locationSource);
*             timestampLastLocation = geoPositionTimestampMs;
*         }
*     },
*     function (error) {
*         print(error);
*     }
*   );
*   //Acquire next location update in 1 second
*   repeatUpdateUserLocation.reset(1.0);
* });
* ```
* ### Examples for Visualising location coordinates in a 2D Map
* For mobile platforms use [Map Component](https://developers.snap.com/lens-studio/features/location-ar/map-component). For Spectacles, follow the [Location Guide section regarding location and heading visualization](https://developers.snap.com/spectacles/about-spectacles-features/apis/location#location-and-heading-visualisation). If you would like to create your own version for a custom 2D Map Visualization, use the below example as a reference which utilizes {@link MapModule | Map Module}.

* ```js
* // Example to create a custom 2D Map Visualization using Map Module
* //@input Asset.MapModule mapModule

* // Create location handler
* var locationService = GeoLocation.createLocationService();

* // Set the accuracy
* locationService.accuracy = GeoLocationAccuracy.Navigation;

* // Get users location.
* locationService.getCurrentPosition(
*     function(geoPosition) {
*         global.lat = geoPosition.latitude;
*         global.lng = geoPosition.longitude;
*     },
*     function(error) {
*         print(error);
*     }
* );

* // Setup MapTextureProvider
* var location = LocationAsset
*     .getGeoAnchoredPosition(global.lng, global.lat)
*     .location.adjacentTile(0,0,-3);
* var provider = script.mapModule.createMapTextureProvider();
* provider.control.location = location;
* script.mesh.mainPass.baseTex = provider;

* //Get the postion of the marker
* var xy = script.mapModule.longLatToImageRatio(global.lng, global.lat, global.mapProvider.control.location);
* ```
*/
declare class LocationService extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Retrieves the device's current location.
    * `onSuccess`: a callback function that takes a GeoPosition object as its sole input parameter.
    * `onError`: a callback function that takes a string error message as its sole input parameter.
    
    * @exposesUserData
    */
    getCurrentPosition(onSucess: (geoPosition: GeoPosition) => void, onError: (error: string) => void): void
    
    /**
    * The accuracy of the provided position.
    */
    accuracy: GeoLocationAccuracy
    
    /**
    * Event to notify when north aligned orientation data is available to use. Use in combination with {@link GeoLocation.getNorthAlignedHeading} to acquire heading direction.
    
    * @readonly
    */
    onNorthAlignedOrientationUpdate: event1<quat, void>
    
}

/**
* Provides access to a location's texture--such as when working with City Scale AR.

* @example
* ```js
* // @input Asset.Material material

* // This is an example of displaying one tile(center tile which is 0, 0) of nearby area. 9 tiles are supported, including:

* // center tile: LocationAsset.getNearby(0, 0, 0)

* // east tile of center: LocationAsset.getNearby(1, 0, 0)
* // west tile of center: LocationAsset.getNearby(-1, 0, 0)
* // south tile of center: LocationAsset.getNearby(0, 1, 0)
* // north tile of center: LocationAsset.getNearby(0, -1, 0)

* // southEast tile of center: LocationAsset.getNearby(1, 1, 0)
* // southWest tile of center: LocationAsset.getNearby(-1, 1, 0)
* // northEast tile of center: LocationAsset.getNearby(1, -1, 0)
* // northWest tile of center: LocationAsset.getNearby(-1, -1, 0)

* // Get the tile location asset of current position
* var centerTile_0_0 = LocationAsset.getNearby(0, 0, 0);

* // Create a Location Texture
* var locationTexture = LocationTextureProvider.create();
* locationTexture.control.location = centerTile_0_0;

* // Create a Location mesh
* var locationMesh = LocationRenderObjectProvider.create();
* locationMesh.control.location = centerTile_0_0;

* // Create a LocatedAtComponent to put location mesh in correct place
* var centerPin = global.scene.createSceneObject("center_tile_0_0");
* centerPin.setParent(script.getSceneObject());
* var locatedAt = centerPin.createComponent("Component.LocatedAtComponent");
* locatedAt.location = centerTile_0_0;

* // Create render mesh to display
* var mesh = global.scene.createSceneObject("mesh");
* mesh.setParent(centerPin);
* var renderMeshVisual = mesh.createComponent("Component.RenderMeshVisual");

* // Assign texture
* var locationMaterial = script.material.clone();
* renderMeshVisual.clearMaterials();
* renderMeshVisual.addMaterial(locationMaterial);
* renderMeshVisual.mainPass["baseTex"] = locationTexture;

* renderMeshVisual.mesh = locationMesh;
* ```
*/
declare class LocationTextureProvider extends TextureProvider {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The location texture's `LocationAsset` nearby tile.
    */
    location: LocationAsset
    
    /**
    * Create location texture.
    */
    static create(): Texture
    
}

/**
* Orients a {@link SceneObject} towards a target {@link SceneObject}.

* @example
* ```js
* // Switches through LookAt targets in a list on each tap
* //@input Component.LookAtComponent lookAt
* //@input SceneObject[] targets

* var targetIndex = 0;

* script.createEvent("TapEvent").bind(function(eventData)
* {
* 	if(script.targets.length > 0)
* 	{
* 		targetIndex = (targetIndex + 1) % script.targets.length;
* 		script.lookAt.target = script.targets[targetIndex];
* 	}
* });
* ```
* ```js
* // Set aimVectors to [-Z Aim, Y Up]
* // (Useful for aiming the camera since it faces towards -Z)
* //@input Component.LookAtComponent lookAt
* script.lookAt.aimVectors = LookAtComponent.AimVectors.NegativeZAimYUp;
* ```
*/
declare class LookAtComponent extends Component {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The "aim" and "up" vectors used when determining rotation.
    * LookAtComponent will try to point the `Aim` axis of the SceneObject towards the target,
    * while keeping the `Up` axis of the SceneObject pointing towards `worldUpVector`.
    */
    aimVectors: LookAtComponent.AimVectors
    
    /**
    * Controls the method of rotation being used.
    */
    lookAtMode: LookAtComponent.LookAtMode
    
    /**
    * Adds an additional rotation offset.
    */
    offsetRotation: quat
    
    /**
    * The SceneObject this LookAtComponent targets.
    */
    target: SceneObject
    
    /**
    * The vector to be considered the "up" vector when determining rotation.
    */
    worldUpVector: LookAtComponent.WorldUpVector
    
}

declare namespace LookAtComponent {
    /**
    * The "aim" and "up" vectors used with {@link LookAtComponent} when determining rotation.
    * LookAtComponent will try to point the `Aim` axis of the SceneObject towards the target,
    * while keeping the `Up` axis of the SceneObject pointing towards `worldUpVector`.
    
    * @see Used By: {@link LookAtComponent#aimVectors}
    
    * @example
    * ```js
    * // Set aimVectors to [-Z Aim, Y Up]
    * // (Useful for aiming the camera since it faces towards -Z)
    * //@input Component.LookAtComponent lookAt
    * script.lookAt.aimVectors = LookAtComponent.AimVectors.NegativeZAimYUp;
    * ```
    */
    enum AimVectors {
        /**
        * X Aim, Y Up
        */
        XAimYUp,
        /**
        * X Aim, Z Up
        */
        XAimZUp,
        /**
        * Y Aim, X Up
        */
        YAimXUp,
        /**
        * Y Aim, Z Up
        */
        YAimZUp,
        /**
        * Z Aim, X Up
        */
        ZAimXUp,
        /**
        * Z Aim, Y Up
        */
        ZAimYUp,
        /**
        * X Aim, -Y Up
        */
        XAimNegativeYUp,
        /**
        * X Aim, -Z Up
        */
        XAimNegativeZUp,
        /**
        * Y Aim, -X Up
        */
        YAimNegativeXUp,
        /**
        * Y Aim, -Z Up
        */
        YAimNegativeZUp,
        /**
        * Z Aim, -X Up
        */
        ZAimNegativeXUp,
        /**
        * Z Aim, -Y Up
        */
        ZAimNegativeYUp,
        /**
        * -X Aim, Y Up
        */
        NegativeXAimYUp,
        /**
        * -X Aim, Z Up
        */
        NegativeXAimZUp,
        /**
        * -Y Aim, X Up
        */
        NegativeYAimXUp,
        /**
        * -Y Aim, Z Up
        */
        NegativeYAimZUp,
        /**
        * -Z Aim, X Up
        */
        NegativeZAimXUp,
        /**
        * -Z Aim, Y Up
        */
        NegativeZAimYUp,
        /**
        * -X Aim, -Y Up
        */
        NegativeXAimNegativeYUp,
        /**
        * -X Aim, -Z Up
        */
        NegativeXAimNegativeZUp,
        /**
        * -Y Aim, -X Up
        */
        NegativeYAimNegativeXUp,
        /**
        * -Y Aim, -Z Up
        */
        NegativeYAimNegativeZUp,
        /**
        * -Z Aim, -X Up
        */
        NegativeZAimNegativeXUp,
        /**
        * -Z Aim, -Y Up
        */
        NegativeZAimNegativeYUp
    }

}

declare namespace LookAtComponent {
    /**
    * Modes used in `LookAtComponent.lookAtMode` to determine the rotation method being used.
    
    * @see Used By: {@link LookAtComponent#lookAtMode}
    
    * @example
    * ```js
    * // @input Component.LookAtComponent lookAt
    
    * script.lookAt.lookAtMode = LookAtComponent.LookAtMode.LookAtPoint;
    * script.lookAt.target = script.getSceneObject();
    * ```
    */
    enum LookAtMode {
        /**
        * Rotation is based on the target object's position
        */
        LookAtPoint,
        /**
        * Rotation is based on the target object's rotation
        */
        LookAtDirection
    }

}

declare namespace LookAtComponent {
    /**
    * Used with {@link LookAtComponent} to set the "up" vector when determining rotation.
    
    * @see Used By: {@link LookAtComponent#worldUpVector}
    
    * @example
    * ```js
    * // Set the LookAtComponent's WorldUpVector to SceneY
    * // @input Component.LookAtComponent lookAtComponent
    * script.lookAtComponent.worldUpVector = LookAtComponent.WorldUpVector.SceneY;
    * ```
    */
    enum WorldUpVector {
        /**
        * Scene's X vector
        */
        SceneX,
        /**
        * Scene's Y vector
        */
        SceneY,
        /**
        * Scene's Z vector
        */
        SceneZ,
        /**
        * Target object's X vector
        */
        TargetX,
        /**
        * Target object's Y vector
        */
        TargetY,
        /**
        * Target object's Z vector
        */
        TargetZ,
        /**
        * Current object's X vector
        */
        ObjectX,
        /**
        * Current object's Y vector
        */
        ObjectY,
        /**
        * Current object's Z vector
        */
        ObjectZ
    }

}

/**
* Represents the full set of Lyrics data for tracked music. Can be accessed through {@link LyricsTracker#fullLyrics | LyricsTracker.fullLyrics} when lyrics are available.

* @see Used By: {@link LyricsTracker#fullLyrics}

* @example
* ```ts
* const externalMusicModule = require("LensStudio:ExternalMusicModule") as ExternalMusicModule;

* @component
* export class LyricsExample extends BaseScriptComponent {
*     // Make sure an AudioTrack asset is included in the scene. It determines the track that plays.
*     // If you see errors, make sure that "Bundled" is unchecked (disabled) on the AudioTrack.
*     @input audioAsset: AudioTrackAsset;

*     onAwake() {
*         const lyricsTracker = externalMusicModule.getLyricsTracker();

*         lyricsTracker.onLyricsAvailable.add(() => {
*             const fullLyrics = lyricsTracker.fullLyrics;

*             // Iterate through all lines in the lyrics data
*             fullLyrics.lines.forEach(line => {

*                 // Iterate through all syncs in the line
*                 line.syncs.forEach(sync => {
*                     // Calculate start time for this sync
*                     const startTime = line.offset + sync.offset;

*                     // Calculate end time for this sync
*                     const endTime = line.offset + sync.offsetEnd;

*                     // Note that these times can be compared to current playback time using lyricsTracker.playbackPosition.

*                     // Get text for this sync
*                     // Note: For RichSync lyrics data, each sync text will be a single word or whitespace between words.
*                     // For LineSync lyrics data, each sync will be the full line of text.
*                     const text = sync.text;
*                 });
*             });
*         });
*     }
* }

* ```
*/
declare class Lyrics extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Duration (in seconds) of the tracked music.
    
    * @readonly
    */
    clipDuration: number
    
    /**
    * Lyrics represented as a list of {@link LyricsLine} objects in the order they appear in the tracked music.
    
    * @readonly
    */
    lines: LyricsLine[]
    
    /**
    * {@link LyricsType | Type} of the lyrics data available. Describes the granularity of timing data available for the lyrics.
    
    * @readonly
    */
    type: LyricsType
    
}

/**
* Represents a single line of Lyrics.

* @see Used By: {@link LyricsTracker#currentLine}, {@link LyricsTracker#nextLine}

* @example
* ```ts
* const externalMusicModule = require("LensStudio:ExternalMusicModule") as ExternalMusicModule;

* @component
* export class LyricsExample extends BaseScriptComponent {
*     // Make sure an AudioTrack asset is included in the scene. It determines the track that plays.
*     // If you see errors, make sure that "Bundled" is unchecked (disabled) on the AudioTrack.
*     @input audioAsset: AudioTrackAsset;

*     onAwake() {
*         const lyricsTracker = externalMusicModule.getLyricsTracker();

*         lyricsTracker.onLineStart.add(line => {
*             print("Line started with the text: " + line.fullLineText);

*             print("Number of syncs within the line: " + line.syncs.length);
*         });

*         lyricsTracker.onLineEnd.add(line => {
*             print("The line has ended.")
*         });
*     }
* }

* ```
*/
declare class LyricsLine extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The full line of lyrics represented as a single string.
    
    * @readonly
    */
    fullLineText: string
    
    /**
    * Offset (in seconds) that this line begins relative to the start of the tracked music. Note that this value can be negative, since music tracks can start halfway through a line being sung. This value is useful to compare with {@link LyricsTracker#playbackPosition | LyricsTracker.playbackPosition}.
    
    * @readonly
    */
    offset: number
    
    /**
    * Offset (in seconds) that this line ends relative to the start of the tracked music. This value is useful to compare with {@link LyricsTracker#playbackPosition | LyricsTracker.playbackPosition}.
    
    * @readonly
    */
    offsetEnd: number
    
    /**
    * Lyrics for this line represented as a list of {@link LyricsSync} objects in the order they appear.
    
    * Note that for {@link LyricsType#LineSync | LineSync} lyrics data, only one {@link LyricsSync} object will be present in the list, representing the entire line.
    
    * @readonly
    */
    syncs: LyricsSync[]
    
}

/**
* Represents a single sync/word of a {@link LyricsLine}.

* @example
* ```ts

* const externalMusicModule = require("LensStudio:ExternalMusicModule") as ExternalMusicModule;

* @component
* export class LyricsExample extends BaseScriptComponent {
*     // Make sure an AudioTrack asset is included in the scene. It determines the track that plays.
*     // If you see errors, make sure that "Bundled" is unchecked (disabled) on the AudioTrack.
*     @input audioAsset: AudioTrackAsset;

*     onAwake() {
*         const lyricsTracker = externalMusicModule.getLyricsTracker();

*         lyricsTracker.onWordStart.add(sync => {
*             print("Sync started: " + sync.text);
*         });

*         lyricsTracker.onWordEnd.add(sync => {
*             print("Sync ended: " + sync.text);
*         })
*     }
* }


* ```
*/
declare class LyricsSync extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Duration (in seconds) of this sync. Equivalent to {@link LyricsSync#offsetEnd | offsetEnd} - {@link LyricsSync#offset | offset}.
    
    * @readonly
    */
    duration: number
    
    /**
    * Offset (in seconds) that this sync begins, relative to the start of the {@link LyricsLine} this sync is part of. Add this offset to the {@link LyricsLine#offset | LyricsLine.offset} to convert it to playback time, which can be compared to {@link LyricsTracker#playbackPosition | LyricsTracker.playbackPosition}.
    
    * @readonly
    */
    offset: number
    
    /**
    * Offset (in seconds) that this sync ends, relative to the start of the {@link LyricsLine} this sync is part of. Add this offset to the {@link LyricsLine#offset | LyricsLine.offset} to convert it to playback time, which can be compared to {@link LyricsTracker#playbackPosition | LyricsTracker.playbackPosition}.
    
    * @readonly
    */
    offsetEnd: number
    
    /**
    * The text value of this sync. Depending on the {@link LyricsType} of the lyrics data, this may be a single word, whitespace, or an entire line of text.
    
    * @readonly
    */
    text: string
    
}

/**
* Provides information about lyrics data for a currently playing track. Can be accessed through {@link ExternalMusicModule#getLyricsTracker | getLyricsTracker()} on {@link ExternalMusicModule}.

* Note that lyrics data may not be available immediately when this class is accessed. Subscribe to {@link LyricsTracker#onLyricsAvailable | onLyricsAvailable} to be notified when lyrics data becomes available.

* Also note that some music tracks may not support lyrics. In this case, {@link LyricsTracker#onLyricsAvailable | onLyricsAvailable} will not be triggered.

* @see Returned By: {@link ExternalMusicModule#getLyricsTracker}

* @example
* ```ts
* // Import ExternalMusicModule through require() or @input
* const externalMusicModule = require("LensStudio:ExternalMusicModule") as ExternalMusicModule;

* @component
* export class LyricsExample extends BaseScriptComponent {
*     // Make sure an AudioTrack asset is included in the scene. It determines the track that plays.
*     // If you see errors, make sure that "Bundled" is unchecked (disabled) on the AudioTrack.
*     @input audioAsset: AudioTrackAsset;

*     onAwake() {
*         const lyricsTracker = externalMusicModule.getLyricsTracker();

*         lyricsTracker.onLyricsAvailable.add(() => {
*             print("Lyrics are available!");
*             print("Is rich sync data available: " + (lyricsTracker.fullLyrics.type == LyricsType.RichSync));
*         });

*         lyricsTracker.onLyricsCleared.add(() => {
*             print("Lyrics have been cleared.")
*         });

*         // Every time a line starts, print the full text of the line
*         lyricsTracker.onLineStart.add(line => {
*             print("Line starting: " + line.fullLineText);
*         });

*         // Every time a word starts, print the word text
*         lyricsTracker.onWordStart.add(word => {
*             print("Word starting: " + word.text);
*         });
*     }
* }
* ```
*/
declare class LyricsTracker extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The current {@link LyricsLine} for the tracked music. Returns `null` if no line is currently playing, or lyrics are not available.
    
    * @readonly
    */
    currentLine: LyricsLine
    
    /**
    * The full {@link Lyrics} for the tracked music. Useful for pre-processing the lyrics of a track ahead of time.
    
    * @readonly
    */
    fullLyrics: Lyrics
    
    /**
    * Returns true if the tracked audio is playing.
    
    * @readonly
    */
    isPlaying: boolean
    
    /**
    * The next {@link LyricsLine} for the tracked music. Returns `null` if no next line is present, or lyrics data is not available.
    
    * @readonly
    */
    nextLine: LyricsLine
    
    /**
    * Event that triggers when a {@link LyricsLine} has ended.
    
    * @readonly
    */
    onLineEnd: event1<LyricsLine, void>
    
    /**
    * Event that triggers when a {@link LyricsLine} has started. Note that if playback begins partway through a line, this event will immediately fire.
    
    * @readonly
    */
    onLineStart: event1<LyricsLine, void>
    
    /**
    * Event that triggers when lyrics data becomes available for the tracked music. Note that when lyrics are not available for the selected music track, this event will never trigger.
    
    * @readonly
    */
    onLyricsAvailable: event0<void>
    
    /**
    * Event that fires when lyrics data is cleared. This usually indicates that the tracked music has either been removed or changed. It's good practice to clear any lyrics visuals when this event fires.
    
    * @readonly
    */
    onLyricsCleared: event0<void>
    
    /**
    * Event that triggers when playback has been reset on the tracked music. It's good practice to clear any lyrics visuals when this event fires.
    
    * @readonly
    */
    onPlaybackReset: event0<void>
    
    /**
    * Event that triggers when playback has started on the tracked music.
    
    * @readonly
    */
    onPlaybackStarted: event1<ExternalMusicInfo, void>
    
    /**
    * Event that triggers when a {@link LyricsSync} has ended.
    
    * @readonly
    */
    onWordEnd: event1<LyricsSync, void>
    
    /**
    * Event that triggers when a {@link LyricsSync} has started. Note that if playback begins partway through a LyricsSync, this event will immediately fire.
    
    * @readonly
    */
    onWordStart: event1<LyricsSync, void>
    
    /**
    * Current playback position (in seconds) of the tracked music.
    
    * This can be used to compare playback position to the {@link LyricsLine#offset | offset} and {@link LyricsLine#offsetEnd | offsetEnd} properties on {@link LyricsLine} objects. It can also be used with offsets on {@link LyricsSync} objects, after adjusting their offsets by the `offset` of the {@link LyricsLine} they belong to.
    
    * @readonly
    */
    playbackPosition: number
    
}

/**
* Types of granularity for lyric data. Accessible through {@link Lyrics#type | Lyrics.type}.

* @see Used By: {@link Lyrics#type}

* @example
* ```ts
* const externalMusicModule = require("LensStudio:ExternalMusicModule") as ExternalMusicModule;

* @component
* export class LyricsExample extends BaseScriptComponent {
*     // Make sure an AudioTrack asset is included in the scene. It determines the track that plays.
*     // If you see errors, make sure that "Bundled" is unchecked (disabled) on the AudioTrack.
*     @input audioAsset: AudioTrackAsset;

*     onAwake() {
*         const lyricsTracker = externalMusicModule.getLyricsTracker();

*         lyricsTracker.onLyricsAvailable.add(() => {
*             print("Is rich sync data available: " + (lyricsTracker.fullLyrics.type == LyricsType.RichSync));
*         });
*     }
* }
* ```
*/
declare enum LyricsType {
    /**
    * Type is not set.
    */
    Unset,
    /**
    * Lyrics data is available for each word and space between words. Each {@link LyricsLine}'s {@link LyricsLine#syncs | syncs} list will contain a {@link LyricsSync} object for each word and space between words in the line. This means that timing data is available for each word in the tracked music.
    */
    RichSync,
    /**
    * Lyrics data is only available for entire lines at a time. Each {@link LyricsLine}'s {@link LyricsLine#syncs | syncs} list will contain a single {@link LyricsSync} object, with its {@link LyricsSync#text | text} property containing the entire line. This means that timing data is only available for entire lines of the song, and is not available for each individual word.
    */
    LineSync
}

/**
* Namespace for Machine Learning related classes and methods.
* For more information, see the [Machine Learning Overview](https://developers.snap.com/lens-studio/features/snap-ml/ml-overview).

* @example
* ```js
* function initMelSpectrogram(frameSize, hopSize, fftSize, numMel, minFreq, maxFreq, sampleRate) {
*     var melBuilder = MachineLearning.createMelSpectrogramBuilder();
*     melSpectrogram = melBuilder.setFrameSize(frameSize)
*         .setHopSize(hopSize)
*         .setFFTSize(fftSize)
*         .setNumMel(numMel)
*         .setMinFreq(minFreq)
*         .setMaxFreq(maxFreq)
*         .setSampleRate(sampleRate)
*         .build();
*     melSpectrogramBuffer = new Float32Array(64000);
* }
* ```
*/
declare class MachineLearning {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Creates a new DelayBuilder object.
    */
    static createDelayBuilder(): DelayBuilder
    
    /**
    * Creates a new DeltaBuilder object.
    */
    static createDeltaBuilder(): DeltaBuilder
    
    /**
    * Creates a new InputBuilder object.
    */
    static createInputBuilder(): InputBuilder
    
    /**
    * Creates a new MFCCBuilder object.
    */
    static createMFCCBuilder(): MFCCBuilder
    
    /**
    * Creates a new MelSpectrogramBuilder object.
    */
    static createMelSpectrogramBuilder(): MelSpectrogramBuilder
    
    static createNoiseReductionBuilder(): NoiseReductionBuilder
    
    /**
    * Creates a new OutputBuilder object.
    */
    static createOutputBuilder(): OutputBuilder
    
    static createPitchShifterBuilder(): PitchShifterBuilder
    
    /**
    * Creates SamplerBuilder.
    */
    static createSamplerBuilder(): SamplerBuilder
    
    /**
    * Creates a new SpectrogramBuilder object.
    */
    static createSpectrogramBuilder(): SpectrogramBuilder
    
    /**
    * Creates a new TransformerBuilder object.
    */
    static createTransformerBuilder(): TransformerBuilder
    
}

declare namespace MachineLearning {
    /**
    * Used with {@link BasePlaceHolder}.
    
    * @see Used By: {@link BasePlaceholder#dataLayout}, {@link BasePlaceholder#internalDataLayout}
    */
    enum DataLayout {
        /**
        * Layout where order is: batch, channels, height, width.
        */
        NCHW,
        /**
        * Layout where order is: batch, height, width, channels.
        */
        NHWC
    }

}

declare namespace MachineLearning {
    /**
    * Timing options for when MLComponent should start or stop running. Used with `MLComponent.runScheduled()`.
    * For more information, see the [MLComponent Scripting](https://developers.snap.com/lens-studio/features/snap-ml/ml-component/scripting-ml-component) guide.
    
    * @see Used By: {@link MLComponent#runScheduled}, {@link MLComponent#runScheduled}
    * @see Returned By: {@link MLComponent#getScheduledEnd}, {@link MLComponent#getScheduledStart}
    
    * @example
    * ```js
    * //@input Component.MLComponent mlComponent
    * script.mlComponent.runScheduled(true, MachineLearning.FrameTiming.Update, MachineLearning.FrameTiming.OnRender);
    * ```
    */
    enum FrameTiming {
        /**
        * Only valid as an end timing. There is no exact time specified when MLComponent should finish its run.
        */
        None,
        /**
        * Run during MLComponent update, before script update.
        */
        Update,
        /**
        * Run in MLComponent LateUpdate, after all scripts update, but before they get LateUpdate.
        */
        LateUpdate,
        /**
        * Run at a specific point during frame rendering.
        */
        OnRender
    }

}

declare namespace MachineLearning {
    /**
    * Inference modes used by `MLComponent.inferenceMode`. Each mode describes how the neural network will be run.
    
    * @see Used By: {@link MLComponent#inferenceMode}
    
    * @example
    * ```js
    * //@input Component.MLComponent mlComponent
    * script.mlComponent.inferenceMode = MachineLearning.InferenceMode.CPU;
    * ```
    */
    enum InferenceMode {
        /**
        * MLComponent will run the neural network on CPU. Available on all devices.
        */
        CPU,
        /**
        * MLComponent will attempt to run the neural network on GPU. If the device doesn't support it, CPU mode will be used instead.
        */
        GPU,
        /**
        * MLComponent will attempt to use a dedicated hardware accelerator to run the neural network. If the device doesn't support it, GPU mode will be used instead.
        */
        Accelerator,
        /**
        * MLComponent will automatically decide how to run the neural network based on what is supported. It will start with Accelerator, then fall back to GPU, then CPU.
        */
        Auto,
        /**
        * MLComponent will run the model on CPU using device native backend (like CoreML on Apple devices).
        */
        NativeCPU,
        /**
        * NativeCPUAndNPU: MLComponent will try to run the model using device Neural Processing Unit (e.g. Apple Neural Engine on Apple devices) if exists and model is supported, with fallback support for running on CPU.
        */
        NativeCPUAndNPU
    }

}

declare namespace MachineLearning {
    /**
    * Describes the current state of the MLComponent model.
    * For more information, see the [MLComponent Scripting](https://developers.snap.com/lens-studio/features/snap-ml/ml-component/scripting-ml-component) guide.
    
    * @see Used By: {@link MLComponent#state}
    
    * @example
    * ```js
    * //@input Component.MLComponent mlComponent
    * if(script.mlComponent.state == MachineLearning.ModelState.Idle) {
    *   script.mlComponent.runScheduled(true, MachineLearning.FrameTiming.Update, MachineLearning.FrameTiming.OnRender);
    * }
    * ```
    */
    enum ModelState {
        /**
        * Model is loading
        */
        Loading,
        /**
        * Model is running
        */
        Running,
        /**
        * Model is built and ready to run
        */
        Idle,
        /**
        * Model is not ready to run
        */
        NotReady,
        /**
        * An error was encountered while loading the model.
        */
        LoadingError
    }

}

declare namespace MachineLearning {
    /**
    * Types of output used by OutputPlaceholder.
    
    * @see Used By: {@link OutputBuilder#setOutputMode}, {@link OutputPlaceholder#mode}
    
    * @example
    * ```js
    * //@input Component.MLComponent mlComponent
    * //@input string outputName
    * //@input Component.Image outputImage
    
    
    * script.mlComponent.onLoadingFinished = onLoadingFinished;
    
    * function onLoadingFinished() {
    *     var output = script.mlComponent.getOutput(script.outputName);
    *     if (output.mode == MachineLearning.OutputMode.Data) {
    *         var outputData = output.data;
    *         for (var i = 0; i < outputData.length; i++) {
    *             print(outputData[i]);
    *         }
    *     } else {
    *         var texture = output.texture;
    *         script.outputImage.mainPass.baseTex = texture;
    *     }
    * }
    * ```
    
    * ```js
    * //@input vec2 outputSize = {1, 1}
    * //@input string outputName = "probs"
    
    * var outputChannels = 200;
    
    * var outputBuilder = MachineLearning.createOutputBuilder();
    * outputBuilder.setName(script.outputName);
    * outputBuilder.setShape(new vec3(script.outputSize.x, script.outputSize.y, outputChannels));
    * outputBuilder.setOutputMode(MachineLearning.OutputMode.Data);
    * var outputPlaceholder = outputBuilder.build();
    * ```
    */
    enum OutputMode {
        /**
        * The output will be in the form of a Float32Array.
        */
        Data,
        /**
        * The output will be in the form of a Texture.
        */
        Texture
    }

}

/**
* Handles input information from user touch input via the {@link InteractionComponent} to control the Scale, Rotation, and Translation of objects.

* @example
* ```
* // Disables the scale functionality on a manipulate component when a user taps
* //@input Component.ManipulateComponent manip

* function onTap(eventData)
* {
*     script.manip.enableManipulateType(ManipulateType.Scale, false);
* }
* var tapEvent = script.createEvent("TapEvent");
* tapEvent.bind(onTap);
* ```
*/
declare class ManipulateComponent extends Component {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Repositions the object to be within the bounds of `minDistance`, `maxDistance`.
    */
    clampWorldPosition(): void
    
    /**
    * Enables or disables the specified ManipulateType for this ManipulateComponent.
    */
    enableManipulateType(type: ManipulateType, enable: boolean): void
    
    /**
    * Checks for an intersection point between the manipulation plane and a line extending
    * from the camera through the specified screen space point. The screen point is passed in as (x, y) with both values
    * ranging from ([0-1], [0-1]), (0,0) being left-top and (1,1) being right-bottom. The result is returned as a
    * {@link ManipulateFrameIntersectResult} object.
    */
    intersectManipulateFrame(screenSpacePoint: vec2): ManipulateFrameIntersectResult
    
    /**
    * Returns whether the specified ManipulateType is enabled for this ManipulateComponent.
    */
    isManipulateTypeEnabled(type: ManipulateType): boolean
    
    /**
    * Changes swivel behavior based on the object's height relative to the camera.
    */
    isContextualSwivel: boolean
    
    /**
    * Returns whether the object that the Manipulate Component is on is currently being manipulated
    
    * @readonly
    */
    isManipulating: boolean
    
    /**
    * The maximum distance the object can travel from the user.
    */
    maxDistance: number
    
    /**
    * The maximum height of the object.
    */
    maxHeight: number
    
    /**
    * The maximum size the object can scale to.
    */
    maxScale: number
    
    /**
    * The minimum distance the object can be from the user.
    */
    minDistance: number
    
    /**
    * The minimum height of the object.
    */
    minHeight: number
    
    /**
    * The minimum size the object can shrink to.
    */
    minScale: number
    
    /**
    * Event fired when manipulation ends.
    
    * @readonly
    */
    onManipulateEnd: event1<ManipulateEndEventArgs, void>
    
    /**
    * Event fired when manipulation starts.
    
    * @readonly
    */
    onManipulateStart: event1<ManipulateStartEventArgs, void>
    
    /**
    * Multiplier for swivel rotation speed.
    * For example, a value of 0.5 will cut rotation speed in half,
    * and a value of 2.0 will double rotation speed.
    */
    rotationScale: number
    
}

/**
* This event is triggered when manipulation on the object ends.

* @example
* ```
* // Creates events for Manipulate Start and Manipulate End events
* var manipulating = true;

* function manipStart(eventData)
* {
*     manipulating = true;
* }
* var manipStartEvent = script.createEvent("ManipulateStartEvent");
* manipStartEvent.bind(manipStart);

* function manipEnd(eventData)
* {
*     manipulating = false;
* }
* var manipEndEvent = script.createEvent("ManipulateEndEvent");
* manipEndEvent.bind(manipEnd);
* ```
*/
declare class ManipulateEndEvent extends SceneObjectEvent {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Arguments used with the `ManipulateComponent.onManipulateEnd` event.

* @example
* ```js
* //@input Component.ManipulateComponent manipulateComponent

* // Subscribe to the onManipulateEndEvent event
* var onManipulateEndEventEvent = script.manipulateComponent.onManipulateEndEvent.add(function(manipulateEndEventArgs){
*     print("On End!");
* });

* // Unsubscribe from the onManipulateEndEvent event
* script.manipulateComponent.onManipulateEndEvent.remove(onManipulateEndEventEvent);
* ```
*/
declare class ManipulateEndEventArgs extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Result object returned from {@link ManipulateComponent.intersectManipulateFrame}.

* @see Returned By: {@link ManipulateComponent#intersectManipulateFrame}

* @example
* ```
* // Returns an intersectManipulateFrame based on user touch position
* //@input Component.ManipulateComponent manip

* function onTap(eventData)
* {
*     var touchPos = eventData.getTouchPosition();
*     var intersectManipFrame = script.manip.intersectManipulateFrame(touchPos);
*     if(intersectManipFrame && intersectManipFrame.isValid())
*     {
*         screenPoint = intersectManipFrame.getIntersectionPoint();

*         print(screenPoint.toString());
*     }
* }
* var tapEvent = script.createEvent("TapEvent");
* tapEvent.bind(onTap);
* ```
*/
declare class ManipulateFrameIntersectResult {
    
    /** @hidden */
    protected constructor()
    
    /**
    * If there was a valid intersection, returns the intersection point in world space.
    */
    getIntersectionPoint(): vec3
    
    /**
    * Returns whether there was a valid intersection.
    */
    isValid(): boolean
    
}

/**
* This event is triggered when manipulation on the object begins.

* @example
* ```
* // Creates events for Manipulate Start and Manipulate End events
* var manipulating = true;

* function manipStart(eventData)
* {
*     manipulating = true;
* }
* var manipStartEvent = script.createEvent("ManipulateStartEvent");
* manipStartEvent.bind(manipStart);

* function manipEnd(eventData)
* {
*     manipulating = false;
* }
* var manipEndEvent = script.createEvent("ManipulateEndEvent");
* manipEndEvent.bind(manipEnd);
* ```
*/
declare class ManipulateStartEvent extends SceneObjectEvent {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Arguments used with the `ManipulateComponent.onManipulateStart` event.

* @example
* ```js
* //@input Component.ManipulateComponent manipulateComponent

* // Subscribe to the onManipulateStart event
* var onManipulateStartEvent = script.manipulateComponent.onManipulateStart.add(function(manipulateStartEventArgs){
*     print("On Start!");
* });

* // Unsubscribe from the onManipulateStart event
* script.manipulateComponent.onManipulateStart.remove(onManipulateStartEvent);
* ```
*/
declare class ManipulateStartEventArgs extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Enum values specifying each type of manipulation. See {@link ManipulateComponent}.

* @see Used By: {@link ManipulateComponent#enableManipulateType}, {@link ManipulateComponent#isManipulateTypeEnabled}

* @example
* ```
* // Disables the scale functionality on a manipulate component when a user taps
* //@input Component.ManipulateComponent manip

* function onTap(eventData)
* {
*     script.manip.enableManipulateType(ManipulateType.Scale, false);
* }
* var tapEvent = script.createEvent("TapEvent");
* tapEvent.bind(onTap);
* ```
*/
declare enum ManipulateType {
    /**
    * The object can be scaled by pinching with two fingers.
    */
    Scale,
    /**
    * The object can be rotated by swiveling with two fingers.
    */
    Swivel,
    /**
    * The object can be moved by touching and dragging.
    */
    Drag
}

/**
* Allows access to map texture data around a specified location.

* @see [Map Component](https://developers.snap.com/lens-studio/features/location-ar/map-component) guide.
*/
declare class MapModule extends Asset {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Creates a new texture which holds a `MapTextureProvider`.
    */
    createMapTextureProvider(): Texture
    
    /**
    * Returns the position of the marker (relative to the associated map tile) based on the specified `longitude`, `latitude` and `location`. The top left corner of the provided `location` has a value of `[0,0]` and the right down corner has a value of `[1.1]`.
    */
    longLatToImageRatio(longitude: number, latitude: number, location: LocationAsset): vec2
    
}

/**
* Used with `MappingSession` to describe the session to be created.

* @see Used By: {@link LocatedAtComponent.createMappingSession}
* @see Returned By: {@link LocatedAtComponent.createMappingOptions}
*/
declare class MappingOptions extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Location hint for mapping. Leave unset or use `LocationAsset.getAROrigin()` for mapping in the current AR session frame. (planned future extension) Use a pre-existing location for incremental mapping.
    */
    location: LocationAsset
    
    /**
    * Must be present to provide a sharing model for the location. Map storage is private to the user, or shared via ConnectedLensSession on the the module (Spectacles only).
    */
    locationCloudStorageModule: LocationCloudStorageModule
    
    /**
    * Use case for mapping. Default "auto".
    */
    policy: string
    
}

/**
* Used with the `LocatedAtComponent` to map the current physical location.

* @see Returned By: {@link LocatedAtComponent.createMappingSession}

* @example
* ```js
* // Have a frame of reference to where
* // tracking starting in the session
* // LocationAsset returns a label for the co-ordinate frame
* var worldOriginLocationAsset = LocationAsset.getAROrigin();

* worldOriginLocatedAt = global.scene.createSceneObject("AR Origin").createComponent("LocatedAtComponent");

* // LocatedAtComponent
* worldOriginLocatedAt.location = worldOriginLocationAsset;

* // MAP NEW LOCATION + GET STORE FOR IT

* // Start mapping the space of the user
* var options = LocatedAtComponent.createMappingOptions();
* options.location = worldOriginLocationAsset;

* mappingSession = LocatedAtComponent.createMappingSession(options);

* mappingSession.onMapped.add((location) => {
*    // we _can_ use location if we like - it is now a persisted private custom
*    // location and has the same coordinate frame as worldOriginLocationAsset
*    currentLocationLocatedAt.location = location;

*    locationCloudStorageModule.storeLocation(
*        location,
*        (locationId) => {
*           // cloud storage is not associated with locations but can store strings
*           // locationCloudStorageModule has privacy model for location <-> string.
*           cloudStorage.putString("previous-location",
*                                  locationId);
*       },
*       (errorString) => {
*           print("failed to obtain persistent id for location: " + errorString);
*       });


*     var options = LocationCloudStorageOptions.create();
*     options.location = location;
*     options.onDiscoveredNearby.add((store) => {
*         //store code here
*     });

*     options.onError.add((error) => {
*         //error code here
*     });

*     // get store and use it
*     locationCloudStorageModule.getNearbyLocationStores(options);

*    ...

*    // Mapping will continue; can be cancelled or throttled here
* });


* ...

* // Progress towards map being of acceptable quality (ie canCheckpoint going true)
* print(mappingSession.quality);           // 0 -> 1
* print(mappingSession.canCheckpoint);     // == (quality >= 1)

* // force onMapped to be fired immediately
* // can be called 'early' once canCheckpoint goes true
* mappingSession.checkpoint();             // error to call with
*                                          // canCheckpoint == false

* print(mappingSession.capacityUsed);      // 0 -> 1
*                                          // when ==1 onMapped is fired
*                                          // quality guaranteed >=1 at
*                                          // that point

* ...

* mappingSession.setThrottling(MappingSession.Auto);
* //                or    (MappingSession.MappingThrottling.Foreground)
* //                or    (MappingSession.MappingThrottling.Background)
* //                or    (MappingSession.MappingThrottling.Off)

* ... or
* mappingSession.cancel();

* ...

* // concurrently with creating a new map

* function onPreviousLocationRetrieved(previousLocationAsset) {
*     // TRACKING PREVIOUS + GETTING STORES FOR PREVIOUS
*     previousLocationLocatedAt.location = previousLocationAsset;
*     previousLocationLocatedAt.onFound.add(() => {
*         // Cancel new location mapping
*         mappingSession.cancel();

*         // we could switch to incremental by creating a new mapping session here
*         // using previousLocationAsset

*         ...

*         var options = LocationCloudStorageOptions.create();
*         options.location = previousLocationAsset;
*         options.onDiscoveredNearby.add((store) => {
*             //store code here
*         });

*         options.onError.add((error) => {
*             //error code here
*         });

*         // get a store for the tracked location
*         locationCloudStorageModule.getNearbyLocationStores(options);
*     });
* };

* function onError(errorString) {
*     print("could not retrieve previous location: " + errorString);
* };

* // use some other means of storage to retrieve a previously stored location id
* var previousLocationId = cloudStorage.getString("previous-location");

* locationCloudStorageModule.retrieveLocation(
*                                 previousLocationId,
*                                 onPreviousLocationRetrieved,
*                                 onError);
* ```
*/
declare class MappingSession extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Stops the current mapping session. No more events will be queued after this is called, although previously queued `onMapped` events may complete.
    */
    cancel(): void
    
    /**
    * Require the `onMapped` event to fire. Fires as soon as minimum quality condition is met. Mapping can be left running and can be called multiple times (Spectacles only).
    */
    checkpoint(): void
    
    /**
    * Minimum conditions for trigerring onMapped via checkpoint() have been met - ie quality >= 1.
    
    * @readonly
    */
    canCheckpoint: boolean
    
    /**
    * Capacity used up for a map, goes from 0 to 1, where 1 will automatically trigger a checkpoint. Capacity will not reach 1.0 while quality <1.0. 1.0 is maximum capacity used, implemented per-device and per-mapping-policy.
    
    * @readonly
    */
    capacityUsed: number
    
    handheldMaximumSize: number
    
    handheldMinimumSize: number
    
    /**
    * @readonly
    */
    onCapacityUsedAtLimit: event0<void>
    
    /**
    * Event fired when checkpoint is requested and then once quality is acceptable.
    
    * @readonly
    */
    onMapped: event1<LocationAsset, void>
    
    /**
    * @readonly
    */
    onQualityAcceptable: event0<void>
    
    /**
    * Progress towards an acceptable map, goes from 0 -> 1.0, where 1.0 is defined as 'Acceptable' given a specific mapping policy.
    
    * @readonly
    */
    quality: number
    
    /**
    * Current throttling of mapping process, i.e. how much effort the device is putting into it. (planned future extension)
    */
    throttling: MappingSession.MappingThrottling
    
    wearableAcceptableRawCapacity: number
    
    wearableAllowEarlyCheckpoint: boolean
    
    wearableMaximumSize: number
    
    wearableMinimumSize: number
    
}

declare namespace MappingSession {
    /**
    
    
    * @see Used By: {@link MappingSession#throttling}
    */
    enum MappingThrottling {
        /**
        * No CPU usage; temporarily pause.
        */
        Off,
        /**
        * Minimum CPU usage while still mapping.
        */
        Background,
        /**
        * A default mapping method. Equivalent to 'Foreground' when mapping new and 'Background' when incremental mapping.
        */
        Auto,
        /**
        * Maximum CPU usage to mapping.
        */
        Foreground
    }

}

/**
* A texture of the map at the given location of a `LocationAsset`.
*/
declare class MapTextureProvider extends TextureProvider {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The location asset associated with the `MapTextureProvider`.
    */
    location: LocationAsset
    
    /**
    * A function that gets called when location data fails to download.
    
    * @readonly
    */
    onFailed: event0<void>
    
    /**
    * A function that gets called when location data is downloaded.
    
    * @readonly
    */
    onReady: event0<void>
    
}

/**
* Represents a texture to be tracked with {@link MarkerTrackingComponent}.

* @see [Marker Tracking](https://developers.snap.com/lens-studio/features/ar-tracking/world/marker-tracking) guide.

* @see Used By: {@link MarkerTrackingComponent#marker}
* @see Returned By: {@link Texture#createMarkerAsset}

* @example
* ```js
* // Set the MarkerAsset on a MarkerTracking component

* //@input Component.MarkerTrackingComponent markerTracker
* //@input Asset.MarkerAsset marker

* script.markerTracker.marker = script.marker;
* ```
*/
declare class MarkerAsset extends Asset {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns the aspect ratio (width / height) of the texture used by the marker asset.
    */
    getAspectRatio(): number
    
    /**
    * The height of the marker asset in real-life centimeters. Used to provide accurate transformation.
    * A {@link MarkerTrackingComponent} using this MarkerAsset will be scaled so that
    * one unit in the SceneObject's local space is equal to one centimeter in real life.
    */
    height: number
    
}

/**
* Base class for marker providers.
* For more information, see the [Marker Tracking](https://developers.snap.com/lens-studio/features/ar-tracking/world/marker-tracking) guide.
*/
declare class MarkerProvider extends Provider {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Used to track images in the camera.

* @remarks Moves the containing object's transform to match the detected image.

* @see [Marker Tracking](https://developers.snap.com/lens-studio/features/ar-tracking/world/marker-tracking) guide.

* @example
* ```js
* //@input Component.MarkerTrackingComponent markerTrackingComponent

* // Get whether or not the marker image is being tracked
* var isMarkerTracking = script.markerTrackingComponent.isTracking();

* // Print current status.
* if (isMarkerTracking) {
* 	print("Image is in Camera feed.");
* } else {
* 	print("Image is NOT in Camera feed.");
* }
* ```
*/
declare class MarkerTrackingComponent extends Component {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns whether the marker image is currently being tracked in camera.
    */
    isTracking(): boolean
    
    /**
    * If true, child objects of this Component's {@link SceneObject} will be disabled when the marker image is not being tracked.
    */
    autoEnableWhenTracking: boolean
    
    /**
    * The marker asset describing the tracking target.
    */
    marker: MarkerAsset
    
    /**
    * A function that gets called when marker tracking begins.
    */
    onMarkerFound: () => void
    
    /**
    * A function that gets called when marker tracking is lost.
    */
    onMarkerLost: () => void
    
}

/**
* Masks out visuals and {@link InteractionComponent} touch events area within a rectangle defined by {@link ScreenTransform} component.

* @remarks
* Any {@link RenderMeshVisual} or {@link InteractionComponent} components will be clipped within user defined 2D bounds. These 2D bounds are defined by a {@link ScreenTransform}.

* @see [Masking Component](https://developers.snap.com/lens-studio/lens-studio-workflow/scene-set-up/2d/masking-component)
*/
declare class MaskingComponent extends BaseMeshVisual {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The radius of the mask's corner.
    */
    cornerRadius: number
    
}

/**
* A 2x2 matrix.

* @see Used By: {@link GeneralDataStore#putMat2}, {@link mat2#add}, {@link mat2#div}, {@link mat2#equal}, {@link mat2#mult}, {@link mat2#sub}
* @see Returned By: {@link GeneralDataStore#getMat2}, {@link mat2#add}, {@link mat2#div}, {@link mat2#inverse}, {@link mat2#mult}, {@link mat2#multiplyScalar}, {@link mat2#sub}, {@link mat2#transpose}, {@link mat2.identity}, {@link mat2.zero}
*/
declare class mat2 {
    /**
    * Creates a new mat2, defaulting to identity values.
    */
    constructor()
    
    /**
    * Returns the result of adding the two matrices together.
    */
    add(mat: mat2): mat2
    
    /**
    * Returns the determinant of the matrix.
    */
    determinant(): number
    
    /**
    * Returns the result of dividing the two matrices.
    */
    div(mat: mat2): mat2
    
    /**
    * Returns whether the two matrices are equal.
    */
    equal(mat: mat2): boolean
    
    /**
    * Returns the inverse of the matrix.
    */
    inverse(): mat2
    
    /**
    * Returns the result of multiplying the two matrices.
    */
    mult(mat: mat2): mat2
    
    /**
    * Returns the result of scalar multiplying the matrix.
    */
    multiplyScalar(scalar: number): mat2
    
    /**
    * Returns the result of subtracting the two matrices.
    */
    sub(mat: mat2): mat2
    
    /**
    * Returns a string representation of the matrix.
    */
    toString(): string
    
    /**
    * Returns the transpose of this matrix.
    */
    transpose(): mat2
    
    /**
    * The first column of the matrix.
    */
    column0: vec2
    
    /**
    * The second column of the matrix.
    */
    column1: vec2
    
    /**
    * Returns a string representation of the matrix.
    */
    description: string
    
    /**
    * Returns the identity matrix.
    */
    static identity(): mat2
    
    /**
    * Returns a matrix with all zero values.
    */
    static zero(): mat2
    
}

/**
* A 3x3 matrix.

* @see Used By: {@link GeneralDataStore#putMat3}, {@link mat3#add}, {@link mat3#div}, {@link mat3#equal}, {@link mat3#mult}, {@link mat3#sub}, {@link quat.fromRotationMat}, {@link TensorMath.solvePnP}, {@link TensorMath.solvePnPExtended}, {@link TensorMath.solvePnPRansac}, {@link Transformer#inverseMatrix}, {@link Transformer#matrix}
* @see Returned By: {@link GeneralDataStore#getMat3}, {@link mat3#add}, {@link mat3#div}, {@link mat3#inverse}, {@link mat3#mult}, {@link mat3#multiplyScalar}, {@link mat3#sub}, {@link mat3#transpose}, {@link mat3.identity}, {@link mat3.makeFromRotation}, {@link mat3.zero}
*/
declare class mat3 {
    /**
    * Creates a new mat3, defaulting to identity values.
    */
    constructor()
    
    /**
    * Returns the result of adding the two matrices together.
    */
    add(mat: mat3): mat3
    
    /**
    * Returns the determinant of the matrix.
    */
    determinant(): number
    
    /**
    * Returns the result of dividing the two matrices.
    */
    div(mat: mat3): mat3
    
    /**
    * Returns whether the two matrices are equal.
    */
    equal(mat: mat3): boolean
    
    /**
    * Returns the inverse of the matrix.
    */
    inverse(): mat3
    
    /**
    * Returns the result of multiplying the two matrices.
    */
    mult(mat: mat3): mat3
    
    /**
    * Returns the result of scalar multiplying the matrix.
    */
    multiplyScalar(scalar: number): mat3
    
    /**
    * Returns the result of subtracting the two matrices.
    */
    sub(mat: mat3): mat3
    
    /**
    * Returns a string representation of the matrix.
    */
    toString(): string
    
    /**
    * Returns the transpose of this matrix.
    */
    transpose(): mat3
    
    /**
    * The first column of the matrix.
    */
    column0: vec3
    
    /**
    * The second column of the matrix.
    */
    column1: vec3
    
    /**
    * The third column of the matrix.
    */
    column2: vec3
    
    /**
    * Returns a string representation of the matrix.
    */
    description: string
    
    /**
    * Returns the identity matrix.
    */
    static identity(): mat3
    
    /**
    * Returns a matrix representing the specified rotation.
    */
    static makeFromRotation(arg1: quat): mat3
    
    /**
    * Returns a matrix with all zero values.
    */
    static zero(): mat3
    
}

/**
* A 4x4 matrix.

* @see Used By: {@link DepthFrameData#toWorldTrackingOriginFromDeviceRef}, {@link DeviceCamera#pose}, {@link GeneralDataStore#putMat4}, {@link mat4#add}, {@link mat4#div}, {@link mat4#equal}, {@link mat4#mult}, {@link mat4#sub}, {@link mat4.compMult}, {@link mat4.compMult}, {@link quat.fromRotationMat4}, {@link TensorMath.mulMatToPoints}, {@link TensorMath.projectPoints}, {@link TrackedMesh#transform}, {@link TrackedPlane#transform}, {@link Transform#setLocalTransform}, {@link Transform#setWorldTransform}
* @see Returned By: {@link BasicTransform#getInvertedMatrix}, {@link BasicTransform#getMatrix}, {@link GeneralDataStore#getMat4}, {@link mat4#add}, {@link mat4#div}, {@link mat4#inverse}, {@link mat4#mult}, {@link mat4#multiplyScalar}, {@link mat4#sub}, {@link mat4#transpose}, {@link mat4.compMult}, {@link mat4.compose}, {@link mat4.fromColumns}, {@link mat4.fromEulerAngles}, {@link mat4.fromEulerX}, {@link mat4.fromEulerY}, {@link mat4.fromEulerZ}, {@link mat4.fromRotation}, {@link mat4.fromRows}, {@link mat4.fromScale}, {@link mat4.fromTranslation}, {@link mat4.identity}, {@link mat4.lookAt}, {@link mat4.makeBasis}, {@link mat4.orthographic}, {@link mat4.outerProduct}, {@link mat4.perspective}, {@link mat4.zero}, {@link Transform#getInvertedWorldTransform}, {@link Transform#getWorldTransform}
*/
declare class mat4 {
    /**
    * Creates a new mat4, defaulting to identity values.
    */
    constructor()
    
    /**
    * Returns the result of adding the two matrices together.
    */
    add(mat: mat4): mat4
    
    /**
    * Returns the determinant of the matrix.
    */
    determinant(): number
    
    /**
    * Returns the result of dividing the two matrices.
    */
    div(mat: mat4): mat4
    
    /**
    * Returns whether the two matrices are equal.
    */
    equal(mat: mat4): boolean
    
    /**
    * Returns an euler angle representation of this matrix's rotation, in radians.
    */
    extractEulerAngles(): vec3
    
    /**
    * Returns an euler angle representation of this matrix's rotation, in radians.
    
    * @deprecated
    */
    extractEulerXYZ(): vec3
    
    /**
    * Returns the inverse of the matrix.
    */
    inverse(): mat4
    
    /**
    * Returns the result of multiplying the two matrices.
    */
    mult(mat: mat4): mat4
    
    /**
    * Returns the direction vector multiplied by this matrix.
    */
    multiplyDirection(direction: vec3): vec3
    
    /**
    * Returns the point `point` multiplied by this matrix.
    */
    multiplyPoint(point: vec3): vec3
    
    /**
    * Returns the result of scalar multiplying the matrix.
    */
    multiplyScalar(scalar: number): mat4
    
    /**
    * Returns the vector multiplied by this matrix.
    */
    multiplyVector(vector: vec4): vec4
    
    /**
    * Returns the result of subtracting the two matrices.
    */
    sub(mat: mat4): mat4
    
    /**
    * Returns a string representation of the matrix.
    */
    toString(): string
    
    /**
    * Returns the transpose of this matrix.
    */
    transpose(): mat4
    
    /**
    * The first column of the matrix.
    */
    column0: vec4
    
    /**
    * The second column of the matrix.
    */
    column1: vec4
    
    /**
    * The third column of the matrix.
    */
    column2: vec4
    
    /**
    * The fourth column of the matrix.
    */
    column3: vec4
    
    /**
    * Returns a string representation of the matrix.
    */
    description: string
    
    /**
    * Returns the two matrices multiplied component-wise.
    */
    static compMult(arg1: mat4, arg2: mat4): mat4
    
    /**
    * Returns a new matrix with translation `translation`, rotation `rotation`, and scale `scale`.
    */
    static compose(translation: vec3, rotation: quat, scale: vec3): mat4
    
    /**
    * Create a 4x4 matrix from four column vectors.
    */
    static fromColumns(column0: vec4, column1: vec4, column2: vec4, column3: vec4): mat4
    
    /**
    * Returns a new matrix with the specified euler angles (in radians).
    */
    static fromEulerAngles(euler: vec3): mat4
    
    /**
    * Returns a new matrix with euler angles `euler` (in radians).
    
    * @deprecated
    */
    static fromEulerAnglesYXZ(euler: vec3): mat4
    
    /**
    * Returns a new matrix with x euler angle `xAngle` (in radians).
    */
    static fromEulerX(xAngle: number): mat4
    
    /**
    * Returns a new matrix with y euler angle `yAngle` (in radians).
    */
    static fromEulerY(yAngle: number): mat4
    
    /**
    * Returns a new matrix with z euler angle `zAngle` (in radians).
    */
    static fromEulerZ(zAngle: number): mat4
    
    /**
    * Returns a new matrix with rotation `rotation`.
    */
    static fromRotation(rotation: quat): mat4
    
    /**
    * Create a 4x4 matrix from four row vectors.
    */
    static fromRows(row0: vec4, row1: vec4, row2: vec4, row3: vec4): mat4
    
    /**
    * Returns a new matrix with scale `scale`.
    */
    static fromScale(scale: vec3): mat4
    
    /**
    * Returns a new matrix with the translation `translation`.
    */
    static fromTranslation(translation: vec3): mat4
    
    /**
    * Returns a new matrix with the yaw, pitch, and roll radians found in `yawPitchRoll`.
    
    * @deprecated
    */
    static fromYawPitchRoll(yawPitchRoll: vec3): mat4
    
    /**
    * Returns the identity matrix.
    */
    static identity(): mat4
    
    /**
    * Returns a new matrix generated using the provided arguments.
    */
    static lookAt(eye: vec3, center: vec3, up: vec3): mat4
    
    /**
    * Returns a new matrix using the provided vectors.
    */
    static makeBasis(x: vec3, y: vec3, z: vec3): mat4
    
    /**
    * Returns a new matrix generated using the provided arguments.
    */
    static orthographic(left: number, right: number, bottom: number, top: number, zNear: number, zFar: number): mat4
    
    /**
    * Returns the outer product of the two matrices.
    */
    static outerProduct(arg1: vec4, arg2: vec4): mat4
    
    /**
    * Returns a new matrix generated using the provided arguments.
    */
    static perspective(fovY: number, aspect: number, zNear: number, zFar: number): mat4
    
    /**
    * Returns a matrix with all zero values.
    */
    static zero(): mat4
    
}

/**
* An asset that describes how visual objects should appear.

* @remarks
* Each Material is a collection of {@link Pass} which define the actual rendering passes.
* Materials are used by {@link MaterialMeshVisual} for drawing meshes in the scene.

* @see [Material Overview](https://developers.snap.com/lens-studio/features/graphics/materials/overview).

* @see Used By: {@link GltfAsset#tryInstantiate}, {@link GltfAsset#tryInstantiateAsync}, {@link GltfAsset#tryInstantiateWithSetting}, {@link HairVisual#hairMaterial}, {@link MaterialMeshVisual#addMaterial}, {@link MaterialMeshVisual#mainMaterial}
* @see Returned By: {@link Material#clone}, {@link MaterialMeshVisual#getMaterial}

* @example
* ```
* // Gets the first pass of a material on a sprite and plays the animation from its texture
* var sprite = script.getSceneObject().getFirstComponent("Component.SpriteVisual");
* var material = sprite.getMaterial(0);
* material.getPass(0).baseTex.control.play(-1,0.0);

* // Print number of passes
* print("Pass count = " + material.getPassCount().toString());
* ```
*/
declare class Material extends Asset {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns a copy of the Material.
    */
    clone(): Material
    
    /**
    * Returns the {@link Pass} of the Material at index `index`.
    */
    getPass(index: number): Pass
    
    /**
    * Returns the number of {@link Pass} for the Material.
    */
    getPassCount(): number
    
    /**
    * The first Pass of the Material.
    */
    mainPass: Pass
    
}

/**
* Inherits from {@link BaseMeshVisual} and provides access to the {@link Material} used in the rendering process.

* @example
* ```js
* // @input Component.MaterialMeshVisual visual

* // Set the material's main color to red
* script.visual.mainPass.baseColor = new vec4(1, 0, 0, 1);
* ```
*/
declare class MaterialMeshVisual extends BaseMeshVisual {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Adds a Material to use for rendering.
    */
    addMaterial(material: Material): void
    
    /**
    * Clears all Materials.
    */
    clearMaterials(): void
    
    /**
    * Returns the Material at index `index`.
    */
    getMaterial(index: number): Material
    
    /**
    * Returns the number of Materials used for rendering.
    */
    getMaterialsCount(): number
    
    /**
    * Returns the first Material.
    */
    mainMaterial: Material
    
    /**
    * Returns the `mainPass` of the `mainMaterial`.
    */
    mainPass: Pass
    
    /**
    * Overrides the mainPass on the material, without affecting other visuals referencing the same material.
    */
    mainPassOverrides: (any|PassPropertyOverrides)
    
    /**
    * Get the array of materials used by the MaterialMeshVisual.
    */
    materials: Material[]
    
    /**
    * Overrides the property on the material, without affecting other visuals referencing the same material.
    
    * @readonly
    */
    propertyOverrides: PropertyOverrides
    
}

/**
* Overrides a material's property. Used with {@link MaterialMeshVisual}.
*/
declare class MaterialPropertyOverrides extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Provides useful math utility functions.
*/
declare class MathUtils {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Clamp floating-point value `v` in the range `[lo, hi]` (inclusive).
    */
    static clamp(v: number, lo: number, hi: number): number
    
    /**
    * Linearly interpolate from floating-point `a` to `b`, based on fraction `t` (where t=0.0 returns a, and t=1.0 returns b). This is equivalent to the vec2 and vec3 lerp() functions, but operates on scalar floating-point values.
    */
    static lerp(a: number, b: number, time: number): number
    
    /**
    * Generate a random floating-point value in the range `[lo, hi)`. Note, the range is inclusive at the lower end, and exclusive at the upper.
    */
    static randomRange(lo: number, hi: number): number
    
    /**
    * Map floating-point `v` from range [inMin, inMax] to [outMin, outMax].
    */
    static remap(v: number, inMin: number, inMax: number, outMin: number, outMax: number): number
    
    /**
    * Constant mapping degrees to radians. This equals pi/180.
    */
    static DegToRad: number
    
    /**
    * Constant mapping radians to degrees. This equals 180/pi.
    */
    static RadToDeg: number
    
}

/**
* Settings for the physical substance, such as friction and bounciness, of a collider. If unset, uses the default matter from the world settings.

* @see Used By: {@link ColliderComponent#matter}, {@link Physics.WorldSettingsAsset#defaultMatter}

* @example
* ```
* // @input Physics.WorldComponent worldComponent
* // @input Physics.Matter matter

* script.matter.friction = 1;
* var worldSettings = script.worldComponent.worldSettings;
* worldSettings.defaultMatter = script.matter;
* ```
*/
declare class Matter extends Asset {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Bounciness, or restitution, for dynamic bodies. This is the proportion of energy preserved after a collision, in the range 0 to 1.
    * This defaults to 0.0. Dynamic bounciness should usually be less than 1.0, to prevent energy from accumulating excessively (there is a certain amount of energy gained due to collision response).
    * The effective bounciness for a collision is the product of both colliding objects. So if your bounciness setting appears to have no effect, it probably means the object it's colliding with has 0 bounciness.
    */
    dynamicBounciness: number
    
    /**
    * Coefficient of friction.
    * The effective friction between two objects is the product of both objects' friction. So for example, the default between two objects is 0.5*0.5=0.25.
    * Typical ranges are between 0 and 1, but larger values (up to 10) are allowed. This may be used to increase the effective friction when colliding against another object with low friction.
    * This is a very simplified approximation of realistic friction, since it does not correctly take into account the combined surface characteristics of both objects. It also doesn't distinguish between static and dynamic friction.
    */
    friction: number
    
    /**
    * Friction applied to rolling objects. This isn't physically realistic, but helps prevent objects from rolling indefinitely.
    */
    rollingFriction: number
    
    /**
    * Friction applied to spinning objects. This isn't physically realistic, but helps prevent objects from spinning indefinitely.
    */
    spinningFriction: number
    
    /**
    * Bounciness (AKA restitution), for static colliders. This is the proportion of energy preserved after a collision, in the range 0 to 1.
    * This defaults to 1.0. Typically we use high bounciness for static colliders because they are unaffected by collision, and thus maximally preserve energy.
    * The effective bounciness for a collision is the product of both colliding objects. So if your bounciness setting appears to have no effect, it probably means the object it's colliding with has 0 bounciness.
    */
    staticBounciness: number
    
}

/**
* Base class for Texture Providers based on selectable media.
*/
declare class MediaPickerTextureProvider extends TextureProvider {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Requests SDK to hide media picking UI.
    */
    hideMediaPicker(): void
    
    /**
    * Requests face mesh and sets a callback to fire when it's ready.
    */
    setFaceMeshReadyCallback(callback: () => void): void
    
    /**
    * Sets a callback to fire when a file is picked through media picking UI.
    */
    setFilePickedCallback(callback: () => void): void
    
    /**
    * Requests SDK to show media picking UI.
    */
    showMediaPicker(): void
    
    /**
    * If set to true, MediaPickerTextureProvider will request media picker UI automatically during loading.
    */
    autoShowMediaPicker: boolean
    
    /**
    * If set to true, MediaPickerTextureProvider will provide a proper texture transform for image with face(s), that will "zoom" UVs to the first found face on the image.
    */
    cropFace: boolean
    
    /**
    * Returns underlying TextureProvider for the last selected media file. If the last media file was not image with at least one face, null is returned.
    */
    faceImageControl: FaceTextureProvider
    
    /**
    * @deprecated
    
    * @readonly
    */
    faceRect: vec4
    
    /**
    * Returns underlying TextureProvider for the last selected media file. If the last media file was not image, null is returned.
    
    * @readonly
    */
    imageControl: TextureProvider
    
    /**
    * Returns true if an image is selected, or a video file has been loaded and is ready for decoding, false otherwise.
    
    * @readonly
    */
    isContentReady: boolean
    
    /**
    * MediaPickerTextureProvider will allow users to select only images with detected faces through media picker UI. See also "cropFace" option.
    */
    isFaceImagePickingEnabled: boolean
    
    /**
    * MediaPickerTextureProvider will allow users to select all images files through media picker UI.
    */
    isImagePickingEnabled: boolean
    
    /**
    * MediaPickerTextureProvider will allow users to select video files through media picker UI.
    */
    isVideoPickingEnabled: boolean
    
    /**
    * Returns underlying VideoTextureProvider for the last selected media file. If the last media file was not video, null is returned.
    
    * @readonly
    */
    videoControl: VideoTextureProvider
    
}

/**
* Computes a mel scale spectrogram - a spectrogram where the frequencies are converted to the mel scale.

* @see Returned By: {@link MelSpectrogramBuilder#build}

* @example
* ```js
* var melSpectrogram = melSpectrogramBuilder
*     .setFrameSize(frameSize)
*     .setHopSize(hopSize)
*     .setFFTSize(fftSize)
*     .setNumMel(numMel)
*     .setMinFreq(minFreq)
*     .setMaxFreq(maxFreq)
*     .setSampleRate(sampleRate)
*     .build();
* ```
*/
declare class MelSpectrogram extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Process in tensor with shape, write result to the outTensor and return the shape of outTensor.
    */
    process(inTensor: Float32Array, inShape: vec3, outTensor: Float32Array): vec3
    
}

/**
* A builder class for MelSpectrogram.

* @see Returned By: {@link MachineLearning.createMelSpectrogramBuilder}, {@link MelSpectrogramBuilder#setFFTSize}, {@link MelSpectrogramBuilder#setFrameSize}, {@link MelSpectrogramBuilder#setHopSize}, {@link MelSpectrogramBuilder#setMaxFreq}, {@link MelSpectrogramBuilder#setMinFreq}, {@link MelSpectrogramBuilder#setNumMel}, {@link MelSpectrogramBuilder#setSampleRate}

* @example
* ```js
* var melSpectrogramBuilder = MachineLearning.createMelSpectrogramBuilder();
* ```
*/
declare class MelSpectrogramBuilder extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Create new MelSpectrogram object.
    */
    build(): MelSpectrogram
    
    /**
    * Set length of the fft window.
    */
    setFFTSize(fftSize: number): MelSpectrogramBuilder
    
    /**
    * Length of the window, which is the length of frameSize and then padded with zeros to match fftSize.
    */
    setFrameSize(frameSize: number): MelSpectrogramBuilder
    
    /**
    * Set number of samples between successive fft segments.
    */
    setHopSize(hopSize: number): MelSpectrogramBuilder
    
    /**
    * Set max frequency.
    */
    setMaxFreq(maxFreq: number): MelSpectrogramBuilder
    
    /**
    * Set min frequency.
    */
    setMinFreq(minFreq: number): MelSpectrogramBuilder
    
    /**
    * Set number of mel bins.
    */
    setNumMel(numMel: number): MelSpectrogramBuilder
    
    /**
    * Set number of samples per second.
    */
    setSampleRate(sampleRate: number): MelSpectrogramBuilder
    
}

/**
* A class for generating meshes at runtime.

* @see Returned By: {@link MeshBuilder.createFromMesh}

* @example
* ### Quad Mesh Example
* ```js
* // Builds a quad mesh and applies it to meshVisual
* //@input Component.MeshVisual meshVisual

* var builder = new MeshBuilder([
*     { name: "position", components: 3 },
*     { name: "normal", components: 3 },
*     { name: "texture0", components: 2 },
* ]);

* builder.topology = MeshTopology.Triangles;
* builder.indexType = MeshIndexType.UInt16;

* var left = -.5;
* var right = .5;
* var top = .5;
* var bottom = -.5;

* builder.appendVerticesInterleaved([
*     // Position         Normal      UV       Index
*     left, top, 0,       0, 0, 1,    0, 1,    // 0
*     left, bottom, 0,    0, 0, 1,    0, 0,    // 1
*     right, bottom, 0,   0, 0, 1,    1, 0,    // 2
*     right, top, 0,      0, 0, 1,    1, 1,    // 3
* ]);

* builder.appendIndices([
*     0,1,2, // First Triangle
*     2,3,0, // Second Triangle
* ]);

* if(builder.isValid()){
*     script.meshVisual.mesh = builder.getMesh();
*     builder.updateMesh();
* }
* else{
*     print("Mesh data invalid!");
* }
* ```

* ### Cube Mesh Example
* ```js
* // Builds a cube mesh and applies it to meshVisual
* //@input Component.MeshVisual meshVisual
* //@input vec3 center = {0,0,0}
* //@input vec3 size = {4,4,4}

* var builder = new MeshBuilder([
*     { name: "position", components: 3 },
*     { name: "normal", components: 3 },
*     { name: "texture0", components: 2 },
* ]);

* builder.topology = MeshTopology.Triangles;
* builder.indexType = MeshIndexType.UInt16;

* var halfSize = script.size.uniformScale(.5);
* var right =  script.center.x + halfSize.x;
* var left =   script.center.x - halfSize.x;
* var top =    script.center.y + halfSize.y;
* var bottom = script.center.y - halfSize.y;
* var front =  script.center.z + halfSize.z;
* var back =   script.center.z - halfSize.z;

* // UVs of quad in order: Top left, Bottom left, Bottom right, Top right
* const QUAD_UVS = [[0,1], [0,0], [1,0], [1,1]];

* // Append data for 4 vertices in a quad shape
* function addQuadVerts(meshBuilder, normal, positions){
*     for(var i=0; i<positions.length; i++){
*         meshBuilder.appendVertices([positions[i], normal, QUAD_UVS[i]]);
*     }
* }

* // Append the indices for two triangles, forming a quad
* function addQuadIndices(meshBuilder, topLeft, bottomLeft, bottomRight, topRight){
*     meshBuilder.appendIndices([
*         topLeft, bottomLeft, bottomRight, // First Triangle
*         bottomRight, topRight, topLeft // Second Triangle
*     ]);
* }

* // Define the normal direction and vertex positions for each side of the cube
* var sides = [
*     { normal: [0,0,1], // Front
*       positions: [[left,top,front], [left,bottom,front], [right,bottom,front], [right,top,front]] },
*     { normal: [0,0,-1], // Back
*       positions: [[right,top,back], [right,bottom,back], [left,bottom,back], [left,top,back]] },
*     { normal: [1,0,0], // Right
*       positions: [[right,top,front], [right,bottom,front], [right,bottom,back], [right,top,back]] },
*     { normal: [-1,0,0], // Left
*       positions: [[left,top,back], [left,bottom,back], [left,bottom,front], [left,top,front]] },
*     { normal: [0,1,0], // Top
*       positions: [[left,top,back], [left,top,front], [right,top,front], [right,top,back]] },
*     { normal: [0,-1,0], // Bottom
*       positions: [[left,bottom,front], [left,bottom,back], [right,bottom,back], [right,bottom,front]] },
* ];

* // For each side, append the vertex data and indices
* for(var i=0; i<sides.length; i++){
*     var index = i * 4;
*     addQuadVerts(builder, sides[i].normal, sides[i].positions);
*     addQuadIndices(builder, index, index+1, index+2, index+3);
* }

* // Make sure the mesh is valid, then apply and update
* if(builder.isValid()){
*     script.meshVisual.mesh = builder.getMesh();
*     builder.updateMesh();
* }
* else{
*     print("Mesh data invalid!");
* }
* ```
*/
declare class MeshBuilder {
    /**
    * Creates a new MeshBuilder with the specified vertex layout.
    
    * Layout is given as a list of "attribute" objects with the following properties:
    
    * __name__ - Attribute name
    * __components__ - Size of the attribute (how many float values it uses)
    
    * ```js
    * var builder = new MeshBuilder([
    *     // vertex position (x,y,z)
    *     { name: "position", components: 3 },
    *     // normal vector (x,y,z)
    *     { name: "normal", components: 3 },
    *     // texture UV (u,v)
    *     { name: "texture0", components: 2 },
    * ]);
    * ```
    */
    constructor(layout: any[])
    
    /**
    * Appends indices to the index list.
    */
    appendIndices(indices: number[]): void
    
    /**
    * Takes a list of list of vertex values according to the layout.
    */
    appendVertices(verts: number[][]): void
    
    /**
    * Similar to `appendVertices`, but takes all values in one large array.
    */
    appendVerticesInterleaved(verts: number[]): void
    
    /**
    * Removes all indices starting at index `from` and ending before index `to`.
    */
    eraseIndices(from: number, to: number): void
    
    /**
    * Removes all vertex data starting at vertex index `from` and ending before vertex index `to`.
    */
    eraseVertices(from: number, to: number): void
    
    /**
    * Returns the number of indices in the index list.
    */
    getIndicesCount(): number
    
    /**
    * Returns a RenderMesh asset that can be applied to a MeshVisual's `mesh` property. This asset stays linked to the MeshBuilder that provided it, so making changes to the mesh data and calling `updateMesh()` will update the RenderMesh as well.
    */
    getMesh(): RenderMesh
    
    /**
    * Returns the number of vertices in the vertex list.
    */
    getVerticesCount(): number
    
    /**
    * Checks whether the current data entered will create a valid mesh.
    */
    isValid(): boolean
    
    /**
    * Add bones to the mesh.
    */
    setBones(bones: string[], inverseMatrices: mat4[]): void
    
    /**
    * Sets data for a single vertex at vertex index `index`.
    */
    setVertexInterleaved(index: number, verts: number[]): void
    
    /**
    * Rebuilds the MeshAsset controlled by this MeshBuilder using the current mesh data.
    */
    updateMesh(): void
    
    /**
    * The index data type used by this MeshBuilder. `MeshIndexType.UInt16` is the value normally used for this.
    */
    indexType: MeshIndexType
    
    /**
    * The topology type used for the mesh.
    */
    topology: MeshTopology
    
    /**
    * Create a MeshBuilder from a RenderMesh.
    */
    static createFromMesh(mesh: RenderMesh): MeshBuilder
    
}

/**
* Formats of mesh classification used by WorldRenderObjectProvider.

* @see Used By: {@link WorldRenderObjectProvider#meshClassificationFormat}
*/
declare enum MeshClassificationFormat {
    /**
    * Do not bake classifications to mesh
    */
    None,
    /**
    * Classifications are baked per vertex - vertices with multiple classes will use the value from the last face
    */
    PerVertexFast
}

/**
* Possible index data types used by {@link MeshBuilder}. `MeshIndexType.UInt16` is the value normally used.

* @see Used By: {@link MeshBuilder#indexType}, {@link RenderMesh#indexType}

* @example
* ```
* // Constructs a new mesh builder
* var builder = new MeshBuilder([
*     { name: "position", components: 3 },
*     { name: "normal", components: 3, normalized: true },
*     { name: "texture0", components: 2 },
*     { name: "color", components: 4 },
* ]);

* builder.topology = MeshTopology.Triangles;
* builder.indexType = MeshIndexType.UInt16;
* ```
*/
declare enum MeshIndexType {
    /**
    * No index data type
    */
    None,
    /**
    * Unsigned integer, this is the value normally used
    */
    UInt16
}

declare class MeshRenderObjectProvider extends RenderObjectProvider {
    
    /** @hidden */
    protected constructor()
    
}

/**


* @see Used By: {@link BaseMeshVisual#meshShadowMode}
*/
declare enum MeshShadowMode {
    None,
    Caster,
    Receiver
}

/**
* Allows meshes to be used as collision shapes, for ColliderComponent and BodyComponent.

* @see Returned By: {@link Shape.createMeshShape}
*/
declare class MeshShape extends Shape {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Bake mesh as a convex hull, generated from mesh colliders. This allows for irregular shapes to be used as dynamic bodies.
    
    **Limitations:**
    
    * They are non-deforming. A hull may be generated from a deforming render mesh, but it will not deform with it. Because of this, intrinsically deforming mesh types will not work as convex hulls. For instance world and face meshes. In these cases the convex hull will exist, but be empty and wont simulate.
    
    * It produces an approximation of the source mesh, reducing triangle count and eliminating concave areas.
    * Concave shapes may be composed of convex hulls in the scene graph. Splitting a concave mesh into convex hulls is known as "convex decomposition". It is an expensive process not supported by Studio, but there are many standalone tools available for this purpose (including plugins for 3D modeling apps). The resulting split mesh can be imported into Studio as a prefab.
    
    * Original triangle data is lost, so it is not available to script in ray casts.
    */
    convex: boolean
    
    /**
    * Sets geometry from a render mesh. The render mesh is automatically converted to a collision mesh.
    */
    mesh: RenderMesh
    
    /**
    * Specifies skinning component used to animate skinned meshes.
    */
    skin: Skin
    
}

/**
* Mesh topology types used by {@link MeshBuilder}.

* @see Used By: {@link MeshBuilder#topology}, {@link RenderMesh#topology}

* @example
* ```
* // Constructs a new mesh builder
* var builder = new MeshBuilder([
*     { name: "position", components: 3 },
*     { name: "normal", components: 3, normalized: true },
*     { name: "texture0", components: 2 },
*     { name: "color", components: 4 },
* ]);

* builder.topology = MeshTopology.Triangles;
* builder.indexType = MeshIndexType.UInt16;
* ```
*/
declare enum MeshTopology {
    /**
    * Draws unconnected triangles. Each group of three vertices specifies a new triangle.
    */
    Triangles,
    /**
    * Draws connected triangles in a strip. After the first two vertices, each vertex defines the third point on a new triangle extending from the previous one.
    */
    TriangleStrip,
    /**
    * Draws connected triangles sharing one central vertex. The first vertex is the shared one, or "hub" vertex. Starting with the third vertex, each vertex forms a triangle connecting with the previous vertex and hub vertex.
    */
    TriangleFan,
    /**
    * Draws individual points. Each vertex specifies a new point to draw.
    */
    Points,
    /**
    * Draws unconnected line segments. Each group of two vertices specifies a new line segment.
    */
    Lines,
    /**
    * Draws connected line segments. Starting with the second vertex, a line is drawn between each vertex and the preceding one.
    */
    LineStrip
}

/**
* Deprecated. Serves as alias for {@link BaseMeshVisual}.
*/
declare class MeshVisual extends Component {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Mel-frequency cepstral coefficients.

* @see Returned By: {@link MFCCBuilder#build}

* @example
* ```js
* var mfccBuilder = MachineLearning.createMFCCBuilder();

* var mfcc = mfccBuilder.setFrameSize(1920)
*     .setHopSize(480)
*     .setFFTSize(2048)
*     .setNumMFCC(13)
*     .setNumMel(13)
*     .setLifter(0)
*     .setMinFreq(0.0)
*     .setMaxFreq(8000.0)
*     .setSampleRate(16000).build();
* ```
*/
declare class MFCC extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Process in tensor with `shape`, write result to the `outTensor`, and returns the shape of `outTensor`.
    */
    process(inTensor: Float32Array, inShape: vec3, outTensor: Float32Array): vec3
    
    /**
    * Max tensor size.
    
    * @readonly
    */
    maxTensorSize: number
    
}

/**
* Builder class for the MFCC (Mel Frequency Cepstral Co-efficients).

* @see Returned By: {@link MachineLearning.createMFCCBuilder}, {@link MFCCBuilder#setFFTSize}, {@link MFCCBuilder#setFrameSize}, {@link MFCCBuilder#setHopSize}, {@link MFCCBuilder#setLifter}, {@link MFCCBuilder#setMaxFreq}, {@link MFCCBuilder#setMinFreq}, {@link MFCCBuilder#setNumMFCC}, {@link MFCCBuilder#setNumMel}, {@link MFCCBuilder#setSampleRate}

* @example
* ```js
* var mfccBuilder = MachineLearning.createMFCCBuilder();
* ```
*/
declare class MFCCBuilder extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Create a new MFCC object.
    */
    build(): MFCC
    
    /**
    * Sets the length of the FFT window.
    */
    setFFTSize(fftSize: number): MFCCBuilder
    
    /**
    * Length of the window, the window will be the length of frameSize and then padded with zeros to mach FFTSize.
    */
    setFrameSize(frameSize: number): MFCCBuilder
    
    /**
    * Sets the number of samples between successive FFT segments.
    */
    setHopSize(hopSize: number): MFCCBuilder
    
    /**
    * If lifter > 0, apply liftering to the MFCCs.
    */
    setLifter(lifter: number): MFCCBuilder
    
    /**
    * Sets the max frequency.
    */
    setMaxFreq(maxFreq: number): MFCCBuilder
    
    /**
    * Sets the min frequency.
    */
    setMinFreq(minFreq: number): MFCCBuilder
    
    /**
    * Sets the number of MFCCs to return.
    */
    setNumMFCC(numMFCC: number): MFCCBuilder
    
    /**
    * Sets number of mel bins.
    */
    setNumMel(numMel: number): MFCCBuilder
    
    /**
    * Sets the number of samples per second.
    */
    setSampleRate(sampleRate: number): MFCCBuilder
    
}

/**
* The Audio Track Provider of the audio from microphone.

* @example
* ```js
* // @input Asset.AudioTrackAsset audioTrack
* // @input int sampleRate = 44100

* var control = script.audioTrack.control;
* if (control.isOfType("Provider.MicrophoneAudioProvider")) {
*     control.start();
* }

* control.sampleRate = script.sampleRate;
* var audioFrame = new Float32Array(control.maxFrameSize);

* script.createEvent("UpdateEvent").bind(function (eventData) {
*     var audioFrameShape = control.getAudioFrame(audioFrame);
*     if (audioFrameShape.x == 0) {
*         return;
*     }
*     // do something with data
* })
* ```
*/
declare class MicrophoneAudioProvider extends AudioTrackProvider {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Writes current frame audio data to the passed in `Float32Array` and returns its shape. The length of the array can't be more than `maxFrameSize`.
    
    * @exposesUserData
    */
    getAudioFrame(audioFrame: Float32Array): vec3
    
    /**
    * Retrieves the current audio frame, converts it to PCM16 (Pulse-Code Modulation) format, and writes the raw audio samples into the provided `audioFrame` as an `Int16Array`. The length of the array cant be more than `maxFrameSize`.
    
    * @exposesUserData
    */
    getAudioFramePCM16(audioFrame: Int16Array): vec3
    
    /**
    * Start processing audio from microphone. Useful to avoid redundant processing.
    */
    start(): void
    
    /**
    * Stop processing audio from microphone.
    */
    stop(): void
    
}

/**
* Represents a machine learning model that can be integrated with an {@link MLComponent}.

* @see Used By: {@link MLComponent#model}

* @example
* ```js
* //@input Asset.MLAsset model

* var mlComponent = script.sceneObject.createComponent('MLComponent');
* mlComponent.model = script.model;
* ```
*/
declare class MLAsset extends BinAsset {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns model metadata as JSON object.
    */
    getMetadata(): any
    
}

/**

* Used to integrate machine learning models into a Lens.

* @remarks
* This component allows developers to use neural networks for processing inputs such as textures or data arrays to produce specific outputs, which could be in the form of processed textures or data alterations. The MLComponent relies on MLAsset that defines the neural network model used. It supports tasks like image classification, object detection, etc.

* @see [MLComponent Overview](https://developers.snap.com/lens-studio/features/snap-ml/ml-component/ml-component-overview).

* @example
* ```js
* //@input Asset.MLAsset model
* var mlComponent = script.sceneObject.createComponent('MLComponent');
* mlComponent.model = script.model;
* ```
*/
declare class MLComponent extends Component {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Builds the MLComponent model when all placeholders are determined. Config is an array of Input and Output placeholders.
    */
    build(placeholders: BasePlaceholder[]): void
    
    /**
    * Build the MLComponent asynchronously.
    */
    buildAsync(placeholders: BasePlaceholder[]): Promise<void>
    
    /**
    * Stops running the MLComponent. The `onRunningFinished` callback will not be executed.
    */
    cancel(): void
    
    /**
    * Returns the InputPlaceholder with the matching name.
    */
    getInput(name: string): InputPlaceholder
    
    /**
    * Returns an array of InputPlaceholders of the MLComponent's model with default settings. Returns empty array if the model asset is not set.
    */
    getInputs(): InputPlaceholder[]
    
    /**
    * Returns the OutputPlaceholder with the matching name.
    */
    getOutput(name: string): OutputPlaceholder
    
    /**
    * Returns an array of OutputPlaceholders of MLComponent's model with default settings. Returns empty array if the model is not set.
    */
    getOutputs(): OutputPlaceholder[]
    
    /**
    * Returns the end time of the scheduled MLComponent run.
    */
    getScheduledEnd(): MachineLearning.FrameTiming
    
    /**
    * Returns the start time of the scheduled MLComponent run.
    */
    getScheduledStart(): MachineLearning.FrameTiming
    
    /**
    * Returns true if running is requested on each frame.
    */
    isRecurring(): boolean
    
    /**
    * Runs the MLComponent once.
    */
    runImmediate(sync: boolean): void
    
    /**
    * Schedules the MLComponent to run at the start timing and terminate at the end timing. The scheduled running will recur if `recurring` is true.
    */
    runScheduled(recurring: boolean, startTiming: MachineLearning.FrameTiming, endTiming: MachineLearning.FrameTiming): void
    
    /**
    * Stops running the MLComponent.
    */
    stop(): void
    
    /**
    * If loading asynchronously, makes the entire system wait until loading is finished.
    */
    waitOnLoading(): void
    
    /**
    * If running asynchronously, makes the entire system wait until the last run is finished.
    */
    waitOnRunning(): void
    
    /**
    * Runs the MLComponent automatically when the object or component it's on is enabled.
    */
    autoRun: boolean
    
    /**
    * Controls the inference mode that MLComponent will run in. For example, GPU, CPU, etc.
    */
    inferenceMode: MachineLearning.InferenceMode
    
    /**
    * Binary ML model supplied by the user.
    */
    model: MLAsset
    
    /**
    * Bind a function that is triggered when the component fails to load a model.
    */
    onLoadingFailed: (error: string) => void
    
    /**
    * Function that gets called when model loading is finished.
    */
    onLoadingFinished: () => void
    
    /**
    * Function that gets called when the model stops running.
    */
    onRunningFinished: () => void
    
    /**
    * Render order of the MLComponent.
    */
    renderOrder: number
    
    /**
    * Returns the current status of the neural network model.
    
    * @readonly
    */
    state: MachineLearning.ModelState
    
}

/**
* Gives access to motion data and touch events from an external device to Spectacles, as well as haptic feedback requests from Spectacles to an external device. Currently, the API supports Mobile Controller only, allowing one motion controller to be connected at a time. Developers use the Motion Controller API through the {@link MotionControllerModule} in Lens Studio.

* @see [Motion Controller Module](https://developers.snap.com/spectacles/about-spectacles-features/apis/motion-controller) guide.

* @see Returned By: {@link MotionControllerModule#getController}

* @wearableOnly

* @example
* ```js
* const MotionControllerModule = require("LensStudio:MotionControllerModule")
* let options = MotionController.Options.create()
* const motionController = MotionControllerModule.getController(options);

* const sceneObject = script.getSceneObject();
* const transform = sceneObject.getTransform();

* motionController.onTransformEvent.add((eventData) => {
*     transform.setWorldPosition(eventData.worldPosition)
*     transform.setWorldRotation(eventData.worldRotation)
* })
* ```
*/
declare class MotionController extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Get the current motion type being provided by the motion controller.
    
    * @wearableOnly
    */
    getMotionState(): MotionController.MotionType
    
    /**
    * Returns the size of the touchpad in centimeters. Returns `null` if motion controller is not connected.
    
    * @wearableOnly
    */
    getTouchpadPhysicalSize(): vec2 | undefined
    
    /**
    * Returns the size of the touchpad in points. Returns `null` if motion controller is not connected.
    
    * @wearableOnly
    */
    getTouchpadPointSize(): vec2 | undefined
    
    /**
    * Returns the tracking quality state of the motion controller, indicating whether the data received from it is accurate or not.
    
    * @wearableOnly
    */
    getTrackingQuality(): MotionController.TrackingQuality
    
    /**
    * Returns the last known position of the motion controller in world coordinate space. If no motion data has been received, or the motion type is set to `3DOF` or `NoMotion`, this value will be `null`.
    
    * @wearableOnly
    */
    getWorldPosition(): vec3
    
    /**
    * Returns the last known rotation of the motion controller in world coordinate space. If no motion data has been received or the motion type is set to `NoMotion`, this value will be `null`.
    
    * @wearableOnly
    */
    getWorldRotation(): quat
    
    /**
    * Invokes haptic feedback on the controller using a preset of options, if supported.
    
    * @wearableOnly
    */
    invokeHaptic(hapticRequest: MotionController.HapticRequest): void
    
    /**
    * Indicates whether the selected controller is currently available to use. This means that the device is connected, properly set up, and transmitting data.
    
    * @wearableOnly
    */
    isControllerAvailable(): boolean
    
    /**
    * An event triggered when the selected controller's state changes between being available for use (connected, properly set up, and transmitting data) and otherwise.
    
    * @readonly
    
    * @wearableOnly
    */
    onControllerStateChange: event1<boolean, void>
    
    /**
    * Event triggered when the motion type of the controller changes.
    
    * @readonly
    
    * @wearableOnly
    */
    onMotionTypeChange: event1<MotionController.MotionType, void>
    
    /**
    * Triggered by a touch event from the controller.
    * Arguments:
    * - **normalizedPosition:** A normalized 2D position of the user's touch on the touchpad. The coordinates range from ([0-1], [0-1]), where (0,0) represents the top-left and (1,1) represents the bottom-right.
    * - **touchId:** Returns the unique identifier of the specific touch; useful for distinguishing between multiple simultaneous touches.
    * - **timestampMilliseconds:** Returns the timestamp, in milliseconds, of when the touch event occurred.
    * - **phase:** The current state of the touch.
    
    * @readonly
    
    * @wearableOnly
    */
    onTouchEvent: event4<vec2, number, number, MotionController.TouchPhase, void>
    
    /**
    * Triggered when the touchpad size is changed. Custom controllers can adjust the interactable area of the touchpad.
    
    * @readonly
    
    * @wearableOnly
    */
    onTouchpadSizeChange: event2<vec2, vec2, void>
    
    /**
    * Event triggered when the tracking quality state of the motion controller changes.
    
    * @readonly
    
    * @wearableOnly
    */
    onTrackingQualityChange: event1<MotionController.TrackingQuality, void>
    
    /**
    * An event is triggered when new motion data becomes available. The arguments are the world position and the world rotation of the motion controller, respectively.
    
    * @readonly
    
    * @wearableOnly
    */
    onTransformEvent: event2<vec3, quat, void>
    
    /**
    * Returns the configuration of the motion controller.
    
    * @readonly
    
    * @wearableOnly
    */
    options: MotionController.MotionControllerOptions
    
}

declare namespace MotionController {
    /**
    * Defines a set of haptic feedback patterns that can be requested.
    
    * @see Used By: {@link MotionController.HapticRequest#hapticFeedback}
    
    * @wearableOnly
    
    * @example
    * ```ts
    * const MotionControllerModule = require("LensStudio:MotionControllerModule")
    
    * @component
    * export class NewScript extends BaseScriptComponent {
    
    *     private controller: MotionController
    *     private feedbackId: number = 0
    
    *     onAwake() {
    *         var options = MotionController.Options.create()
    *         options.motionType = MotionController.MotionType.SixDoF
    
    *         this.controller = MotionControllerModule.getController(options)
    
    *         this.controller.onTouchEvent.add(this.testHapticFeedback.bind(this));
    *     }
    *     testHapticFeedback(normalizedPosition, touchId, timestampMs, phase) {
    
    *         if (phase != MotionController.TouchPhase.Began) {
    *             //we only want to trigger the feedback on touch start
    *             return;
    *         }
    *         this.feedbackId = (this.feedbackId + 1) % 8
    *         var request = MotionController.HapticRequest.create()
    *         switch (this.feedbackId) {
    *             case 0:
    *                 request.hapticFeedback = MotionController.HapticFeedback.Default
    *                 print("Default")
    *                 break
    *             case 1:
    *                 request.hapticFeedback = MotionController.HapticFeedback.Tick
    *                 print("Tick")
    *                 break
    *             case 2:
    *                 request.hapticFeedback = MotionController.HapticFeedback.Select
    *                 print("Select")
    *                 break
    *             case 3:
    *                 request.hapticFeedback = MotionController.HapticFeedback.Success
    *                 print("Success")
    *                 break
    *             case 4:
    *                 request.hapticFeedback = MotionController.HapticFeedback.Error
    *                 print("Error")
    *                 break
    *             case 5:
    *                 request.hapticFeedback = MotionController.HapticFeedback.VibrationLow
    *                 print("Vibration Low")
    *                 break
    *             case 6:
    *                 request.hapticFeedback = MotionController.HapticFeedback.VibrationMedium
    *                 print("Vibration Medium")
    *                 break
    *             case 7:
    *                 request.hapticFeedback = MotionController.HapticFeedback.VibrationHigh
    *                 print("Vibration High")
    *                 break
    *             default:
    *                 print("Unknown feedbackId = " + this.feedbackId)
    *         }
    *         request.duration = 0.3
    *         this.controller.invokeHaptic(request)
    *     }
    * }
    * ```
    */
    enum HapticFeedback {
        /**
        * Default value, same as `Tick`.
        
        * @wearableOnly
        */
        Default,
        /**
        * A brief, single haptic effect that simulates a ticking or clicking sensation
        
        * @wearableOnly
        */
        Tick,
        /**
        * A subtle haptic effect used to confirm a selection or interaction.
        
        * @wearableOnly
        */
        Select,
        /**
        * A positive haptic pattern indicating that an action was completed successfully.
        
        * @wearableOnly
        */
        Success,
        /**
        * A negative haptic pattern indicating that an action failed or encountered an issue.
        
        * @wearableOnly
        */
        Error,
        /**
        * A gentle vibration for less intense feedback.
        
        * @wearableOnly
        */
        VibrationLow,
        /**
        * A moderate vibration for standard feedback intensity.
        
        * @wearableOnly
        */
        VibrationMedium,
        /**
        * A strong vibration for more pronounced feedback.
        
        * @wearableOnly
        */
        VibrationHigh
    }

}

declare namespace MotionController {
    /**
    * Describes a request for haptic feedback.
    
    * @see Used By: {@link MotionController#invokeHaptic}
    * @see Returned By: {@link MotionController.HapticRequest.create}
    
    * @wearableOnly
    
    * @example
    * ```
    * //typescript
    
    * const MotionControllerModule = require("LensStudio:MotionControllerModule")
    
    * @component
    * export class NewScript extends BaseScriptComponent {
    *     @input
    *     @allowUndefined
    *     target: SceneObject
    
    *     @input setRotation: boolean
    *     @input setPosition: boolean
    
    *     private transform;
    *     private controller;
    
    *     onAwake() {
    *         var options = MotionController.Options.create()
    *         options.motionType = MotionController.MotionType.SixDoF
    
    *         this.controller = MotionControllerModule.getController(options)
    *         this.target = this.target === undefined ? this.sceneObject : this.target;
    *         this.transform = this.target.getTransform()
    
    *         this.controller.onTransformEvent.add(this.updateTransform.bind(this));
    
    *         this.controller.onTouchEvent.add(this.onTouchEvent.bind(this));
    *     }
    
    *     updateTransform(position, rotation) {
    *         if (this.setPosition) {
    *             this.transform.setWorldPosition(position);
    *         }
    *         if (this.setRotation) {
    *             this.transform.setWorldRotation(rotation);
    *         }
    *     }
    
    *     onTouchEvent(normalizedPosition, touchId, timestampMs, phase) {
    *         if (phase != MotionController.TouchPhase.Began) {
    *             return
    *         }
    *         var request = MotionController.HapticRequest.create()
    *         request.hapticFeedback = MotionController.HapticFeedback.Tick
    *         request.duration = 1.0
    
    *         this.controller.invokeHaptic(request)
    *     }
    * }
    * ```
    */
    class HapticRequest extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * How long the haptic request should last for.
        
        * @wearableOnly
        */
        duration: number
        
        /**
        * A type of haptic feedback.
        
        * @wearableOnly
        */
        hapticFeedback: MotionController.HapticFeedback
        
        /**
        * Create an instance of the Haptic Request.
        
        * @wearableOnly
        */
        static create(): MotionController.HapticRequest
        
    }

}

declare namespace MotionController {
    /**
    * Settings for configuring a motion controller.
    
    * @see Used By: {@link MotionController#options}, {@link MotionControllerModule#getController}
    * @see Returned By: {@link MotionController.MotionControllerOptions.create}
    
    * @wearableOnly
    
    * @example
    * ```
    * const MotionControllerModule = require("LensStudio:MotionControllerModule")
    * let options = MotionController.Options.create()
    * options.motionType = MotionController.MotionType.ThreeDoF
    * const motionController = MotionControllerModule.getController(options)
    * ```
    */
    class MotionControllerOptions extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * The unique identifier to connect to a motion controller. The only value currently supported is empty (`""`), which will result in the Mobile Controller being requested.
        
        * @wearableOnly
        */
        controllerId: string
        
        /**
        * Represents the motion type of the motion controller.
        
        * @wearableOnly
        */
        motionType: MotionController.MotionType
        
        /**
        * Creates the configurations to be used with the {@link MotionController}
        
        * @wearableOnly
        */
        static create(): MotionController.MotionControllerOptions
        
    }

}

declare namespace MotionController {
    /**
    * Enum for describing the motion type.
    
    * @see Used By: {@link MotionController.MotionControllerOptions#motionType}
    * @see Returned By: {@link MotionController#getMotionState}
    
    * @wearableOnly
    
    * @example
    * ```js
    * const MotionControllerModule = require('LensStudio:MotionControllerModule');
    * let options = MotionController.Options.create();
    * options.motionType = MotionController.MotionType.SixDoF;
    * const motionController = MotionControllerModule.getController(options);
    
    * const sceneObject = script.getSceneObject();
    * const transform = sceneObject.getTransform();
    
    * motionController.onTransformEvent.add((worldPosition, worldRotation) => {
    *   transform.setWorldPosition(worldPosition);
    *   transform.setWorldRotation(worldRotation);
    * });
    * ```
    */
    enum MotionType {
        /**
        * Transform of this object does not change.
        
        * @wearableOnly
        */
        NoMotion,
        /**
        * Only the rotation of the object is changed.
        
        * @wearableOnly
        */
        ThreeDoF,
        /**
        * Both position and rotation of the object are changed.
        
        * @wearableOnly
        */
        SixDoF
    }

}

declare namespace MotionController {
    /**
    * Settings for configuring a motion controller.
    */
    class Options {
        
        /** @hidden */
        protected constructor()
        
        /**
        * Create a new options object.
        
        * @deprecated
        
        * @wearableOnly
        */
        static create(): MotionController.MotionControllerOptions
        
    }

}

declare namespace MotionController {
    /**
    * Enum that defines a current state of a touch interaction with the touchpad of the motion controller.
    
    * @wearableOnly
    
    * @example
    * ```
    * controller.onTouchEvent.add(eventData => {
    *     switch (eventData.phase) {
    *         case MotionController.TouchPhase.Began:
    *             print("Began touch")
    *             break;
    *         case MotionController.TouchPhase.Canceled:
    *             print("Canceled touch")
    *             break;
    *         case MotionController.TouchPhase.Moved:
    *             print("Touch moved")
    *             break;
    *         case MotionController.TouchPhase.Ended:
    *             print("Touch Ended")
    *             break;
    *     }
    * })
    * ```
    */
    enum TouchPhase {
        /**
        * Indicates that a touch event has started. This is triggered when the user initially touches the interactive area.
        
        * @wearableOnly
        */
        Began,
        /**
        * Indicates that the touch event has moved. This is triggered when the user drags or slides their finger across the interactive area.
        
        * @wearableOnly
        */
        Moved,
        /**
        * Indicates that the touch event has ended. This occurs when the user lifts their finger off the interactive area, completing the touch interaction.
        
        * @wearableOnly
        */
        Ended,
        /**
        * Indicates that the touch event was interrupted or canceled, typically due to an error or the touch being outside the interactive area.
        
        * @wearableOnly
        */
        Canceled
    }

}

declare namespace MotionController {
    /**
    * Describes Motion Controller tracking quality state, whether the data received from the Motion Controller is accurate or not.
    
    * @see Returned By: {@link MotionController#getTrackingQuality}
    
    * @wearableOnly
    */
    enum TrackingQuality {
        /**
        * The tracking quality is unknown. This usually means that the controller is not available.
        
        * @wearableOnly
        */
        Unknown,
        /**
        * Transform tracking of the Motion Controller is providing optimal results.
        
        * @wearableOnly
        */
        Normal,
        /**
        * Transform tracking of the Motion Controller is providing limited quality results.
        
        * @wearableOnly
        */
        Limited
    }

}

/**
* Provides access to {@link MotionController}.

* @see [Motion Controller Module](https://developers.snap.com/spectacles/about-spectacles-features/apis/motion-controller) guide.

* @wearableOnly

* @example
* ```
* const MotionControllerModule = require("LensStudio:MotionControllerModule")
* let options = MotionController.Options.create()
* options.motionType = MotionController.MotionType.SixDoF
* const motionController = MotionControllerModule.getController(options)
* ```
*/
declare class MotionControllerModule extends Asset {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Get the Motion Controller with the provided options. If no options are provided, default value will be used.
    
    * @wearableOnly
    */
    getController(options: MotionController.MotionControllerOptions): MotionController
    
}

/**
* Triggered when the tracked face's mouth closes.

* @example
* ```js
* var event = script.createEvent("MouthClosedEvent");
* event.faceIndex = 0;
* event.bind(function (eventData)
* {
* 	print("Mouth was closed on face 0");
* });
* ```
*/
declare class MouthClosedEvent extends FaceTrackingEvent {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Triggered when the tracked face's mouth opens.

* @example
* ```js
* var event = script.createEvent("MouthOpenedEvent");
* event.faceIndex = 0;
* event.bind(function (eventData)
* {
* 	print("Mouth was opened on face 0");
* });
* ```
*/
declare class MouthOpenedEvent extends FaceTrackingEvent {
    
    /** @hidden */
    protected constructor()
    
}

/**
* An instance of a Connected Lens session among a group of participants who were successfully invited into the experience.

* @see Used By: {@link BaseMultiplayerSessionOptions#onConnected}, {@link BaseMultiplayerSessionOptions#onDisconnected}, {@link BaseMultiplayerSessionOptions#onError}, {@link BaseMultiplayerSessionOptions#onHostUpdated}, {@link BaseMultiplayerSessionOptions#onMessageReceived}, {@link BaseMultiplayerSessionOptions#onRealtimeStoreCreated}, {@link BaseMultiplayerSessionOptions#onRealtimeStoreDeleted}, {@link BaseMultiplayerSessionOptions#onRealtimeStoreKeyRemoved}, {@link BaseMultiplayerSessionOptions#onRealtimeStoreOwnershipUpdated}, {@link BaseMultiplayerSessionOptions#onRealtimeStoreUpdated}, {@link BaseMultiplayerSessionOptions#onUserJoinedSession}, {@link BaseMultiplayerSessionOptions#onUserLeftSession}, {@link CloudStorageOptions#session}, {@link ColocatedTrackingComponent#join}, {@link ColocatedTrackingComponent#startBuilding}, {@link ConnectedLensModule#shareSession}, {@link ConnectedLensSessionOptions#onSessionCreated}, {@link LocationCloudStorageModule#session}
*/
declare class MultiplayerSession extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Sets the realtime store to unowned.
    */
    clearRealtimeStoreOwnership(store: GeneralDataStore, onSuccess: (store: GeneralDataStore) => void, onError: (message: string) => void): void
    
    /**
    * Creates a realtime store to share data within a multiplayer session. The realtime store will allow you to pass data between multiple connected users in the same experience, such as: syncing the position of objects in the scene, syncing the current game score, etc.
    */
    createRealtimeStore(options: RealtimeStoreCreateOptions, onSuccess: (store: GeneralDataStore) => void, onError: (message: string) => void): void
    
    /**
    * Deletes a realtime store.
    */
    deleteRealtimeStore(store: GeneralDataStore, onSuccess: (store: GeneralDataStore) => void, onError: (message: string) => void): void
    
    /**
    * Deletes the value from the specified scope.
    
    * @deprecated
    */
    deleteStoredValue(key: string, scope: StorageScope, onDeleted: () => void, onError: (code: string, description: string) => void): void
    
    getLocalUserId(localUserIdCallback: (userId: string) => void): void
    
    /**
    * Gets information about the current user, which includes their display name and unique `userID` which can be used to identify them in session.
    */
    getLocalUserInfo(localUserInfoCallback: (userInfo: ConnectedLensModule.UserInfo) => void): void
    
    /**
    * Returns information about the passed in RealtimeStore.
    */
    getRealtimeStoreInfo(store: GeneralDataStore): ConnectedLensModule.RealtimeStoreCreationInfo
    
    /**
    * Returns a unix timestamp in milliseconds of the current time according to the server. Useful for synchronizing time-based game events across devices.
    * `-1` will be returned if session is not connected to the server.
    */
    getServerTimestamp(): number
    
    /**
    * Get a value stored within the specified scope.
    
    * @deprecated
    */
    getStoredValue(key: string, scope: StorageScope, onRetrieved: (key: string, value: (mat4|mat3|mat2|vec4|vec2|vec3|boolean|quat|number|string)) => void, onError: (code: string, description: string) => void): void
    
    /**
    * List all values stored within the specified scope.
    
    * @deprecated
    */
    listStoredValues(scope: StorageScope, cursor: string, onRetrieved: (values: (mat4|mat3|mat2|vec4|vec2|vec3|boolean|quat|number|string)[][], cursor: string) => void, onError: (code: string, description: string) => void): void
    
    /**
    * Requests for ownership of the realtime store.
    */
    requestRealtimeStoreOwnership(store: GeneralDataStore, onSuccess: (store: GeneralDataStore) => void, onError: (message: string) => void): void
    
    /**
    * Sends a string message via the realtime backend.
    */
    sendMessage(message: string): void
    
    /**
    * Sends a string message with a timeout in milliseconds. Allows messages to be dropped from the outgoing message queue when they become too old. Useful to ensure the quick delivery of subsequent messages.
    */
    sendMessageWithTimeout(message: string, timeoutMs: number): void
    
    /**
    * Saves the value to the specified scope.
    
    * @deprecated
    */
    setStoredValue(key: string, value: (mat4|mat3|mat2|vec4|vec2|vec3|boolean|quat|number|string), options: StorageOptions, onSaved: () => void, onError: (code: string, description: string) => void): void
    
    /**
    * Get the number of active users in the Session.
    
    * @readonly
    */
    activeUserCount: number
    
    /**
    * @readonly
    */
    activeUsersInfo: ConnectedLensModule.UserInfo[]
    
    /**
    * Get all the Realtime Stores in the current session.
    
    * @readonly
    */
    allRealtimeStores: GeneralDataStore[]
    
}

/**
* A type containing two arrays which map positionally to each other.   For example, this is used by {@link FaceRenderObjectProvider}, to provide a `names` array which contains the list of expresion names on the face, while the `values` array which contains the weight of each expression.
*/
declare class NamedValues extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The array of names which positionally correspond to the `value` array property.
    
    * @readonly
    */
    names: string[]
    
    /**
    * The array of values which positionally correspond to the `names` array property.
    
    * @readonly
    */
    values: Float32Array
    
}

/**
* Tracking type used by the {@link DeviceTracking} component to specify what type of plane to detect.

* @see Used By: {@link WorldOptions#nativePlaneTrackingType}
*/
declare enum NativePlaneTrackingType {
    /**
    * No planes will be detected.
    */
    None,
    /**
    * Only horizontal planes will be detected.
    */
    Horizontal,
    /**
    * Only vertical planes will be detected.
    */
    Vertical,
    /**
    * Both horizontal and vertical planes will be detected.
    */
    Both
}

/**


* @see Returned By: {@link NoiseReductionBuilder#build}
*/
declare class NoiseReduction extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    process(inTensor: Float32Array, inShape: vec3, outTensor: Float32Array): vec3
    
    amount: number
    
    /**
    * @readonly
    */
    maxTensorSize: number
    
}

/**


* @see Returned By: {@link MachineLearning.createNoiseReductionBuilder}, {@link NoiseReductionBuilder#setSampleRate}
*/
declare class NoiseReductionBuilder extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    build(): NoiseReduction
    
    setSampleRate(sampleRate: number): NoiseReductionBuilder
    
}

/**
* Base class for configuring object tracking in the {@link ObjectTracking3D} component.

* @see Used By: {@link ObjectTracking3D#trackingAsset}
*/
declare class Object3DAsset extends Asset {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Represents reusable {@link SceneObject} hierarchy stored as a resource.

* @remarks
* Can be instantiated through script or brought into the scene by dragging from `Asset Browser` to `Scene Hierarchy` panel.

* @see [Prefabs](https://developers.snap.com/lens-studio/lens-studio-workflow/prefabs) guide.

* @example
* ```js
* // Instantiate a prefab under a parent scene object
* // @input Asset.ObjectPrefab prefab
* // @input SceneObject parentObject
* var newObject = script.prefab.instantiate(script.parentObject);
* print("Prefab Instantiated: " + newObject.name);
* ```
*/
declare class ObjectPrefab extends Asset {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Creates and returns a new instance of this object hierarchy underneath the specified parent object.
    * If parent is null, the object will be created with no parent.
    */
    instantiate(parent: SceneObject): SceneObject
    
    /**
    * Instantiate a prefab asynchronously.
    */
    instantiateAsync(parent: SceneObject, onSuccess: (sceneObject: SceneObject) => void, onFailure: (error: string) => void, onProgress: (progress: number) => void): void
    
}

/**
* Provides additional data for the tracked object. For example, with hand tracking, you can figure out whether the tracked hand is the left hand by accessing the `isLeft` property [true/false], as well as the probability of this data through the `isLeftProbability` property [0-1].

* @see Used By: {@link ObjectTracking#objectSpecificData}, {@link ObjectTracking3D#objectSpecificData}

* @example
* ```js
* // @input Component.ObjectTracking handTracking
* var handSpecificData =  script.handTracking.objectSpecificData;
* if (handSpecificData) {
*     var isLeftHand = handSpecificData.isLeft;
*     print(isLeftHand);
* }
* ```
*/
declare class ObjectSpecificData extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Used to track objects in 2D space, such as body parts, pet, hand.

* @remarks
* Moves the local {@link ScreenTransform} to match the detected image.

* @see [Object Tracking](https://developers.snap.com/lens-studio/features/ar-tracking/world/object-tracking) guide.
* @see[Hand Gestures](https://developers.snap.com/lens-studio/features/ar-tracking/hand/hand-gestures) guide.

* @example
* ```js
* // @input Component.ObjectTracking objectTracking

* script.objectTracking.onObjectFound = function() {
*     print("Object has been found!");
* };

* script.objectTracking.onObjectLost = function() {
*     print("Object has been lost!");
* };
* ```

* ```js
* // @input Component.ObjectTracking  handTracking

* var descriptor = "open";
* // All possible descriptors for Hand Tracking are:
* // "victory", "open", "index_finger", "horns", "close", "thumb"

* script.handTracking.registerDescriptorStart(descriptor, function () {
*     print(descriptor + " gesture was detected!");
* });
* ```
*/
declare class ObjectTracking extends Component {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns true if the object is currently being tracked on camera.
    */
    isTracking(): boolean
    
    /**
    * Registers a callback to be executed when the passed in descriptor ends for this tracked object. For example, the possible descriptors for hand tracking are: "victory", "open", "index_finger", "horns", "close", "thumb".
    */
    registerDescriptorEnd(descriptor: string, callback: (descriptor: string) => void): void
    
    /**
    * Registers a callback to be executed when the passed in descriptor starts for this tracked object. For example, the possible descriptors for hand tracking are: "victory", "open", "index_finger", "horns", "close", "thumb".
    */
    registerDescriptorStart(descriptor: string, callback: (descriptor: string) => void): void
    
    /**
    * If true, child objects of this Component's {@link SceneObject} will be disabled when the object is not being tracked.
    */
    autoEnableWhenTracking: boolean
    
    /**
    * The index of the object being tracked.
    */
    objectIndex: number
    
    /**
    * Gets additional data of the current object being tracked.
    
    * @readonly
    */
    objectSpecificData: ObjectSpecificData
    
    /**
    * Function that gets called when the tracked object is found.
    */
    onObjectFound: () => void
    
    /**
    * Function that gets called when the tracked object is lost.
    */
    onObjectLost: () => void
    
}

/**
* Used to track objects in 3D space, such as body, hands.

* @see [3D Body and Hand Tracking](https://developers.snap.com/lens-studio/features/ar-tracking/body/object-tracking-3d) guide.

* @example
* ```js
* // @input Component.ObjectTracking3D objectTracking3D

* script.objectTracking3D.onTrackingStarted = function(){
*     print("ObjectTracking3D Started Tracking");
* }

* script.objectTracking3D.onTrackingLost = function(){
*     print("ObjectTracking3D Lost Tracking");
* }
* ```


* Using `objectSpecificData` to get additional information about the tracked object.

* ```js
* //@input Component.ObjectTracking3D objectTracking3D

* script.createEvent("UpdateEvent").bind(()=> {
*     getHandVelocity();
* })

* function getHandVelocity() {
*     // In Lens Studio we do not have the objectSpecificData we need.
*     if (global.deviceInfoSystem.isEditor()) {
*       return new vec3(0, 1, -35);
*     }

*     // Each ObjectTracking3D Component might provide their own specific data
*     let objectSpecificData = this.objectTracking3D.objectSpecificData;
*     if (objectSpecificData) {
*       let handVelocity = objectSpecificData["global"];
*       let fingerVelocity = objectSpecificData["index-3"];

*       print("Hand Velocity: " + handVelocity);
*       print("Finger Velocity: " + fingerVelocity);

*       return handVelocity;
*     } else {
*       return vec3.zero();
*     }
* }
* ```

* ```ts
* @component
* export class NewScript extends BaseScriptComponent {
*     @input
*     objectTracking3D: ObjectTracking3D;

*     onAwake() {
*         this.createEvent("UpdateEvent").bind(() => {
*             this.getHandVelocity();
*         });
*     }
*     getHandVelocity() {
*         // In Lens Studio we do not have the objectSpecificData we need.
*         if (global.deviceInfoSystem.isEditor()) {
*             return new vec3(0, 1, -35);
*         }

*         // Each ObjectTracking3D Component might provide their own specific data
*         let objectSpecificData = this.objectTracking3D.objectSpecificData;
*         if (objectSpecificData) {
*             let handVelocity = objectSpecificData["global"];
*             let fingerVelocity = objectSpecificData["index-3"];

*             print("Hand Velocity: " + handVelocity);
*             print("Finger Velocity: " + fingerVelocity);

*             return handVelocity;
*         } else {
*             return vec3.zero();
*         }
*     }
* }
* ```
*/
declare class ObjectTracking3D extends Component {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Attaches the SceneObject to the specified attachment point.
    */
    addAttachmentPoint(name: string, object: SceneObject): void
    
    /**
    * Creates a SceneObject which is attached to the specified attachment point.
    */
    createAttachmentPoint(name: string): SceneObject
    
    /**
    * Returns all SceneObjects currently attached to the specified point.
    */
    getAttachedObjects(name: string): SceneObject[]
    
    /**
    * Returns whether the specified attachment point is being tracked.
    */
    isAttachmentPointTracking(name: string): boolean
    
    /**
    * Returns whether this object is currently being tracked.
    */
    isTracking(): boolean
    
    removeAttachmentPoint(object: SceneObject): void
    
    /**
    * Whether world rotation is applied or not.
    */
    attachmentModeInheritRotation: boolean
    
    /**
    * Whether world scale is applied or not.
    */
    attachmentModeInheritScale: boolean
    
    /**
    * Index of the object to track, starting at 0. Useful when tracking multiple instances of the same type of object.
    */
    objectIndex: number
    
    /**
    * The data provided by the tracker on this component.
    
    * @see {@link SpectaclesHandSpecificData}
    
    * @readonly
    
    * @wearableOnly
    */
    objectSpecificData: ObjectSpecificData
    
    /**
    * Function called when tracking is lost.
    */
    onTrackingLost: () => void
    
    /**
    * Function called when tracking begins.
    */
    onTrackingStarted: () => void
    
    /**
    * When true, the attached root SceneObject's world position will be updated to match the tracked object's world position.
    */
    trackPosition: boolean
    
    /**
    * Asset containing tracking parameters, such as the tracking model and specific options.
    */
    trackingAsset: Object3DAsset
    
    /**
    * Strategy for updating attached SceneObjects.
    */
    trackingMode: ObjectTracking3D.TrackingMode
    
}

declare namespace ObjectTracking3D {
    /**
    * Strategies for updating attached objects. Used by the {@link ObjectTracking3D} component.
    
    * @see Used By: {@link ObjectTracking3D#trackingMode}
    */
    enum TrackingMode {
        /**
        * Update the local transform's rotation and scale
        */
        ProportionsAndPose,
        /**
        * Update the local transform's rotation only
        */
        PoseOnly,
        /**
        * Applies world position and world rotation by default, and optionally world scale. Additionally, the world rotation can be opt-out via the `attachmentModeInheritRotation` property
        */
        Attachment
    }

}

declare class ObjectTrackingMaskedTextureProvider extends CropTextureProvider {
    
    /** @hidden */
    protected constructor()
    
    objectIndex: number
    
}

declare class ObjectTrackingNormalsTextureProvider extends ObjectTrackingMaskedTextureProvider {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Controls a segmentation texture and its placement using information provided by Object tracking.
*/
declare class ObjectTrackingTextureProvider extends ObjectTrackingMaskedTextureProvider {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Triggered when the Lens starts, earlier than all OnStart events. Also fires immediately on a newly instantiated or copied object.
*/
declare class OnAwakeEvent extends SceneEvent {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Triggered when the associated ScriptComponent is destroyed while the lens is running.

* @example
* ```
* script.createEvent("OnDestroyEvent").bind(function(){
*     // Clean up objects
* })
* ```
*/
declare class OnDestroyEvent extends SceneEvent {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Triggered when the `ScriptComponent` this event is bound to is disabled.

* @example
* ```
* script.createEvent("OnDisableEvent").bind(function(){
*     print("Component Disabled");
* })
* ```
*/
declare class OnDisableEvent extends SceneEvent {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Triggered when the `ScriptComponent` this event is bound to is enabled.

* @example
* ```
* script.createEvent("OnEnableEvent").bind(function(){
*     print("Component Enabled");
* })
* ```
*/
declare class OnEnableEvent extends SceneEvent {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Triggered when users tap the menu button in Palm UI.

* When paused, certain behaviors like animations or script updates continue running in the background, while inputs (e.g., Hand Tracking, Mobile Controller) are disabled to allow the system overlay to take priority. Developers should manage scenarios where the Lens is paused to maintain a seamless experience.

* @wearableOnly

* @example
* Example js
* ```js
* script.createEvent("OnPauseEvent").bind(function (eventData)
* {
*     print("Paused");
* });
* ```

* Example ts
* ```ts
* @component
* export class NewScript extends BaseScriptComponent {
*   onAwake() {
*     this.createEvent("OnPauseEvent").bind(() => {
*       print("Paused");
*     });
*   }
* }
* ```
*/
declare class OnPauseEvent extends SceneEvent {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Triggered when users tap the menu button in Palm UI during pause state.

* When paused, certain behaviors like animations or script updates continue running in the background, while inputs (e.g., Hand Tracking, Mobile Controller) are disabled to allow the system overlay to take priority. Developers should manage scenarios where the Lens is resumed after pause to maintain a seamless experience.

* @wearableOnly

* @example
* Example js
* ```js
* script.createEvent("OnResumeEvent").bind(function (eventData)
* {
*     print("Resumed");
* });
* ```

* Example ts
* ```ts
* @component
* export class NewScript extends BaseScriptComponent {
*   onAwake() {
*     this.createEvent("OnResumeEvent").bind(() => {
*       print("Resumed");
*     });
*   }
* }
* ```
*/
declare class OnResumeEvent extends SceneEvent {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Triggered when the Lens starts, after all OnAwakeEvents have triggered. Also triggers later on newly instantiated or copied objects.
*/
declare class OnStartEvent extends SceneEvent {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Types of operating system that may be running on the device.

* @see Returned By: {@link DeviceInfoSystem#getOS}

* @example
* ```js
* var osType = global.deviceInfoSystem.getOS();
* var isMobile = (osType == OS.iOS || osType == OS.Android);
* ```
*/
declare enum OS {
    /**
    * iOS device
    */
    iOS,
    /**
    * MacOS device
    */
    MacOS,
    /**
    * Android device
    */
    Android,
    /**
    * Windows device
    */
    Windows
}

/**
* Used in {@link Text}'s `outlineSettings` property.
* Configures how text outlining will appear on a Text component.

* @see Used By: {@link Text#outlineSettings}

* @example
* ```js
* // @input Component.Text textComponent

* var outlineSettings = script.textComponent.outlineSettings;

* outlineSettings.enabled = true;
* outlineSettings.size = 0.25;
* outlineSettings.fill.color = new vec4(0, 0, 1, 1);
* ```
*/
declare class OutlineSettings extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Whether outline is enabled on the Text.
    */
    enabled: boolean
    
    /**
    * Settings for how the outline is drawn.
    */
    fill: TextFill
    
    /**
    * The strength of the outline effect, ranging from 0.0 (no outline) to 1.0 (very strong outline).
    */
    size: number
    
}

/**
* Builds OutputPlaceholders for MLComponent.

* @see Returned By: {@link MachineLearning.createOutputBuilder}, {@link OutputBuilder#setName}, {@link OutputBuilder#setOutputMode}, {@link OutputBuilder#setShape}, {@link OutputBuilder#setTransformer}

* @example
* ```js
* //@input vec2 outputSize = {1, 1}
* //@input string outputName = "probs"

* var outputChannels = 200;

* var outputBuilder = MachineLearning.createOutputBuilder();
* outputBuilder.setName(script.outputName);
* outputBuilder.setShape(new vec3(script.outputSize.x, script.outputSize.y, outputChannels));
* outputBuilder.setOutputMode(MachineLearning.OutputMode.Data);
* var outputPlaceholder = outputBuilder.build();
* ```
*/
declare class OutputBuilder extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Builds and returns a new OutputPlaceholder.
    */
    build(): OutputPlaceholder
    
    /**
    * Sets the name of the OutputPlaceholder to be built.
    */
    setName(name: string): OutputBuilder
    
    /**
    * Sets the OutputMode of the OutputPlaceholder to be built.
    */
    setOutputMode(outputMode: MachineLearning.OutputMode): OutputBuilder
    
    /**
    * Sets the shape of the OutputPlaceholder to be built.
    */
    setShape(shape: vec3): OutputBuilder
    
    /**
    * Sets the Transformer of the OutputPlaceholder to be built.
    */
    setTransformer(transformer: Transformer): OutputBuilder
    
}

/**
* Provides output data from the neural network used by an MLComponent.
* For more information, see the [MLComponent Scripting](https://developers.snap.com/lens-studio/features/snap-ml/ml-component/scripting-ml-component) guide.

* @see Returned By: {@link MLComponent#getOutput}, {@link OutputBuilder#build}

* @example
* ```js
* //@input Component.MLComponent mlComponent
* //@input string outputName = "output"

* var outputData;

* script.mlComponent.onLoadingFinished = onLoadingFinished;

* function onLoadingFinished() {
*     var output = script.mlComponent.getOutput(script.outputName);
*     outputData = output.data;
* }

* function onUpdate() {
*     // do something with data
* }
* ```
*/
declare class OutputPlaceholder extends BasePlaceholder {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Output as a Float32Array. Usable when `mode` is set to `MachineLearning.OutputMode.Data`.
    
    * @readonly
    
    * @exposesUserData
    */
    data: Float32Array
    
    /**
    * Which type of data the output is provided as. For example, Texture or Data.
    */
    mode: MachineLearning.OutputMode
    
    /**
    * Output as a Texture. Usable when `mode` is set to `MachineLearning.OutputMode.Texture`.
    
    * @readonly
    */
    texture: Texture
    
}

/**
* Exposes state generated for ColliderComponent overlap events.

* @see {@link ColliderComponent}
* @see {@link OverlapEnterEventArgs}
* @see {@link OverlapExitEventArgs}
* @see {@link OverlapStayEventArgs}

* @see Used By: {@link OverlapEnterEventArgs#overlap}, {@link OverlapExitEventArgs#overlap}, {@link OverlapStayEventArgs#overlap}

* @example
* ```js
* // When the collider begins to overlap another collider, print information about the overlap

* // @input Physics.ColliderComponent collider

* script.collider.onOverlapEnter.add(function (e) {
*     var overlap = e.overlap;
*     print("OverlapEnter(" + overlap.id + "): Overlap with: " + overlap.collider.getSceneObject().name);
* });
* ```
*/
declare class Overlap extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The collider which is overlapping.
    
    * @readonly
    */
    collider: ColliderComponent
    
    /**
    * ID of the overlap, unique for this overlap.
    
    * @readonly
    */
    id: number
    
}

/**
* Args used for {@link ColliderComponent.onOverlapEnter}, which is triggered when the collider begins overlapping another object. Typically used for volume triggers.

* @example
* ```js
* // When the collider begins to overlap another collider, print information about the overlap

* // @input Physics.ColliderComponent collider

* script.collider.onOverlapEnter.add(function (e) {
*     var overlap = e.overlap;
*     print("OverlapEnter(" + overlap.id + "): Overlap with: " + overlap.collider.getSceneObject().name);
* });
* ```
*/
declare class OverlapEnterEventArgs extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Number of objects currently overlapping.
    
    * @readonly
    */
    currentOverlapCount: number
    
    /**
    * Array of all current overlaps.
    
    * @readonly
    */
    currentOverlaps: Overlap[]
    
    /**
    * Structure containing information about the current overlap.
    
    * @readonly
    */
    overlap: Overlap
    
}

/**
* Args used for {@link ColliderComponent.onOverlapExit}, which is triggered when the collider stops overlapping another object. Typically used for volume triggers.

* @example
* ```js
* // When the collider stops overlapping another collider, print information about the overlap

* // @input Physics.ColliderComponent collider

* script.collider.onOverlapExit.add(function (e) {
*     var overlap = e.overlap;
*     print("OverlapExit(" + overlap.id + "): Overlap with: " + overlap.collider.getSceneObject().name);
* });
* ```
*/
declare class OverlapExitEventArgs extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Number of objects currently overlapping.
    
    * @readonly
    */
    currentOverlapCount: number
    
    /**
    * Array of all current overlaps.
    
    * @readonly
    */
    currentOverlaps: Overlap[]
    
    /**
    * Structure containing information about the current overlap.
    
    * @readonly
    */
    overlap: Overlap
    
}

/**
* Args used for {@link ColliderComponent.onOverlapStay}, which is triggered every frame while the collider continues overlapping another object. Typically used for volume triggers.

* @example
* ```js
* // While the collider continues to overlap another collider, print information about the overlap

* // @input Physics.ColliderComponent collider

* script.collider.onOverlapStay.add(function (e) {
*     var overlap = e.overlap;
*     print("OverlapStay(" + overlap.id + "): Overlap with: " + overlap.collider.getSceneObject().name);
* });
* ```
*/
declare class OverlapStayEventArgs extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Number of objects currently overlapping.
    
    * @readonly
    */
    currentOverlapCount: number
    
    /**
    * Array of all current overlaps.
    
    * @readonly
    */
    currentOverlaps: Overlap[]
    
    /**
    * Structure containing information about the current overlap.
    
    * @readonly
    */
    overlap: Overlap
    
}

/**
* The arguments of the PalmTapDown event on `GestureModule`

* @wearableOnly
*/
declare class PalmTapDownArgs extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The module's confidence in detecting the gesture.
    
    * @readonly
    
    * @wearableOnly
    */
    confidence: number
    
}

/**
* The arguments of the PalmTapUp event on `GestureModule`

* @wearableOnly
*/
declare class PalmTapUpArgs extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The module's confidence in detecting the gesture.
    
    * @readonly
    
    * @wearableOnly
    */
    confidence: number
    
}

/**
* Controls how a mesh will get rendered. Each Pass acts as an interface for the shader it's associated with.
* Any properties on a Pass's shader will automatically become properties on that Pass.
* For example, if the shader defines a variable named `baseColor`, a script would be able to access that property as `material.mainPass.baseColor`.

* @see Used By: {@link Material#mainPass}, {@link MaterialMeshVisual#mainPass}
* @see Returned By: {@link Material#getPass}

* @example
* ```
* var sprite = script.getSceneObject().getFirstComponent("Component.SpriteVisual");
* var pass = sprite.mainPass;

* pass.baseTex.control.play(-1,0.0);
* ```
*/
declare class Pass extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    [index:string]: any
    
    baseTex: Texture
    
    baseColor: vec4
    
    /**
    * The blend mode used for rendering. Possible values:
    
    * | BlendMode                     | Value | Expression                    |
    * | ----------------------------- | ----- | ----------------------------- |
    * | Normal                        | 0     | SrcAlpha, OneMinusSrcAlpha    |
    * | MultiplyLegacy [DEPRECATED]	| 1	    | DstColor, OneMinusSrcAlpha    |
    * | AddLegacy [DEPRECATED]        | 2     | One, One                      |
    * | Screen	                    | 3	    | One, OneMinusSrcColor         |
    * | PremultipliedAlpha            | 4	    | One, OneMinusSrcAlpha         |
    * | AlphaToCoverage               | 5	    | Blend Disabled                |
    * | Disabled                      | 6	    | Blend Disabled                |
    * | Add                           | 7	    | SrcAlpha, One                 |
    * | AlphaTest                     | 8	    | Blend Disabled                |
    * | ColoredGlass                  | 9	    | Blend Disabled                |
    * | Multiply                      | 10    | DstColor, Zero                |
    * | Min                           | 11    | One, One                      |
    * | Max                           | 12    | One, One                      |
    
    * ```
    * // Sets the blend mode of the main pass for a material to Screen
    * //@input Asset.Material material
    
    * script.material.mainPass.blendMode = 3;
    * ```
    */
    blendMode: BlendMode
    
    /**
    * Controls the masking of color channels with a vec4b representing each channel with a boolean.
    */
    colorMask: vec4b
    
    /**
    * The cull mode used for rendering.
    */
    cullMode: CullMode
    
    /**
    * Enables depth-sorting.
    */
    depthTest: boolean
    
    /**
    * Enables writing pixels to the depth buffer.
    */
    depthWrite: boolean
    
    /**
    * Set max corner of aabb. It only applys when user selects FrustumCullMode.UserDefinedAABB
    */
    frustumCullMax: vec3
    
    /**
    * Set min corner of aabb. It only applys when user selects FrustumCullMode.UserDefinedAABB
    */
    frustumCullMin: vec3
    
    /**
    * Mode for setting frustum culling on Pass
    */
    frustumCullMode: FrustumCullMode
    
    /**
    * Extend render object's aabb to (1 + value). Only applys when user select FrustumCullMode.Extend
    */
    frustumCullPad: number
    
    /**
    * Number of times the pass will be rendered. Useful with the Instance ID node in Material Editor.
    */
    instanceCount: number
    
    /**
    * Line width used for rendering.
    */
    lineWidth: number
    
    /**
    * The name of the Pass.
    */
    name: string
    
    /**
    * Changes the position that each polygon gets drawn.
    */
    polygonOffset: vec2
    
    /**
    * Returns texture samplers used by this pass to set filtering and wrap modes.
    
    * @readonly
    */
    samplers: SamplerWrappers
    
    /**
    * The stencil test state for Pass.
    */
    stencilState: StencilState
    
    /**
    * Whether the material renders on both sides of a mesh face.
    */
    twoSided: boolean
    
    /**
    * Enables writing pixels to the frame.
    
    * @deprecated
    */
    writesColor: boolean
    
}

/**
* Overrides a material's pass property. Used with {@link MaterialMeshVisual}.
*/
declare class PassPropertyOverrides extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    [index:string]: any
    
    baseTex: Texture
    
    baseColor: vec4
    
}

/**
* Similar to {@link Pass}, except used by {@link VFXAsset}.
*/
declare class PassWrapper extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The blend mode used for rendering.
    */
    blendMode: BlendMode
    
    /**
    * Controls the masking of color channels with a vec4b representing each channel with a boolean.
    */
    colorMask: vec4b
    
    /**
    * The cull mode used for rendering.
    */
    cullMode: CullMode
    
    /**
    * Enables depth-sorting.
    */
    depthTest: boolean
    
    /**
    * Enables writing pixels to the depth buffer.
    */
    depthWrite: boolean
    
    /**
    * Number of times the pass will be rendered. Useful with the Instance ID node in Material Editor.
    */
    instanceCount: number
    
    /**
    * Line width used for rendering.
    */
    lineWidth: number
    
    /**
    * The name of the pass wrapper.
    */
    name: string
    
    /**
    * Changes the position that each polygon gets drawn.
    */
    polygonOffset: vec2
    
    /**
    * A proxy class that provides the access to the properties of the passes under the hood of {@link VFXAsset}. PassWrapper.samplers property which is tied to the Pass.samplers one. For example:
    * ```js
    * pass.samplers.texture_name.Fitering = FilteringMode.Nearest
    * ```
    
    * @readonly
    */
    samplers: SamplerWrappers
    
    /**
    * Whether the material renders on both sides of a mesh face.
    */
    twoSided: boolean
    
}

/**
* Allows for retrieval of a collection of Pass objects used by VFXAsset

* @see Used By: {@link VFXAsset#feedbacks}, {@link VFXAsset#outputs}, {@link VFXAsset#simulations}
*/
declare class PassWrappers extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns an array of Pass objects
    
    * @readonly
    */
    allPasses: PassWrapper[]
    
}

/**
* Allows data to be stored and retrieved between Lens sessions.
* In other words, data can be saved on device and loaded back in the next time the Lens is opened.
* Can be accessed with `global.persistentStorageSystem`.

* See the [Persistent Storage guide](https://developers.snap.com/lens-studio/features/persistent-cloud-storage/persistent-storage) for more information.

* @example
* ```js
* // Get the data store from PersistentStorageSystem
* var store = global.persistentStorageSystem.store;

* // Load score from previous Lens session. Defaults to 0 if data isn't found
* var score = store.getFloat("totalScore");
* print("loaded score: " + score);

* function increaseScore() {
*     // Increase score, then write it to the data store
*     score += 1;
*     store.putFloat("totalScore", score);
*     print("new score: " + score);
* }

* // Increase score on tap event
* script.createEvent("TapEvent").bind(increaseScore);
* ```
*/
declare class PersistentStorageSystem extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The `GeneralDataStore` object used to store and retrieve data.
    */
    store: GeneralDataStore
    
}

/**
* The person that a tracker should use. Allows you to to apply the same person index to a group of Components. Useful for correlating tracking between Body Tracking and Face Tracking.

* @see Used By: {@link BodyDepthTextureProvider#trackingScope}, {@link BodyInstanceSegmentationTextureProvider#trackingScope}, {@link BodyNormalsTextureProvider#trackingScope}, {@link BodyRenderObjectProvider#trackingScope}, {@link BodyTrackingAsset#trackingScope}
*/
declare class PersonTrackingScope extends TrackingScope {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The scope that the current PersonTrackingScope be based on.
    */
    parentScope: TextureTrackingScope
    
    /**
    * The person to track. The first person is 0, the second is 1, and so on.
    */
    personIndex: number
    
}

/**
* Namespace containing physics classes and static physics methods.

* @example
* ```js
* // Change gravity in the root world.
* Physics.getRootWorldSettings().gravity = new vec3(0.0, -100.0, 0.0);
* ```
*/
declare class Physics {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Create an intersection probe that spans all worlds.
    */
    static createGlobalProbe(): Probe
    
    /**
    * Create an intersection probe for the implicit root world.
    */
    static createRootProbe(): Probe
    
    /**
    * Get settings that apply to the implicit root world.
    */
    static getRootWorldSettings(): Physics.WorldSettingsAsset
    
}

declare namespace Physics {
    /**
    * Namespace containing static helper methods for {@link Constraint}.
    */
    class Constraint {
        
        /** @hidden */
        protected constructor()
        
        /**
        * Create a constraint of the given type, with default settings.
        */
        static create(type: Physics.ConstraintType): Constraint
        
    }

}

declare namespace Physics {
    /**
    * Constraint type used by a {@link Constraint}.   See also: {@link Constraint}, {@link ConstraintComponent}.
    
    * @see Used By: {@link Constraint#constraintType}, {@link Physics.Constraint.create}
    
    * @example
    * ```js
    * // Given two objects, with hangingBox below staticBox
    * // apply a constraint on hangingBox so that it hangs on the staticBox
    * // Try assigning a box object for each input!
    
    * // @input SceneObject staticBox
    * // @input SceneObject hangingBox
    
    * // Setup the boxes to have physics
    * var staticBox = script.staticBox;
    * var staticBoxBody = staticBox.createComponent("Physics.BodyComponent");
    * staticBoxBody.dynamic = false;
    
    * var hangingBox = script.hangingBox;
    * var hangingBoxBody = hangingBox.createComponent("Physics.BodyComponent");
    
    * // Create a child object that we can set up as our hinge
    * var hingeObj = global.scene.createSceneObject("hinge");
    * hingeObj.setParent(hangingBox);
    
    * // First we use the child object as a constraint
    * var hingeConstraint = hingeObj.createComponent("Physics.ConstraintComponent");
    
    * // Then we set up the constraint as a hinge
    * hingeConstraint.debugDrawEnabled = true;
    * hingeConstraint.constraint = Physics.Constraint.create(Physics.ConstraintType.Hinge);
    
    * // Attach our hinge to another Physics body
    * hingeConstraint.target = staticBoxBody;
    
    * // Position the hinge
    * hingeObj.getTransform().setLocalPosition(new vec3(-7.5, 7.5, 0))
    
    * // Tell the system to recalculate the simulation based on the above parameters
    * hingeConstraint.reanchorTarget();
    * ```
    */
    enum ConstraintType {
        /**
        * Constrains colliders to fixed rotation and position. See {@link FixedConstraint}.
        */
        Fixed,
        /**
        * Constrains colliders to rotate around a single axis. See {@link HingeConstraint}.
        */
        Hinge,
        /**
        * Constrains colliders to rotate around a point. See {@link PointConstraint}.
        */
        Point
    }

}

declare namespace Physics {
    /**
    * Script interface for applying collision filtering to colliders and ray/shape-casts.
    
    * @see Used By: {@link ColliderComponent#filter}, {@link ColliderComponent#overlapFilter}, {@link Physics.WorldSettingsAsset#defaultFilter}, {@link Probe#filter}
    * @see Returned By: {@link Physics.Filter.create}
    
    * @example
    * ```js
    * // Create a new filter and adjust its settings.
    * var filter = Physics.Filter.create();
    * filter.skipLayers = LayerSet.fromNumber(101);
    * filter.onlyColliders = [script.myCollider1, script.myCollider2];
    
    * // Assign the filter as default for the root world so it affects all colliders.
    * var settings = Physics.getRootWorldSettings();
    * settings.defaultFilter = filter;
    
    * // Perform a ray cast using the filter.
    * var probe = Physics.createGlobalProbe();
    * probe.filter = filter;
    * probe.rayCast(new vec3(0, 100, 0), new vec3(0, -100, 0), function (hit) {
    *    print(hit);
    * });
    
    * // Use the filter for collider overlap test.
    * script.myCollider3.overlapFilter = filter;
    * script.myCollider3.onOverlapEnter.add(function (e) {
    *     print("OverlapEnter(" + e.overlap.id + "): " + e.overlap.collider);
    * });
    * ```
    */
    class Filter extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * Include dynamic objects in intersection tests.
        */
        includeDynamic: boolean
        
        /**
        * Include intangible objects in intersection tests.
        */
        includeIntangible: boolean
        
        /**
        * Include static objects in intersection tests.
        */
        includeStatic: boolean
        
        /**
        * If non-empty, only perform collision with these colliders. In other words: the set of colliders to include when performing collision tests, excluding all others. If empty, this setting is disabled (effectively including all colliders, minus skipColliders).
        */
        onlyColliders: ColliderComponent[]
        
        /**
        * If non-empty, only perform collision with colliders in these layers. In other words: the set of layers to include when performing collision tests, excluding all others. If empty, this setting is disabled (effectively including all layers, minus skipLayers).
        */
        onlyLayers: LayerSet
        
        /**
        * Skip collision with these colliders. In other words: the set of colliders to exclude when performing collision tests. This takes precedence over onlyColliders, so a collider that is in both is skipped.
        */
        skipColliders: ColliderComponent[]
        
        /**
        * Skip collision with colliders in these layers. In other words: the set of layers to exclude when performing collision tests. This takes precedence over onlyLayers, so a layer that is in both is skipped.
        */
        skipLayers: LayerSet
        
        /**
        * Create an instance with default settings.
        */
        static create(): Physics.Filter
        
    }

}

declare namespace Physics {
    /**
    * Type of force to use when applying force or torque to a {@link BodyComponent}.
    
    * @see Used By: {@link BodyComponent#addForce}, {@link BodyComponent#addForceAt}, {@link BodyComponent#addRelativeForce}, {@link BodyComponent#addRelativeForceAt}, {@link BodyComponent#addRelativeTorque}, {@link BodyComponent#addTorque}
    
    * @example
    * ```js
    * // @input Physics.BodyComponent bodyComponent
    
    * // Add instant impulse force upward
    * script.bodyComponent.addForce(new vec3(0, 500, 0), Physics.ForceMode.Impulse);
    
    * // Add instant impulse torque to rotate the object
    * script.bodyComponent.addTorque(new vec3(0, 0, Math.PI * -25), Physics.ForceMode.Impulse);
    
    * // Every frame, add upward force over time, relative to the object rotation
    * // This is similar to a rocket boosting the object upward
    * script.createEvent("UpdateEvent").bind(function() {
    *     script.bodyComponent.addRelativeForce(new vec3(0, 2000, 0), Physics.ForceMode.Force);
    * });
    * ```
    */
    enum ForceMode {
        /**
        * Continuous force (kg*cm/s^2), used for cases where force is applied over multiple frames.
        */
        Force,
        /**
        * Continuous acceleration (cm/s^2), applied without respect to mass, used for cases where force is applied over multiple frames.
        */
        Acceleration,
        /**
        * Instantaneous force impulse (kg*cm/s).
        */
        Impulse,
        /**
        * Instantaneous change in velocity (cm/s), applied without respect to mass.
        */
        VelocityChange
    }

}

declare namespace Physics {
    /**
    * Stores reusable settings for a Physics {@link WorldComponent} such as gravity magnitude and direction.
    
    * @see {@link WorldComponent.worldSettings}.
    * @see [World Component](https://developers.snap.com/lens-studio/features/physics/physics-component#physics-world) guide.
    
    * @see Used By: {@link ColliderComponent#worldSettings}, {@link WorldComponent#worldSettings}
    * @see Returned By: {@link Physics.getRootWorldSettings}, {@link Physics.WorldSettingsAsset.create}
    
    * @example
    * ```js
    * // Change gravity in a given World Settings Asset
    
    * // @input Physics.WorldSettingsAsset worldAsset
    * script.worldAsset.gravity = new vec3(0.0, -300.0, 0.0);
    * ```
    
    * ```js
    * // Create world settings.
    * var settings = Physics.WorldSettingsAsset.create();
    * print(settings);
    
    * // Modify layer collision matrix.
    * settings.setLayersCollidable(0, 3, false);
    * settings.setLayersCollidable(200, 20, false);
    * print(settings);
    * print(settings.getLayersCollidable(3, 0));
    * settings.resetLayerCollisionMatrix();
    * print(settings);
    * print(settings.getLayersCollidable(3, 0));
    
    * // Assign world settings to a collider.
    * script.myCollider.worldSettings = settings;
    * ```
    */
    class WorldSettingsAsset extends Asset {
        
        /** @hidden */
        protected constructor()
        
        /**
        * Given 2 layer numbers A and B, returns true if colliders in A collide with colliders in B, and vice-versa. The layer numbers correspond to those used to form a `LayerSet` with `LayerSet.fromNumber()`. The relationship is symmetric, so if collision is disabled for (A, B), it is also disabled for (B, A). This accesses a flag in the "Layer Collision Matrix", as viewable in Studio. Note however that layer numbers are not the same as layer IDs. To get the number of a Studio-created layer, use `LayerSet.numbers`.
        */
        getLayersCollidable(layerNumberA: number, layerNumberB: number): boolean
        
        /**
        * Resets the layer collision matrix such that all layer pairs are enabled.
        */
        resetLayerCollisionMatrix(): void
        
        /**
        * Given 2 layer numbers A and B, enables or disables collision between colliders in those layers. The layer numbers correspond to those used to form a `LayerSet` with `LayerSet.fromNumber()`. The relationship is symmetric, so if collision is disabled for (A, B), it is also disabled for (B, A). This accesses a flag in the "Layer Collision Matrix", as viewable in Studio. Note however that layer numbers are not the same as layer IDs. To get the number of a Studio-created layer, use LayerSet.numbers.
        */
        setLayersCollidable(layerNumberA: number, layerNumberB: number, enable: boolean): void
        
        /**
        * Speed limit, in world space cm/s. Set to 0 to disable this.
        */
        absoluteSpeedLimit: number
        
        /**
        * Default Filter used for colliders in the world.
        */
        defaultFilter: Physics.Filter
        
        /**
        * Default Matter used for colliders in the world. This is used for a collider when its matter field is unset.
        */
        defaultMatter: Matter
        
        /**
        * Gravity acceleration vector (cm/s^2). Defaults to real-world gravity of 980 cm/s^2, downward.
        */
        gravity: vec3
        
        /**
        * Speed limit, relative to shape size. Set to 0 to disable this. The effective world space speed limit is scaled by simulation rate, so this is the maximum distance the object can move in a single step. The default of 0.5 only allows the object to move by half its size in a single step, which should prevent tunneling.
        */
        relativeSpeedLimit: number
        
        /**
        * Simulation rate, in steps per second. Larger values improve simulation accuracy at the cost of performance. This is limited to intervals of 30hz, in the range 30-240hz, with 60hz as the default.
        */
        simulationRate: number
        
        /**
        * Slow down simulation step frequency (higher values are slower). Limited to >=1.0. This achieves a slow-motion effect by reducing the number of simulation steps each frame. Useful for debugging as large values will cause noticeably discrete steps.
        */
        slowDownStep: number
        
        /**
        * Slow down simulation time (higher values are slower). This achieves a slow-motion effect by scaling simulation time. Unlike slowDownStep, it will maintain smooth motion, but has accuracy problems at large scales.
        */
        slowDownTime: number
        
        /**
        * Create an instance with default settings.
        */
        static create(): Physics.WorldSettingsAsset
        
    }

}

/**
* The arguments of the PinchDown event on `GestureModule`

* @wearableOnly
*/
declare class PinchDownArgs extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The orientation of the detected gesture.
    
    * @readonly
    
    * @wearableOnly
    */
    palmOrientation: vec3
    
}

/**
* The arguments of the PinchStrength event on `GestureModule`

* @wearableOnly
*/
declare class PinchStrengthArgs extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The pinch strength of the detected gesture.
    
    * @readonly
    
    * @wearableOnly
    */
    strength: number
    
}

/**
* The arguments of the PinchUp event on `GestureModule`

* @wearableOnly
*/
declare class PinchUpArgs extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The orientation of the detected gesture.
    
    * @readonly
    
    * @wearableOnly
    */
    palmOrientation: vec3
    
}

/**
* Attaches the {@link SceneObject} to the mesh surface of a specific {@link RenderMeshVisual}.

* @see [Pin To Mesh](https://developers.snap.com/lens-studio/lens-studio-workflow/scene-set-up/3d/pin-to-mesh#adding-a-pin-to-mesh-component) guide.
*/
declare class PinToMeshComponent extends Component {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The position offset to apply.
    */
    offsetPosition: vec3
    
    /**
    * The euler angle offset to apply. Only has an effect when `orientation` is set to `PositionAndDirection`.
    */
    offsetRotation: vec3
    
    /**
    * The orientation type to use.
    */
    orientation: PinToMeshComponent.Orientation
    
    /**
    * The UV coordinates on the target mesh to attach to.
    */
    pinUV: vec2
    
    /**
    * @deprecated
    */
    preferedTriangle: number
    
    /**
    * The preferred triangle index to attach to when multiple triangles contain the desired UV coordinate.
    */
    preferredTriangle: number
    
    /**
    * Index of the UV coordinate set to use for pinning.
    */
    preferredUVLayerIndex: number
    
    /**
    * The target mesh to attach to.
    */
    target: BaseMeshVisual
    
    /**
    * If enabled, interpolated vertex normals will be used when calculating the attachment position.
    */
    useInterpolatedVertexNormal: boolean
    
}

declare namespace PinToMeshComponent {
    /**
    * Used with {@link PinToMeshComponent.orientation}.
    
    * @see Used By: {@link PinToMeshComponent#orientation}
    */
    enum Orientation {
        /**
        * Pins only the position. Rotation is independent from the target mesh.
        */
        OnlyPosition,
        /**
        * Pins both the position and direction. The normal of the target mesh is the Y axis.
        * The U texture coordinate of the target mesh's UV is the X axis.
        */
        PositionAndDirection
    }

}

/**


* @see Returned By: {@link PitchShifterBuilder#build}
*/
declare class PitchShifter extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    process(inTensor: Float32Array, inShape: vec3, outTensor: Float32Array): vec3
    
    /**
    * @readonly
    */
    maxTensorSize: number
    
    pitch: number
    
}

/**


* @see Returned By: {@link MachineLearning.createPitchShifterBuilder}, {@link PitchShifterBuilder#setSampleRate}
*/
declare class PitchShifterBuilder extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    build(): PitchShifter
    
    setSampleRate(sampleRate: number): PitchShifterBuilder
    
}

/**
* Used with `AnimationClip` to describe how the clip should be played.

* @see Used By: {@link AnimationClip#playbackMode}
*/
declare enum PlaybackMode {
    /**
    * Plays forward once.
    */
    Single,
    /**
    * Repeated play the clip forward from the beginning to the end.
    */
    Loop,
    /**
    * Plays the clip forward once, then backward once, repeatedly.
    */
    PingPong
}

/**
* Represents 3D locations of stationary features in the environment. The resulting cloud of points provides a sparse description of the 3D environment.

* @see Returned By: {@link DeviceTracking#getPointCloud}

* @example
* ```
* // Enable point clouds
* script.tracking.worldOptions.pointCloudEnabled = true;

* // .....
* // Use point clouds
* var positions = script.tracking.getPointCloud().positions;
* var ids = script.tracking.getPointCloud().ids;
* print("Number of points: " + positions.length);
* ```
*/
declare class PointCloud extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The confidence level of the point cloud.
    
    * @readonly
    
    * @experimental
    */
    confidences: number[]
    
    /**
    * A list of numeric identifiers for each unique 3D point in the cloud.
    
    * @readonly
    */
    ids: number[]
    
    /**
    * A list of vec3 objects representing the 3D position of every point.
    
    * @readonly
    */
    positions: vec3[]
    
}

/**
* A type of constraint that only allows rotation.  See also: {@link ConstraintComponent}.

* @example
* ```js
* // Create a constraint on an object
* var pointConstraint = hingeObj.createComponent("Physics.ConstraintComponent");

* // Set up options on that constraint
* pointConstraint.debugDrawEnabled = true;
* pointConstraint.constraint = Physics.Constraint.create(Physics.ConstraintType.Point);
* ```
*/
declare class PointConstraint extends Constraint {
    
    /** @hidden */
    protected constructor()
    
}

/**
* An Audio Component effect that allows the Lens to simulate sound based on the direction of the Audio Listener relative to the Audio Component.

* @see Used By: {@link SpatialAudio#positionEffect}

* @example
* ```js
* // @input Component.AudioComponent audio
* var spatialAudio = script.audio.spatialAudio;

* var positionEffect = spatialAudio.positionEffect;
* positionEffect.enabled = true;
* ```
*/
declare class PositionEffect extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * If enabled, the position effect will be applied.
    */
    enabled: boolean
    
}

/**
* Uses an input color lookup table image to adjust the coloring of the Lens.

* @see [Color Correction Post Effect](https://developers.snap.com/lens-studio/features/graphics/materials/post-effects#color-correction) guide.

* @example
* ```
* // Sets the post effects texture to a custom texture in script and sets the color to red
* //@input Component.PostEffectVisual postEffect
* //@input Asset.Texture texture

* script.postEffect.mainPass.baseTex = script.texture;
* script.postEffect.mainPass.baseColor = new vec4(1.0,0.0,0.0,1.0);
* ```
*/
declare class PostEffectVisual extends MaterialMeshVisual {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Performs collision tests (such as ray casts) in one or more world.
* @see {@link Physics.createGlobalProbe}
* @see {@link Physics.createRootProbe}
* @see {@link WorldComponent.createProbe}

* @see Returned By: {@link Physics.createGlobalProbe}, {@link Physics.createRootProbe}, {@link WorldComponent#createProbe}

* @example
* ```js
* // Given an object with a BodyComponent
* // If the object intersects with the ray described
* // by rayStart and rayEnd, print a message.

* var rayStart = new vec3(0.0, -100.0, 0.0);
* var rayEnd = new vec3(0.0, 100.0, 0.0);

* // Create a probe to raycast through all worlds.
* var globalProbe = Physics.createGlobalProbe();
* globalProbe.rayCast(rayStart, rayEnd, function (hit) {
*     if (hit) {
*         print("Raycast hit: " + hit.collider.getSceneObject().name);
*     }
* });

* // Create a probe to raycast through only the implicit root world.
* var rootProbe = Physics.createRootProbe();
* rootProbe.rayCast(rayStart, rayEnd, function (hit) {
*     if (hit) {
*         print("Raycast hit all: " + hit.collider.getSceneObject().name);
*     }
* });
* ```

* ```js
* // On Tap, raycast through the scene at the tapped position and print out a message for each object hit.

* // @input Component.Camera camera

* var cameraTransform = script.camera.getTransform();
* var camera = script.camera;

* script.createEvent("TapEvent").bind(function(touchArgs) {
*     var touchPos = touchArgs.getTapPosition();
*     var nearPos = camera.screenSpaceToWorldSpace(touchPos, 0);
*     var farPos = camera.screenSpaceToWorldSpace(touchPos, camera.far - camera.near);
*     Physics.createGlobalProbe().rayCastAll(nearPos, farPos, function(hitResults) {
*         print("Hit " + hitResults.length + " objects:")
*         for(var i=0; i<hitResults.length; i++) {
*             var hit = hitResults[i];
*             print(hit.collider.getSceneObject().name + " at " + hit.position);
*         }
*     });
* });
* ```
*/
declare class Probe extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns (via callback) the nearest intersection in any world. If there is no hit, the callback is called with a null hit argument.  Ray casts are performed after simulation update, which occurs after script Update but prior to LateUpdate.
    */
    rayCast(start: vec3, end: vec3, hitCB: (hit: RayCastHit) => void): void
    
    /**
    * Returns (via callback) all intersections in every world. The callback receives an array of hits, in order of nearest to farthest. If there were no hits, the array length is 0.  Ray casts are performed after simulation update, which occurs after script Update but prior to LateUpdate.
    */
    rayCastAll(start: vec3, end: vec3, hitCB: (hit: RayCastHit[]) => void): void
    
    /**
    * Like rayCast(), but sweeps a sphere from start to end positions. This is shorthand for calling shapeCast() with a SphereShape. Returns (via callback) the nearest intersection. If there is no hit, the callback is called with a null hit argument. Ray casts are performed after simulation update, which occurs after script Update but prior to LateUpdate.
    */
    shapeCast(shape: Shape, start: vec3, startRot: quat, end: vec3, endRot: quat, hitCB: (hit: RayCastHit) => void): void
    
    /**
    * Like rayCastAll(), but sweeps a shape from start to end transforms (expressed as position and rotation).  The provided shape can be created with one of the Shape.create*() functions, or referenced from a Physics.ColliderComponent.  Does not support MeshShape, and will throw an exception if attempted.  Returns (via callback) all intersections.  The callback receives an array of hits, in order of nearest to farthest. If there were no hits, the array length is 0. Ray casts are performed after simulation update, which occurs after script Update but prior to LateUpdate.
    */
    shapeCastAll(shape: Shape, start: vec3, startRot: quat, end: vec3, endRot: quat, hitCB: (hit: RayCastHit[]) => void): void
    
    /**
    * Like rayCast(), but sweeps a sphere from start to end positions.  This is shorthand for calling shapeCast() with a SphereShape.  Returns (via callback) the nearest intersection.  If there is no hit, the callback is called with a null hit argument.  Ray casts are performed after simulation update, which occurs after script Update but prior to LateUpdate.
    */
    sphereCast(radius: number, start: vec3, end: vec3, hitCB: (hit: RayCastHit) => void): void
    
    /**
    * Like rayCastAll(), but sweeps a sphere from start to end positions. This is shorthand for calling shapeCastAll() with a SphereShape. Returns (via callback) all intersections. The callback receives an array of hits, in order of nearest to farthest. If there were no hits, the array length is 0.  Ray casts are performed after simulation update, which occurs after script Update but prior to LateUpdate.
    */
    sphereCastAll(radius: number, start: vec3, end: vec3, hitCB: (hit: RayCastHit[]) => void): void
    
    /**
    * Show intersection tests with debug-draw.
    */
    debugDrawEnabled: boolean
    
    /**
    * Filter settings applied to intersection tests.
    */
    filter: Physics.Filter
    
}

/**
* RenderObjectProvider for mesh objects generated procedurally.
*/
declare class ProceduralMeshRenderObjectProvider extends MeshRenderObjectProvider {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Provides a texture that can be written to or read from. Can be accessed using Texture.control on a Procedural Texture.

* @example
* ```js
* // @input Component.Image image

* var width = 64;
* var height = 64;
* var channels = 4; // RGBA

* var newTex = ProceduralTextureProvider.createWithFormat(width, height, TextureFormat.RGBA8Unorm);
* var newData = new Uint8Array(width * height * channels);

* for (var y=0; y<height; y++) {
*     for (var x=0; x<width; x++) {
*         // Calculate index
*         var index = (y * width + x) * channels;

*         // Set R, G, B, A
*         newData[index] = (x / (width-1)) * 255;
*         newData[index+1] = (y / (height-1)) * 255;
*         newData[index+2] = 0;
*         newData[index+3] = 255;
*     }
* }

* newTex.control.setPixels(0, 0, width, height, newData);
* script.image.mainPass.baseTex = newTex;
* ```
*/
declare class ProceduralTextureProvider extends TextureProvider {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns a Uint8 array containing the pixel values in a region of the texture. The region starts at the pixel coordinates x, y, and extends rightward by width and upward by height. Values returned are integers ranging from 0 to 255.
    
    * @exposesUserData
    */
    getPixels(x: number, y: number, width: number, height: number, data: Uint8Array): void
    
    /**
    * Returns a Float32 array containing the pixel values in a region of the texture. The region starts at the pixel coordinates x, y, and extends rightward by width and upward by height.
    */
    getPixelsFloat32(x: number, y: number, width: number, height: number, data: Float32Array): void
    
    /**
    * Sets a region of pixels on the texture. The region starts at the pixel coordinates x, y, and extends rightward by width and upward by height. Uses the values of the passed in Uint8Array data, which should be integer values ranging from 0 to 255.
    
    * When used with `createWithFormat`, use `TextureFormat.RGBA8Unorm`. This is the corresponding format for Uint8Array.
    */
    setPixels(x: number, y: number, width: number, height: number, data: Uint8Array): void
    
    /**
    * Sets a region of pixels on the texture. The region starts at the pixel coordinates x, y, and extends rightward by width and upward by height. Uses the values of the passed in Float32Array data.
    */
    setPixelsFloat32(x: number, y: number, width: number, height: number, data: Float32Array): void
    
    /**
    * Creates a new, blank Texture Provider using the passed in dimensions and Colorspace. The ProceduralTextureProvider can be accessed through the control property on the returned texture.
    
    * @deprecated
    */
    static create(width: number, height: number, colorspace: Colorspace): Texture
    
    /**
    * Creates a new Procedural Texture based on the passed in texture. The ProceduralTextureProvider can be accessed through the control property on the returned texture.
    */
    static createFromTexture(texture: Texture): Texture
    
    /**
    * Creates a new provider stored with the specified`format`.
    */
    static createWithFormat(width: number, height: number, format: TextureFormat): Texture
    
}

/**
* Declares the precise location tracking permission for your Lens project.

* @see [Permissions Overview](https://developers.snap.com/spectacles/permission-privacy/overview#list-of-permissions-types).
* @see [Location](https://developers.snap.com/spectacles/about-spectacles-features/apis/location) guide for Spectacles.
* @see {@link LocationService}

* @example
* ```js
* require('LensStudio:ProcessedLocationModule');
* ```
*/
declare class ProcessedLocationModule extends Asset {
    
    /** @hidden */
    protected constructor()
    
}

/**


* @see Used By: {@link VFXAsset#properties}
*/
declare class Properties extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Overrides a property. Used with {@link MaterialMeshVisual}.

* @see Used By: {@link MaterialMeshVisual#propertyOverrides}
*/
declare class PropertyOverrides extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Base class for all resource providers. Providers are the implementation for {@link Asset}.

* @remarks
* For example: {@link VideoTextureProvider} is the implementation for a {@link Texture} backed by a video file.

* @example
* In most cases, providers can be accessed via the `control` property of an asset.

* For example, we can jump to the last frame of an animated texture:
* ```js
* // @input Asset.Texture myAnimatedTexture
* const myAnimatedTextureProvider = script.myAnimatedTexture.control;
* const frameCountOfTexture = myAnimatedTextureProvider.getFramesCount();
* myAnimatedTextureProvider.pauseAtFrame(frameCountOfTexture - 1);
* ```

* Or do it in TypeScript:
* ```ts
* @component
* export class ProviderExample extends BaseScriptComponent {
*   @input()
*   myAnimatedTexture: Texture;

*   onAwake() {
*     const myAnimatedTextureProvider = this.myAnimatedTexture.control as AnimatedTextureFileProvider;
*     const frameCountOfTexture = myAnimatedTextureProvider.getFramesCount();
*     myAnimatedTextureProvider.pauseAtFrame(frameCountOfTexture - 1);
*   }
* }
* ```
*/
declare class Provider extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    getLoadStatus(): LoadStatus
    
}

/**
* A quaternion, used to represent rotation.

* @see Used By: {@link DeviceTracking#createTrackedWorldPoint}, {@link GeneralDataStore#putQuat}, {@link GeoLocation.getNorthAlignedHeading}, {@link LookAtComponent#offsetRotation}, {@link mat3.makeFromRotation}, {@link mat4.compose}, {@link mat4.fromRotation}, {@link Probe#shapeCast}, {@link Probe#shapeCast}, {@link Probe#shapeCastAll}, {@link Probe#shapeCastAll}, {@link quat#dot}, {@link quat#equal}, {@link quat#multiply}, {@link quat.angleBetween}, {@link quat.angleBetween}, {@link quat.lerp}, {@link quat.lerp}, {@link quat.slerp}, {@link quat.slerp}, {@link ScreenTransform#rotation}, {@link TensorMath.rotatePoints3d}, {@link TrackedPoint#orientation}, {@link Transform#setLocalRotation}, {@link Transform#setWorldRotation}
* @see Returned By: {@link AnimationCurveTrack#evaluateRotation}, {@link BasicTransform#getRotation}, {@link GeneralDataStore#getQuat}, {@link MotionController#getWorldRotation}, {@link quat#invert}, {@link quat#multiply}, {@link quat.angleAxis}, {@link quat.fromEulerAngles}, {@link quat.fromEulerVec}, {@link quat.fromRotationMat}, {@link quat.fromRotationMat4}, {@link quat.lerp}, {@link quat.lookAt}, {@link quat.quatIdentity}, {@link quat.rotationFromTo}, {@link quat.slerp}, {@link Transform#getLocalRotation}, {@link Transform#getWorldRotation}

* @example
* ```js
* // Rotate a SceneObject around the world Y axis
* //@input SceneObject obj

* // Degrees to rotate by
* var degrees = 90 * getDeltaTime();

* // Convert degrees to radians
* var radians = degrees * (Math.PI / 180);

* // Axis to rotate around
* var axis = vec3.up();

* // Rotation we will apply to the object's current rotation
* var rotationToApply = quat.angleAxis(radians, axis);

* // Get the object's current world rotation
* var oldRotation = script.obj.getTransform().getWorldRotation();

* // Get the new rotation by rotating the old rotation by rotationToApply
* var newRotation = rotationToApply.multiply(oldRotation);

* // Set the object's world rotation to the new rotation
* script.obj.getTransform().setWorldRotation(newRotation);
* ```
*/
declare class quat {
    /**
    * Creates a new quat.
    */
    constructor(w: number, x: number, y: number, z: number)
    
    /**
    * Returns the dot product of the two quats.
    */
    dot(quat: quat): number
    
    /**
    * Returns whether this quat and `b` are equal.
    */
    equal(b: quat): boolean
    
    /**
    * Returns the rotation angle of the quat.
    */
    getAngle(): number
    
    /**
    * Returns the rotation axis of the quat.
    */
    getAxis(): vec3
    
    /**
    * Returns an inverted version of the quat.
    */
    invert(): quat
    
    /**
    * Returns the product of this quat and `b`.
    */
    multiply(b: quat): quat
    
    /**
    * Returns the result of rotating direction vector `vec3` by this quat.
    */
    multiplyVec3(vec3: vec3): vec3
    
    /**
    * Normalizes the quat.
    */
    normalize(): void
    
    /**
    * Returns an euler angle representation of the quat, in radians.
    
    * @deprecated
    */
    toEuler(): vec3
    
    /**
    * Returns an euler angle representation of the quat, in radians.
    */
    toEulerAngles(): vec3
    
    /**
    * Returns a string representation of the quat.
    */
    toString(): string
    
    /**
    * w component of the quat.
    */
    w: number
    
    /**
    * x component of the quat.
    */
    x: number
    
    /**
    * y component of the quat.
    */
    y: number
    
    /**
    * z component of the quat.
    */
    z: number
    
    /**
    * Returns a new quat with angle `angle` and axis `axis`.
    */
    static angleAxis(angle: number, axis: vec3): quat
    
    /**
    * Returns the angle between `a` and `b`.
    */
    static angleBetween(a: quat, b: quat): number
    
    /**
    * Returns a new quat using the euler angles `x`, `y`, `z` (in radians).
    */
    static fromEulerAngles(x: number, y: number, z: number): quat
    
    /**
    * Returns a new quat using the euler angle `eulerVec` (in radians).
    */
    static fromEulerVec(eulerVec: vec3): quat
    
    /**
    * Creates a quaternion from a matrix.
    */
    static fromRotationMat(rotationMat: mat3): quat
    
    /**
    * Creates a quaternion from a {@link mat4}.
    */
    static fromRotationMat4(rotationMat4: mat4): quat
    
    /**
    * Returns a new quat linearly interpolated between `a` and `b`.
    */
    static lerp(a: quat, b: quat, t: number): quat
    
    /**
    * Returns a new quat with a forward vector `forward` and up vector `up`.
    */
    static lookAt(forward: vec3, up: vec3): quat
    
    /**
    * Returns a new quat using the euler angles `x`, `y`, `z` (in radians).
    
    * @deprecated
    */
    static quatFromEuler(x: number, y: number, z: number): quat
    
    /**
    * Returns the identity quaternion.
    */
    static quatIdentity(): quat
    
    /**
    * Returns a rotation quat between direction vectors `from` and `to`.
    */
    static rotationFromTo(from: vec3, to: vec3): quat
    
    /**
    * Returns a new quat spherically linearly interpolated between `a` and `b`.
    */
    static slerp(a: quat, b: quat, t: number): quat
    
}

/**
* The base class for animation tracks using quaternion values.

* @deprecated
*/
declare class QuaternionAnimationTrack extends AnimationTrack {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Represents an animation track using quaternion value keyframes.

* @deprecated
*/
declare class QuaternionAnimationTrackKeyFramed extends QuaternionAnimationTrack {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Adds a key with value `value` at time `time`.
    
    * @deprecated
    */
    addKey(time: number, value: quat): void
    
    /**
    * Removes all keys.
    
    * @deprecated
    */
    removeAllKeys(): void
    
    /**
    * Removes key at index `index`.
    
    * @deprecated
    */
    removeKeyAt(index: number): void
    
}

/**
* Represents a rotation animation track using euler angles.

* @deprecated
*/
declare class QuaternionAnimationTrackXYZEuler extends QuaternionAnimationTrack {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns child track at index `index`.
    
    * @deprecated
    */
    getChildTrackByIndex(index: number): AnimationTrack
    
    /**
    * Sets child track at index `index` to track `track`.
    
    * @deprecated
    */
    setChildTrackByIndex(index: number, track: AnimationTrack): void
    
}

/**
* Declares the coarse location tracking permission for your Lens project.

* @see [Permissions Overview](https://developers.snap.com/spectacles/permission-privacy/overview#list-of-permissions-types).
* @see [Location](https://developers.snap.com/spectacles/about-spectacles-features/apis/location) guide for Spectacles.
* @see {@link LocationService}

* @example
* ```js
* require('LensStudio:RawLocationModule');
* ```
*/
declare class RawLocationModule extends Asset {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Hit results of a ray-cast provided to script. See {@link Probe}.

* @see Used By: {@link Probe#rayCast}, {@link Probe#shapeCast}, {@link Probe#sphereCast}

* @example
* ```js
* // Given an object with a BodyComponent
* // If the object intersects with the ray described
* // by rayStart and rayEnd, print a message.

* var rayStart = new vec3(0.0, -100.0, 0.0);
* var rayEnd = new vec3(0.0, 100.0, 0.0);

* // Create a probe to raycast through all worlds.
* var globalProbe = Physics.createGlobalProbe();
* globalProbe.rayCast(rayStart, rayEnd, function (hit) {
*     if (hit) {
*         print("Raycast hit: " + hit.collider.getSceneObject().name);
*     }
* });

* // Create a probe to raycast through only the implicit root world.
* var rootProbe = Physics.createRootProbe();
* rootProbe.rayCast(rayStart, rayEnd, function (hit) {
*     if (hit) {
*         print("Raycast hit all: " + hit.collider.getSceneObject().name);
*     }
* });
* ```

* ```js
* // On Tap, raycast through the scene at the tapped position and print out a message for each object hit.

* // @input Component.Camera camera

* var cameraTransform = script.camera.getTransform();
* var camera = script.camera;

* script.createEvent("TapEvent").bind(function(touchArgs) {
*     var touchPos = touchArgs.getTapPosition();
*     var nearPos = camera.screenSpaceToWorldSpace(touchPos, 0);
*     var farPos = camera.screenSpaceToWorldSpace(touchPos, camera.far - camera.near);
*     Physics.createGlobalProbe().rayCastAll(nearPos, farPos, function(hitResults) {
*         print("Hit " + hitResults.length + " objects:")
*         for(var i=0; i<hitResults.length; i++) {
*             var hit = hitResults[i];
*             print(hit.collider.getSceneObject().name + " at " + hit.position);
*         }
*     });
* });
* ```
*/
declare class RayCastHit extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The collider that was hit.
    
    * @readonly
    */
    collider: ColliderComponent
    
    /**
    * Distance from the ray origin to the point of intersection.
    
    * @readonly
    */
    distance: number
    
    /**
    * Surface normal on the collider at the point of intersection.
    
    * @readonly
    */
    normal: vec3
    
    /**
    * Position at the point of intersection.
    
    * @readonly
    */
    position: vec3
    
    /**
    * Set to `true` to skip remaining results, if any.
    */
    skipRemaining: boolean
    
    /**
    * Ray interpolant at the point of intersection, in the range [0, 1].
    
    * @readonly
    */
    t: number
    
    /**
    * The hit triangle, if it exists. This is set on intersection with mesh-based colliders, and null otherwise.
    
    * @readonly
    */
    triangle: TriangleHit
    
}

/**
* The options for the realtime store.

* @see Used By: {@link MultiplayerSession#createRealtimeStore}
* @see Returned By: {@link RealtimeStoreCreateOptions.create}

* @example
* ```
* // @input Asset.ConnectedLensModule connectedLensModule

* var options = ConnectedLensSessionOptions.create();
* script.connectedLensModule.createSession(options)

* var initialStore = GeneralDataStore.create();
* initialStore.putInt('state', 1);

* var options = RealtimeStoreCreateOptions.create();
* options.ownership = RealtimeStoreCreateOptions.Ownership.Unowned;
* options.persistence = RealtimeStoreCreateOptions.Persistence.Session;
* options.initialStore = initialStore;

* session.createRealtimeStore(options,
*     function onSuccess(store) {
*       print('Store created! In On connected');

*     },
*     function onError(message) {
*       print('Unable to create a store: ' + message)
*     }
* )
* ```
*/
declare class RealtimeStoreCreateOptions extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    allowOwnershipTakeOver: boolean
    
    /**
    * An existing store to be used as the initial values for the real time store.
    */
    initialStore: GeneralDataStore
    
    /**
    * The ownership model for the realtime store.
    */
    ownership: RealtimeStoreCreateOptions.Ownership
    
    /**
    * The persistence model for the realtime store.
    */
    persistence: RealtimeStoreCreateOptions.Persistence
    
    /**
    * Writes an id string to the store that can be used to identify it later.
    */
    storeId: string
    
    /**
    * Creates the realtime store options object.
    */
    static create(): RealtimeStoreCreateOptions
    
}

declare namespace RealtimeStoreCreateOptions {
    /**
    * The ownership model of a realtime store.
    
    * @see Used By: {@link RealtimeStoreCreateOptions#ownership}
    */
    enum Ownership {
        /**
        * Indicates ownership of an entity. Only the owner can edit a store.
        */
        Owned,
        /**
        * Indicates ownership of an entity. Any user in the session can edit an unowned store.
        */
        Unowned
    }

}

declare namespace RealtimeStoreCreateOptions {
    /**
    * The persistence model for a realtime store.
    
    * @see Used By: {@link ConnectedLensModule.RealtimeStoreCreationInfo#persistence}, {@link RealtimeStoreCreateOptions#persistence}
    */
    enum Persistence {
        /**
        * Indicates that the entity will be deleted after it is broadcast.
        */
        Ephemeral,
        /**
        * Indicates that the entity will be deleted when the owner leaves.
        */
        Owner,
        /**
        * Indicates that the entity will be marked as unowned when the owner leaves. The Entity will be deleted when all clients leave.
        */
        Session,
        /**
        * Indicates that the entity will be marked as unowned when the owner leaves. The Entity will be created as unowned when any client rejoins.
        */
        Persist
    }

}

/**
* An axis aligned rectangle.
* Used by `anchors` and `offsets` in {@link ScreenTransform} to represent screen boundaries.
* Rect can only store finite numbers in the range Number.MIN_VALUE to Number.MAX_VALUE.

* @see Used By: {@link BackgroundSettings#margins}, {@link CameraModule.ImageRequest#crop}, {@link Canvas#worldSpaceRect}, {@link RectCropTextureProvider#cropRect}, {@link ScreenTransform#anchors}, {@link ScreenTransform#offsets}, {@link TensorMath.isInRectangle}, {@link Text#worldSpaceRect}, {@link Text3D#worldSpaceRect}
* @see Returned By: {@link Rect.create}, {@link Text#getBoundingBox}, {@link Text3D#getBoundingBox}

* @example
* ```js
* // @input Component.ScreenTransform screenTransform

* // Move the ScreenTransform's anchor center
* script.screenTransform.anchors.setCenter(new vec2(-0.25, 0.5));

* // Change the ScreenTransform's anchor size
* script.screenTransform.anchors.setSize(new vec2(0.25, 0.25));
* ```
*/
declare class Rect extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns the rectangle's center position as (x, y).
    */
    getCenter(): vec2
    
    /**
    * Returns the size of the rectangle as (width, height).
    */
    getSize(): vec2
    
    /**
    * Sets the rectangle's center position while maintaining its size.
    */
    setCenter(value: vec2): void
    
    /**
    * Sets the rectangle's size while maintaining its center position.
    */
    setSize(value: vec2): void
    
    /**
    * Returns a string representation of the Rect.
    */
    toString(): string
    
    /**
    * The y position of the rectangle's bottom side.
    */
    bottom: number
    
    /**
    * The x position of the rectangle's left side.
    */
    left: number
    
    /**
    * The x position of the rectangle's right side.
    */
    right: number
    
    /**
    * The y position of the rectangle's top side.
    */
    top: number
    
    /**
    * Creates a new Rect with the given properties.
    */
    static create(left: number, right: number, bottom: number, top: number): Rect
    
}

/**
* Positions {@link ScreenTransform} according to a cropped region of a texture provided by {@link CropTextureProvider}.

* @remarks
* Used with Hand Segmentation texture and ML face effects.

* @see [Crop Textures](https://developers.snap.com/lens-studio/features/snap-ml/ml-component/crop-textures) guide.

* @example
* ```js
* //@input Asset.Texture cropTexture
* var setter = script.getSceneObject().createComponent('RectangleSetter');
* setter.cropTexture = script.cropTexture;
* ```
*/
declare class RectangleSetter extends Component {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Cropped texture to match the screen region of. Should be a texture using a RectCropTextureProvider, such as a Screen Crop Texture or Face Crop Texture.
    */
    cropTexture: Texture
    
}

/**
* Texture Provider providing a cropped region of the input texture. The region is specified by the cropRect in local space and rotation.
* Can be accessed using Texture.control on a RectCropTexture asset, such as a Screen Crop Texture.
* For more information, see the [Crop Textures](https://developers.snap.com/lens-studio/features/snap-ml/ml-component/crop-textures) guide.

* @example
* ```js
* // @input Asset.Texture screenCropTexture

* // Zoom in
* script.screenCropTexture.control.cropRect = Rect.create(-.5, .5, -.5, .5);

* // Rotate by 90 degrees
* script.screenCropTexture.control.rotation = Math.PI * .5;

* ```
*/
declare class RectCropTextureProvider extends CropTextureProvider {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The cropped region to draw.
    */
    cropRect: Rect
    
    /**
    * Angle, in radians, the cropped region is rotated by.
    */
    rotation: number
    
}

/**


* @see Used By: {@link RemoteServiceModule#performApiRequest}, {@link RemoteServiceModule#subscribeApiRequest}
* @see Returned By: {@link RemoteApiRequest.create}
*/
declare class RemoteApiRequest extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    body: (Uint8Array|number[]|string)
    
    endpoint: string
    
    parameters: Record<string, string>
    
    static create(): RemoteApiRequest
    
}

/**


* @see Used By: {@link RemoteServiceModule#performApiRequest}, {@link RemoteServiceModule#subscribeApiRequest}
*/
declare class RemoteApiResponse extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Converts the response into a DynamicResource object, which can be used by RemoteMediaModule to load the media content in the response body.
    */
    asResource(): DynamicResource
    
    /**
    * @readonly
    */
    body: string
    
    /**
    * @readonly
    */
    metadata: {[key:string]:string}
    
    /**
    * The integer status code of the response.
    
    * The meaning of possible status code values are defined as follows:
    * - 1: Success. This code corresponds to the 2XX HTTP response status codes.
    * - 2: Redirected. This code corresponds to the 3XX HTTP response status codes.
    * - 3: Bad request. This code corresponds to the 4XX HTTP response status codes other than 401, 403, 404, 408, 413, 414, and 431 which are mapped separately below.
    * - 4: Access denied. This code corresponds to the HTTP response status codes 401 and 403.
    * - 5: Not found. This code corresponds to the HTTP response status code 404. It is also returned when the API spec is not found by the remote API service.
    * - 6: Timeout. This code corresponds to the HTTP response status codes 408 and 504.
    * - 7: Request too large. This code corresponds to the HTTP response status codes 413, 414, and 431.
    * - 8: Server error. This code corresponds to the 5XX HTTP response status codes other than 504 (timeout).
    * - 9: Request cancelled by the caller.
    * - 10: Internal error happened inside the remote API framework (i.e., not from the remote service being called).
    
    * All other values have undefined meaning and should be treated as internal error (code 10).
    
    * @readonly
    */
    statusCode: number
    
    /**
    * @readonly
    */
    uriResources: DynamicResource[]
    
}

/**
* Allows the Lens to download and integrate remote media content such as 3D GLTF assets, images, audio tracks, and video textures into Lenses.

* @example
* ```
* // @input Asset.BitmojiModule bitmojiModule
* // @input Asset.RemoteMediaModule remoteMediaModule
* // @input Asset.Material pbrMaterialHolder

* script.bitmojiModule.requestBitmoji3DResource(
*     function (bitmoji3DResource) {
*       script.remoteMediaModule.loadResourceAsGltfAsset(
*         bitmoji3DResource,
*         onDownloaded,
*         onFail
*       )
*     }
*   )

* function onDownloaded (gltfAsset){
*   var root = scene.createSceneObject("BitmojiAvatar");
*   var avatar = gltfAsset.tryInstantiate(root, script.pbrMaterialHolder);
* }

* function onFail (e){
*     print(e);
* }
* ```
*/
declare class RemoteMediaModule extends Asset {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Downloads the remote image resource from the {@link DynamicResource} object and loads the resource as {@link AudioTrackAsset}.
    */
    loadResourceAsAudioTrackAsset(resource: DynamicResource, onSuccess: (audioTrackAsset: AudioTrackAsset) => void, onFailure: (errorMessage: string) => void): void
    
    /**
    * Downloads the remote file from the Dynamic Resource object and loads the resource as bytes.
    */
    loadResourceAsBytes(resource: DynamicResource, onSuccess: (bytes: Uint8Array) => void, onFailure: (errorMessage: string) => void): void
    
    /**
    * Downloads the remote file from the Dynamic Resource object and loads the resource as {@link GaussianSplattingAsset}.
    */
    loadResourceAsGaussianSplattingAsset(resource: DynamicResource, onSuccess: (gaussianSplattingAsset: GaussianSplattingAsset) => void, onFailure: (errorMessage: string) => void): void
    
    /**
    * Downloads a remote 3D GLTF file given a {@link DynamicResource}.
    */
    loadResourceAsGltfAsset(resource: DynamicResource, onSuccess: (glTFAsset: GltfAsset) => void, onFailure: (errorMessage: string) => void): void
    
    /**
    * Downloads the remote image resource from the {@link DynamicResource} object and loads the resource as an {@link Image} Texture.
    */
    loadResourceAsImageTexture(resource: DynamicResource, onSuccess: (texture: Texture) => void, onFailure: (errorMessage: string) => void): void
    
    /**
    * Downloads the remote file from the Dynamic Resource object and loads the resource as a string.
    */
    loadResourceAsString(resource: DynamicResource, onSuccess: (string: string) => void, onFailure: (errorMessage: string) => void): void
    
    /**
    * Downloads the remote image resource from the {@link DynamicResource} object and loads the resource as a {@link VideoTextureProvider}.
    */
    loadResourceAsVideoTexture(resource: DynamicResource, onSuccess: (texture: Texture) => void, onFailure: (errorMessage: string) => void): void
    
}

/**
* Provides a reference to a remote asset (i.e. assets outside of the Lens size limit) that can be downloaded at runtime using script.

* @see [Remote Assets](https://developers.snap.com/lens-studio/features/lens-cloud/remote-assets-overview)

* @example
* ```js
* //@input Asset.RemoteReferenceAsset myRemoteReferenceAsset
* //@input Component.AudioComponent myAudioComponent

* function onFailed() {
*     print("Asset wasn't downloaded from the reference " + script.myRemoteReferenceAsset.name);
* }

* function previewDownloadedAsset(asset) {
*     if (asset.isOfType("Asset.AudioTrackAsset")) {
*         script.myAudioComponent.audioTrack = asset;
*         script.myAudioComponent.play(1);
*     } else {
*         print("Warning, asset has type " + asset.getTypeName() + ", please set Audio Component input to display");
*     }
* }

* script.myRemoteReferenceAsset.downloadAsset(previewDownloadedAsset, onFailed);

* ```
*/
declare class RemoteReferenceAsset extends Asset {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Request to download the Remote Reference Asset.
    */
    downloadAsset(onDownloaded: (asset: Asset) => void, onFailed: () => void): void
    
}

/**
* A http request which can be sent using the `RemoteServiceModule`.

* @see Used By: {@link InternetModule#performHttpRequest}
* @see Returned By: {@link RemoteServiceHttpRequest.create}

* @wearableOnly

* @CameraKit
*/
declare class RemoteServiceHttpRequest extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Get the header of the http request.
    
    * @wearableOnly
    
    * @CameraKit
    */
    getHeader(name: string): string
    
    /**
    * Set the header of the http request.
    
    * @wearableOnly
    
    * @CameraKit
    */
    setHeader(name: string, value: string): void
    
    /**
    * The body of the http request.
    
    * @wearableOnly
    
    * @CameraKit
    */
    body: (Uint8Array|number[]|string)
    
    /**
    * The content type of the http request.
    
    * @wearableOnly
    
    * @CameraKit
    */
    contentType: string
    
    /**
    * The headers of the http request.
    
    * @wearableOnly
    
    * @CameraKit
    */
    headers: {[key:string]:string}
    
    /**
    * The method which should be used to send this http request.
    
    * @wearableOnly
    
    * @CameraKit
    */
    method: RemoteServiceHttpRequest.HttpRequestMethod
    
    /**
    * The URL which this http request should point to.
    
    * @wearableOnly
    
    * @CameraKit
    */
    url: string
    
    /**
    * Create a new http request.
    
    * @wearableOnly
    
    * @CameraKit
    */
    static create(): RemoteServiceHttpRequest
    
}

declare namespace RemoteServiceHttpRequest {
    /**
    * The http method which should be used to send this http request.
    
    * @see Used By: {@link RemoteServiceHttpRequest#method}
    
    * @wearableOnly
    
    * @CameraKit
    */
    enum HttpRequestMethod {
        /**
        * Get method.
        
        * @wearableOnly
        
        * @CameraKit
        */
        Get,
        /**
        * Post method.
        
        * @wearableOnly
        
        * @CameraKit
        */
        Post,
        /**
        * Put method.
        
        * @wearableOnly
        
        * @CameraKit
        */
        Put,
        /**
        * Delete method.
        
        * @wearableOnly
        
        * @CameraKit
        */
        Delete
    }

}

/**
* The response returned by a `RemoteServiceHttpRequest` call.

* @see Used By: {@link InternetModule#performHttpRequest}

* @wearableOnly

* @CameraKit
*/
declare class RemoteServiceHttpResponse extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Get the result as a `DynamicResource` to be used with `RemoteMediaModule`.
    
    * @wearableOnly
    
    * @CameraKit
    */
    asResource(): DynamicResource
    
    /**
    * Get the header of the response.
    
    * @wearableOnly
    
    * @CameraKit
    */
    getHeader(name: string): string
    
    /**
    * @deprecated
    
    * @wearableOnly
    
    * @CameraKit
    */
    loadAsTexture(): Texture
    
    /**
    * The body of the response.
    
    * @readonly
    
    * @wearableOnly
    
    * @CameraKit
    */
    body: string
    
    /**
    * the content type of the response.
    
    * @readonly
    
    * @wearableOnly
    
    * @CameraKit
    */
    contentType: string
    
    /**
    * The headers of the response.
    
    * @readonly
    
    * @wearableOnly
    
    * @CameraKit
    */
    headers: {[key:string]:string}
    
    /**
    * The http response status code.
    
    * @readonly
    
    * @wearableOnly
    
    * @CameraKit
    */
    statusCode: number
    
}

/**
* Provides access to Snap authorized remote services.
*/
declare class RemoteServiceModule extends Asset {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The createWebAPISocket(endpoint, params) method initiates a WebSocket connection with the specified `endpoint` and the specified `params` to the Snap authorized remote services. Returns a {@link WebSocket} object that can be used to send and receive messages from the server.
    
    * __Syntax__
    
    * ```js
    * createAPIWebSocket(endpoint, params)
    * ```
    
    * - `endpoint` Defines the Snap authorized remote service endpoint to which to establish the WebSocket connection.
    * - `params` Defines the parameters that will be used to establish the connection.
    
    * __Example__
    
    * ```js
    * //@input Asset.RemoteServiceModule remoteServiceModule
    * var remoteServiceModule = script.remoteServiceModule
    
    * // Create WebSocket connection.
    * let socket = script.remoteServiceModule.createAPIWebSocket("real_time", {"api-token" : "token", "api-example" : "realtime=v1", "model": "model"});
    
    * // Listen for the open event
    * socket.onopen = (event) => { print("Socket opened"); };
    
    * // Listen for messages
    * socket.onmessage = async (event) => { print("Socket message"); };
    
    * // Listen for the close event
    * socket.onclose = (event) => { print("Socket closed"); };
    * ```
    
    * @wearableOnly
    */
    createAPIWebSocket(endpoint: string, params: any): WebSocket
    
    /**
    * Deprecated. This method has been moved to {@link InternetModule}.
    
    * @deprecated
    
    * @wearableOnly
    */
    createWebSocket(url: string): WebSocket
    
    /**
    * Deprecated. This method has been moved to {@link InternetModule}.
    
    * @deprecated
    
    * @experimental
    
    * @wearableOnly
    */
    createWebView(options: WebViewOptions, onSuccess: (texture: Texture) => void, onFailure: (errorMessage: string) => void): void
    
    /**
    * Deprecated. This method has been moved to {@link InternetModule}.
    
    * @deprecated
    
    * @wearableOnly
    */
    fetch(request: (Request|string), options?: any): Promise<Response>
    
    /**
    * Get a `DynamicResource` to be used with `RemoteMediaModule` from `mediaUrl`.
    
    * @wearableOnly
    
    * @CameraKit
    */
    makeResourceFromUrl(mediaUrl: string): DynamicResource
    
    performApiRequest(request: RemoteApiRequest, onApiResponse: (response: RemoteApiResponse) => void): void
    
    /**
    * Deprecated. This method has been moved to {@link InternetModule}.
    
    * @deprecated
    
    * @wearableOnly
    
    * @CameraKit
    */
    performHttpRequest(requestOptions: RemoteServiceHttpRequest, onHttpResponse: (response: RemoteServiceHttpResponse) => void): void
    
    subscribeApiRequest(request: RemoteApiRequest, onApiResponse: (response: RemoteApiResponse) => void): string
    
    /**
    * Deprecated. This method has been moved to {@link InternetModule}.
    
    * @deprecated
    
    * @experimental
    
    * @wearableOnly
    */
    static createWebViewOptions(resolution: vec2): WebViewOptions
    
}

/**
* Represents a mesh asset.

* @see {@link RenderMeshVisual}.

* @see Used By: {@link ClothVisual#mesh}, {@link ClothVisual#simulatedMesh}, {@link MeshBuilder.createFromMesh}, {@link MeshShape#mesh}, {@link RenderMeshVisual#mesh}, {@link TrackedPlane#mesh}, {@link VFXAsset#mesh}
* @see Returned By: {@link LocationRenderObjectProvider.create}, {@link MeshBuilder#getMesh}

* @example
* ```js
* //@input Component.RenderMeshVisual meshComponent
* //@input Asset.RenderMesh mesh
* script.meshComponent.mesh = script.mesh;
* ```
*/
declare class RenderMesh extends Asset {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Get the bone inverse matrices on the mesh
    */
    extractBoneInverseMatrices(): mat4[]
    
    /**
    * Get the bone names on the mesh.
    */
    extractBoneNames(): string[]
    
    /**
    * Returns a list of indices of each vertices in the RenderMesh.
    */
    extractIndices(): number[]
    
    /**
    * Returns a list of values of each vertices in the RenderMesh for the specified attribute.
    */
    extractVerticesForAttribute(attributeName: string): number[]
    
    /**
    * Returns the maximum value in each dimension of the axis-aligned bounding box containing this mesh.
    
    * @readonly
    */
    aabbMax: vec3
    
    /**
    * Returns the minimum value in each dimension of the axis-aligned bounding box containing this mesh.
    
    * @readonly
    */
    aabbMin: vec3
    
    /**
    * The RenderObjectProvider for this RenderMesh, which can provide more controls depending on the mesh type.
    * See also: {@link FaceRenderObjectProvider}
    */
    control: RenderObjectProvider
    
    /**
    * The index data type used by this mesh.
    
    * @readonly
    */
    indexType: MeshIndexType
    
    /**
    * The topology type used by this mesh.
    
    * @readonly
    */
    topology: MeshTopology
    
}

/**
* Extends {@link MaterialMeshVisual}, adding the capability to utilize specific {@link RenderMesh} assets to depict 3D models within a scene.

* @remarks
* @see {@link BaseMeshVisual}
* @see {@link MaterialMeshVisual}
*/
declare class RenderMeshVisual extends MaterialMeshVisual {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Resets all blend shape weights on the component.
    */
    clearBlendShapeWeights(): void
    
    /**
    * Returns the names of the blend shapes on this RenderMeshVisual.
    */
    getBlendShapeNames(): string[]
    
    /**
    * Returns the weight of the blend shape with the matching name.
    */
    getBlendShapeWeight(name: string): number
    
    /**
    * Returns whether this component has a weight for the blend shape with matching name.
    */
    hasBlendShapeWeight(name: string): boolean
    
    /**
    * @deprecated
    */
    setBlendShape(value: BlendShapes): void
    
    /**
    * Sets the weight of the blend shape with the matching name.
    */
    setBlendShapeWeight(name: string, weight: number): void
    
    /**
    * Sets the {@link Skin} to use for rendering this mesh.
    */
    setSkin(value: Skin): void
    
    /**
    *  Clears the blend shape with the matching name.
    */
    unsetBlendShapeWeight(name: string): void
    
    /**
    * If enabled, normal directions are also blended by blend shapes.
    */
    blendNormals: boolean
    
    /**
    * The {@link BlendShapes} component used for rendering this mesh.
    
    * @deprecated
    */
    blendShape: BlendShapes
    
    /**
    * If enabled, blend shapes will affect the component.
    */
    blendShapesEnabled: boolean
    
    /**
    * If enabled, rays are generated from this object during Ray Tracing so that it can receive Ray Tracing Reflections.
    */
    emitter: boolean
    
    /**
    * The {@link RenderMesh} asset to render.
    */
    mesh: RenderMesh
    
    /**
    * If enabled, rays can hit this object during Ray Tracing so that it may be visible in Ray Tracing Reflections.
    */
    receiver: boolean
    
}

/**
* Provider for RenderMesh data.

* @see Used By: {@link RenderMesh#control}
*/
declare class RenderObjectProvider extends Provider {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Controls a camera texture resource.
* Can be accessed through {@link Texture.control} on a Camera texture.
* For more information, see the [Camera and Layers](https://developers.snap.com/lens-studio/lens-studio-workflow/scene-set-up/camera) guide.
*/
declare class RenderTargetProvider extends TextureProvider {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The anti-aliasing technique applied on the render target.
    */
    antialiasingMode: RenderTargetProvider.AntialiasingMode
    
    /**
    * The quality of the anti-aliasing technique applied on the render target.
    */
    antialiasingQuality: RenderTargetProvider.AntialiasingQuality
    
    /**
    * When `clearColorEnabled` is true and `inputTexture` is null, this color is used to clear this RenderTarget the first time it is drawn to each frame.
    */
    clearColor: vec4
    
    /**
    * If true, the color on this RenderTarget will be cleared the first time it is drawn to each frame.
    * `inputTexture` will be used to clear it unless it is null, in which case `clearColor` is used instead.
    
    * @deprecated
    */
    clearColorEnabled: boolean
    
    /**
    * Sets the clear color option.
    */
    clearColorOption: ClearColorOption
    
    /**
    * If true, the depth buffer will be cleared on this RenderTarget the first time it is drawn to each frame.
    */
    clearDepthEnabled: boolean
    
    /**
    * When `clearColorEnabled` is true, this texture is used to clear this RenderTarget the first time it is drawn to each frame.
    * If this texture is null, `clearColor` will be used instead.
    */
    inputTexture: Texture
    
    /**
    * How MSAA should be applied on the render target.
    */
    msaaStrategy: RenderTargetProvider.MSAAStrategy
    
    /**
    * When `useScreenResolution` is false, controls the horizontal and vertical resolution of the Render Target.
    */
    resolution: vec2
    
    /**
    * When Use Screen Resolution is enabled, this scales the render target resolution relative to the device resolution.
    */
    resolutionScale: number
    
    /**
    * The texture type of the render target.
    */
    textureType: RenderTargetProvider.TextureType
    
    /**
    * If true, the Render Target's resolution will match the device's screen resolution.
    */
    useScreenResolution: boolean
    
}

declare namespace RenderTargetProvider {
    /**
    * The anti-aliasing technique to use on a render target.
    
    * @see Used By: {@link RenderTargetProvider#antialiasingMode}
    */
    enum AntialiasingMode {
        /**
        * No anti-aliasing technique is applied.
        */
        Disabled,
        /**
        * MSAA anti-aliasing technique is applied.
        */
        MSAA
    }

}

declare namespace RenderTargetProvider {
    /**
    * The fidelity of the anti-aliasing technique to apply.
    
    * @see Used By: {@link RenderTargetProvider#antialiasingQuality}
    */
    enum AntialiasingQuality {
        /**
        * Apply the anti-aliasing technique with low quality.
        */
        Low,
        /**
        * Apply the anti-aliasing technique with medium quality.
        */
        Medium,
        /**
        * Apply the anti-aliasing technique with high quality.
        */
        High,
        /**
        * Apply the anti-aliasing technique with default quality (which corresponds to Ultra).
        */
        Default,
        /**
        * Apply the anti-aliasing technique with ultra quality.
        */
        Ultra
    }

}

declare namespace RenderTargetProvider {
    /**
    * How MSAA should be applied to the render target.
    
    * @see Used By: {@link RenderTargetProvider#msaaStrategy}
    */
    enum MSAAStrategy {
        /**
        * Apply MSAA with the default strategy (always on, without optimization).
        */
        Default,
        /**
        * Apply MSAA only when it's needed.
        */
        OnlyWhenRequired
    }

}

declare namespace RenderTargetProvider {
    /**
    * Specifies what kind of render target it is. Some texture types, for example TextureCubemap, need additional properties set on the rendering camera to work correctly.
    
    * @see Used By: {@link RenderTargetProvider#textureType}
    
    * @example
    * ```js
    * //@input Asset.Texture renderTarget
    
    * // Change the render target from a regular 2d render target to a cubemap
    * if (script.renderTarget.control.textureType == RenderTargetProvider.TextureType.Texture2D) {
    *   script.renderTarget.control.textureType = RenderTargetProvider.TextureType.TextureCubemap;
    * }
    * ```
    */
    enum TextureType {
        /**
        * A 2d texture. Render targets are this type by default.
        */
        Texture2D,
        /**
        * A cubemap render target.
        */
        TextureCubemap
    }

}

/**
* Represents an HTTP request used by the Fetch API in {@link RemoteServiceModule}.

* @wearableOnly

* @example
* ```js
* let request = new Request("https://example.com", {
*     method: "POST",
*     body: '{"foo": "bar"}',
* });

* let url = request.url;
* let method = request.method;
* let body = request.json();
* ```
*/
declare class Request extends ScriptObject {
    /**
    * Construct a new Request. Takes a URL and an optional JSON object with options. Available options are `body`, `method`, `headers`, `redirect`, and `keepalive`.
    
    * ```
    * let request = new Request("https://<Your URL>.com", {
    *     method: "POST",
    *     body: JSON.stringify({ user: { name: "user", career: "developer" }}),
    *     headers: {
    *         "Content-Type": "application/json",
    *     },
    * });
    * ```
    
    * @wearableOnly
    */
    constructor(input: string, options?: any)
    
    /**
    * Retreive the body as `Uint8Array`.
    
    * @wearableOnly
    */
    bytes(): Promise<Uint8Array>
    
    /**
    * Retrieve the body as a json object.
    
    * @wearableOnly
    */
    json(): Promise<any>
    
    /**
    * Retrieve the body as a string.
    
    * @wearableOnly
    */
    text(): Promise<string>
    
    /**
    * True if one of the body retrieval methods has been called for this Request.
    
    * @readonly
    
    * @wearableOnly
    */
    bodyUsed: boolean
    
    /**
    * The {@link Headers} of the Request.
    
    * @readonly
    
    * @wearableOnly
    */
    headers: Headers
    
    /**
    * The HTTP request method. Must be one of these strings: `GET`, `POST`, `PUT`, or `DELETE`. Default is `GET`.
    
    * @readonly
    
    * @wearableOnly
    */
    method: string
    
    /**
    * Indicates how redirects are handled. Can be one the following strings: `follow`, `error`, or `manual`. Default value is `follow`.
    
    * @readonly
    
    * @wearableOnly
    */
    redirect: string
    
    /**
    * The URL of the request.
    
    * @readonly
    
    * @wearableOnly
    */
    url: string
    
}

/**
* Represents an HTTP response used by the Fetch API in {@link RemoteServiceModule}.

* @wearableOnly

* @example
* ```js
* let response = await this.remoteServiceModule.fetch(request);
* if (response.ok) {
*     let contentType = response.headers.get("Content-Type");
*     if (contentType === "application/json") {
*         let responseJson = await response.json();
*         let username = responseJson.json["user"];

*         ...
*     }
* }
* ```
*/
declare class Response extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Retrieve the body as `Uint8Array`.
    
    * @wearableOnly
    */
    bytes(): Promise<Uint8Array>
    
    /**
    * Retrieve the body as a json object.
    
    * @wearableOnly
    */
    json(): Promise<any>
    
    /**
    * Retrieve the body as a string.
    
    * @wearableOnly
    */
    text(): Promise<string>
    
    /**
    * True if one of the body retrieval methods has been called for this Response.
    
    * @readonly
    
    * @wearableOnly
    */
    bodyUsed: boolean
    
    /**
    * The {@link Headers} of the Response.
    
    * @readonly
    
    * @wearableOnly
    */
    headers: Headers
    
    /**
    * True if the response returned HTTP Status Code 200 (OK).
    
    * @readonly
    
    * @wearableOnly
    */
    ok: boolean
    
    /**
    * The response's HTTP status code. HTTP status code values: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status.
    
    * @readonly
    
    * @wearableOnly
    */
    status: number
    
    /**
    * The HTTP status code converted to string.
    
    * @readonly
    
    * @wearableOnly
    */
    statusText: string
    
    /**
    * The URL of the Response. This is the final URL obtained after any redirects.
    
    * @readonly
    
    * @wearableOnly
    */
    url: string
    
}

/**
* Adds subtle retouching effects to detected faces such as soft skin, teeth whitening, etc.

* @see [Retouch](https://developers.snap.com/lens-studio/features/ar-tracking/face/face-retouch) guide.

* @example
* ```
* // Sets a Retouch component's teeth whitening intensity

* //@input Component.RetouchVisual retouchVisual

* script.retouchVisual.teethWhiteningIntensity = 0.4;

* ```
*/
declare class RetouchVisual extends MaterialMeshVisual {
    
    /** @hidden */
    protected constructor()
    
    isAuto(): boolean
    
    /**
    * @readonly
    */
    eyeSharpeningEnabled: boolean
    
    /**
    * @readonly
    */
    eyeWhiteningEnabled: boolean
    
    /**
    * The strength of the eye whitening effect.
    */
    eyeWhiteningIntensity: number
    
    /**
    * The index of the face the effect is being applied to.
    */
    faceIndex: number
    
    /**
    * @deprecated
    */
    lookupTexture: Texture
    
    /**
    * The Texture used to mask the effect.
    
    * @deprecated
    */
    maskTexture: Texture
    
    /**
    * The strength of the eye sharpening effect.
    */
    sharpenEyeIntensity: number
    
    /**
    * @readonly
    */
    softSkinEnabled: boolean
    
    /**
    * The strength of the soft-skin effect.
    */
    softSkinIntensity: number
    
    /**
    * The blur radius of the soft skin effect.
    
    * @deprecated
    */
    softSkinRadius: number
    
    /**
    * @readonly
    */
    teethWhiteningEnabled: boolean
    
    /**
    * The strength of the teeth whitening effect.
    */
    teethWhiteningIntensity: number
    
    /**
    * The tracking context this effect is being applied to.
    */
    trackingScope: (PersonTrackingScope|TextureTrackingScope|FaceTrackingScope)
    
}

/**
* Texture Provider giving the camera texture that is the opposite of {@link CameraTextureProvider}. The provider will have {@link LoadStatus.Loading} until the camera feed is available. On some devices it will never be available. Use {@link DeviceInfoSystem#supportsDualCamera} to check the current device.

* For example, if the `CameraTextureProvider` is providing the rear camera feed, `ReverseCameraTextureProvider` would provide the front camera feed.

* @example
* ```js
* // @input Component.Text displayText

* global.deviceInfoSystem.supportsDualCamera(supportsDualCamera => {
*     if (supportsDualCamera) {
*         script.displayText.text = "Supports dual camera";
*     } else {
*         script.displayText.text = "Does not support dual camera";
*     }
* })
* ```
*/
declare class ReverseCameraTextureProvider extends TextureProvider {
    
    /** @hidden */
    protected constructor()
    
}

/**


* @see Used By: {@link TensorMath.getRotatedRectPoints}
* @see Returned By: {@link RotatedRect.create}, {@link TensorMath.minAreaRect}

* @example
* ```
* var rotatedRect = RotatedRect.create(center, size, angle);
* var rectPoints = new Float32Array(8);
* TensorMath.getRotatedRectPoints(rotatedRect, rectPoints);
* ```
*/
declare class RotatedRect extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns the description of RotatedRect.
    */
    toString(): string
    
    /**
    * The rotation angle in degrees.
    */
    angle: number
    
    /**
    * The center point (mass center) of RotatedRect.
    */
    center: vec2
    
    /**
    * The width and height of RotatedRect.
    */
    size: vec2
    
    /**
    * Creates a RotatedRect object.
    */
    static create(center: vec2, size: vec2, angle: number): RotatedRect
    
}

/**
* Used with {@link DeviceTracking#rotationOptions} to change settings for Rotation tracking mode.

* @see Used By: {@link DeviceTracking#rotationOptions}
*/
declare class RotationOptions extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * If enabled, rotation will be inverted.
    */
    invertRotation: boolean
    
}

/**


* @see Used By: {@link InputBuilder#setSampler}
* @see Returned By: {@link SamplerBuilder#build}
*/
declare class Sampler extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Class for building Sampler.

* @see Returned By: {@link MachineLearning.createSamplerBuilder}, {@link SamplerBuilder#setBorderColor}, {@link SamplerBuilder#setFilteringMode}, {@link SamplerBuilder#setUseMipmaps}, {@link SamplerBuilder#setWrapMode}, {@link SamplerBuilder#setWrapUMode}, {@link SamplerBuilder#setWrapVMode}, {@link SamplerBuilder#setWrapWMode}
*/
declare class SamplerBuilder extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Builds Sampler.
    */
    build(): Sampler
    
    /**
    * Sets border color that will be used for `WrapMode.ClampToBorder` case.
    */
    setBorderColor(borderColor: vec4): SamplerBuilder
    
    /**
    * Sets the filtering mode used for textures sampling.
    */
    setFilteringMode(filteringMode: FilteringMode): SamplerBuilder
    
    /**
    * Sets the flag to use/generate texture mipmaps if they exist/requested.
    */
    setUseMipmaps(value: boolean): SamplerBuilder
    
    /**
    * Sets the same wrap mode for all U, V and W axes.
    */
    setWrapMode(wrapMode: WrapMode): SamplerBuilder
    
    /**
    * Sets the wrap mode U axis.
    */
    setWrapUMode(wrapMode: WrapMode): SamplerBuilder
    
    /**
    * Sets the wrap mode V axis.
    */
    setWrapVMode(wrapMode: WrapMode): SamplerBuilder
    
    /**
    * Sets the wrap mode W axis.
    */
    setWrapWMode(wrapMode: WrapMode): SamplerBuilder
    
}

/**
* An accessor for Pass.samplers when using PassWrappers
*/
declare class SamplerWrapper extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * How the texture will be filtered by the sampler.
    */
    filtering: FilteringMode
    
    /**
    * The texture used by the sampler.
    */
    texture: Texture
    
    /**
    * Whether the texture should wrap.
    */
    wrap: WrapMode
    
    /**
    * Whether the texture should wrap in the x-axis.
    */
    wrapU: WrapMode
    
    /**
    * Whether the texture should wrap in the y-axis.
    */
    wrapV: WrapMode
    
    /**
    * Whether the texture should wrap in the z-axis.
    */
    wrapW: WrapMode
    
}

/**
* A proxy class that provides the access to the properties of the sampler under the hood of the passes contained in the {@link Material} asset and {@link VFXAsset} via either VFXAsset's `PassWrapper.samplers` or Material's `Pass.samplers`. Each property returns a corresponding {@link SamplerWrapper}.

* This class uses dynamic properties, meaning that its properties depend on the referenced material or vfx asset and thus not shown below.

* In the example below, the material that is referenced in the material asset contains the `baseTex` property, which this class then provides access to.

* @see Used By: {@link Pass#samplers}, {@link PassWrapper#samplers}

* @example
* ```js
* // @input Asset.Material myMaterial

* const samplers = script.myMaterial.mainPass.samplers;
* const baseTex = samplers.baseTex;

* baseTex.filtering = FilteringMode.Nearest;

* baseTex.wrapU = WrapMode.ClampToEdge;
* baseTex.wrapV = WrapMode.ClampToEdge;
* baseTex.wrapZ = WrapMode.ClampToEdge;
* baseTex.wrap = WrapMode.ClampToEdge;
* ```
*/
declare class SamplerWrappers extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Provides access to a Scan system that allows users to scan objects, places, and cars with a database of item labels within a Lens.

* @see [Scan Overview](https://developers.snap.com/lens-studio/features/lens-cloud/scan/scan-overview)

* @exposesUserData

* @example
* ```javascript
* //@input Asset.ScanModule scanModule
* //@input Asset.Texture someTexture

* function scanComplete(json)
* {
*    var obj = JSON.parse(json);
*    var annotations = obj["annotations"][ScanModule.Contexts.Objects]["annotations"];
*    if(annotations.length > 0 && annotations[0].confidence > 0.95)
*    {
*        print("Scan found: " + annotations[0].name);
*    }
* }

* function onFailure(reason)
* {
*    print("Scan failure: " + reason);
* }

* //if not filled via GUI
* script.scanModule.targetTexture = script.someTexture;

* script.createEvent("LongPressStartEvent").bind(function() {
*    script.scanModule.scan([ScanModule.Contexts.Objects], scanComplete, onFailure);
* });
* ```
*/
declare class ScanModule extends Asset {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Starts a single Scan call using the provided list of ScanModule.Contexts. On success it will invoke `scanComplete` providing a JSON string. On failure it will invoke `onFailure` with a failure message passed in as an argument.
    
    * @exposesUserData
    */
    scan(contexts: string[], scanComplete: (resultJson: string) => void, scanFailed: (failureMessage: string) => void): void
    
    /**
    * Optional property to pass in a texture for Scan to use.
    
    * @exposesUserData
    */
    scanTarget: Texture
    
}

declare namespace ScanModule {
    /**
    * Contexts used in `ScanModule.scan()`.
    */
    class Contexts {
        
        /** @hidden */
        protected constructor()
        
        /**
        * Category containing cars.
        */
        static Cars: string
        
        /**
        * Category containing dogs.
        
        * @deprecated
        */
        static Dogs: string
        
        /**
        * Category containing objects.
        */
        static Objects: string
        
        /**
        * Category containing places.
        */
        static Places: string
        
    }

}

/**
* The base class for scenewide events. SceneEvents can be created using {@link ScriptComponent}'s {@link ScriptComponent#createEvent} method.

* @see [Script Events](https://developers.snap.com/lens-studio/features/scripting/script-events) guide.

* @see Used By: {@link ScriptComponent#removeEvent}

* @example
* You can bind to events in JavaScript:
* ```js
* // Bind a function to the MouthOpened event
* function onMouthOpen(eventData)
* {
* 	print("mouth was opened");
* }
* var event = script.createEvent("MouthOpenedEvent");
* event.bind(onMouthOpen);
* ```

* ```js
* // Print text 1 second later
* function delayedResponse(eventData)
* {
* 	print("Printed 1 second later.");
* }
* var event = script.createEvent("DelayedCallbackEvent");
* event.bind(delayedResponse);
* event.reset(1);
* ```

* You can also bind to events in TypeScript:
* ```ts
* // Print current elapsed Lens time every frame update.
* @component
* export class NewScript extends BaseScriptComponent {
*   onAwake() {
*     let event = this.createEvent('UpdateEvent');
*     // Bind the function printTime to the event UpdateEvent
*     event.bind(this.printTime.bind(this));
*   }

*   printTime(eventData: UpdateEvent) {
*     // Print the elapsed Lens time
*     print(getTime().toString());
*   }
* }
* ```
*/
declare class SceneEvent extends IEventParameters {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Binds a callback function to this event.
    */
    bind(evCallback: (arg1: this) => void): void
    
    /**
    * Returns the typename of the SceneEvent.
    */
    getTypeName(): string
    
    /**
    * If true, the event is able to trigger. If false, the event will not trigger.
    */
    enabled: boolean
    
}

/**
* An object in the scene hierarchy, containing a {@link Transform} and possibly {@link Component}.
* A script can access the SceneObject holding it through the method `script.getSceneObject()`.

* @see Used By: {@link ClothVisual#setVertexBinding}, {@link Component#sceneObject}, {@link GltfAsset#tryInstantiate}, {@link GltfAsset#tryInstantiateAsync}, {@link GltfAsset#tryInstantiateAsync}, {@link GltfAsset#tryInstantiateWithSetting}, {@link LookAtComponent#target}, {@link ObjectPrefab#instantiate}, {@link ObjectPrefab#instantiateAsync}, {@link ObjectPrefab#instantiateAsync}, {@link ObjectTracking3D#addAttachmentPoint}, {@link ObjectTracking3D#removeAttachmentPoint}, {@link SceneObject#copySceneObject}, {@link SceneObject#copyWholeHierarchy}, {@link SceneObject#setParent}, {@link SceneObject#setParentPreserveWorldTransform}, {@link Skin#setSkinBone}
* @see Returned By: {@link ClothVisual#getVertexBinding}, {@link Component#getSceneObject}, {@link GltfAsset#tryInstantiate}, {@link GltfAsset#tryInstantiateWithSetting}, {@link ObjectPrefab#instantiate}, {@link ObjectTracking3D#createAttachmentPoint}, {@link SceneObject#copySceneObject}, {@link SceneObject#copyWholeHierarchy}, {@link SceneObject#getChild}, {@link SceneObject#getParent}, {@link SceneObjectEvent#getSceneObject}, {@link ScriptScene#createSceneObject}, {@link ScriptScene#getRootObject}, {@link Skin#getSkinBone}, {@link Transform#getSceneObject}

* @example
* ```js
* // Look for a MaterialMeshVisual on this SceneObject
* var sceneObj = script.getSceneObject();
* var meshVisual = sceneObj.getComponent("Component.MaterialMeshVisual");
* if(meshVisual)
* {
* 	// ...
* }
* ```

* ```js
* // Rename each child SceneObject
* var sceneObj = script.getSceneObject();
* for(var i=0; i<sceneObj.getChildrenCount(); i++)
* {
* 	var child = sceneObj.getChild(i);
* 	var newName = i + "_" + child.name;
* 	child.name = newName;
* 	print(child.name);
* }
* ```
*/
declare class SceneObject extends SerializableWithUID {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Copies `component` and adds it to the SceneObject, then returns it.
    */
    copyComponent<K extends Component>(component: K): K
    
    /**
    * Creates a shallow copy of the passed in `sceneObject` (not including its hierarchy), and parents it to this SceneObject.
    */
    copySceneObject(sceneObject: SceneObject): SceneObject
    
    /**
    * Creates a deep copy of the passed in `sceneObject` (including its hierarchy), and parents it to this SceneObject.
    */
    copyWholeHierarchy(sceneObject: SceneObject): SceneObject
    
    /**
    * Adds a new component of type `typeName` to the SceneObject.
    */
    createComponent<K extends keyof ComponentNameMap>(typeName: K): ComponentNameMap[K]
    
    /**
    * Destroys the SceneObject.
    */
    destroy(): void
    
    /**
    * Returns a list of all components attached to the SceneObject.
    
    * @deprecated
    */
    getAllComponents(): Component[]
    
    /**
    * Returns this SceneObject's child at index `index`.
    */
    getChild(index: number): SceneObject
    
    /**
    * Returns the number of children the SceneObject has.
    */
    getChildrenCount(): number
    
    /**
    * Returns the first attached Component with type matching or deriving from `componentType`.
    */
    getComponent<K extends keyof ComponentNameMap>(componentType: K): ComponentNameMap[K]
    
    /**
    * Returns the attached component of type `componentType` at index `index`.  If `componentType` is an empty string, all component types are considered.
    
    * @deprecated
    */
    getComponentByIndex<K extends keyof ComponentNameMap>(componentType: K, index: number): ComponentNameMap[K]
    
    /**
    * Returns the number of components of type `componentType` attached to the SceneObject.  If `componentType` is an empty string, the total number of components attached is returned.
    
    * @deprecated
    */
    getComponentCount<K extends keyof ComponentNameMap>(componentType: K): number
    
    /**
    * Returns a list of attached components with types matching or deriving from `componentType`.
    */
    getComponents<K extends keyof ComponentNameMap>(componentType: K): ComponentNameMap[K][]
    
    /**
    * Returns the first attached component of type `componentType`.  If `componentType` is an empty string, the first component of any type is returned.
    
    * @deprecated
    */
    getFirstComponent<K extends keyof ComponentNameMap>(componentType: K): ComponentNameMap[K]
    
    /**
    * Returns the SceneObject's parent in the hierarchy, or null if there isn't one.
    */
    getParent(): SceneObject
    
    /**
    * Returns the current render layer of the SceneObject.
    
    * @deprecated
    */
    getRenderLayer(): number
    
    /**
    * Returns the Transform attached to the SceneObject.
    */
    getTransform(): Transform
    
    /**
    * Returns whether the SceneObject has a parent in the scene hierarchy.
    */
    hasParent(): boolean
    
    /**
    * Unparents the SceneObject in the hierarchy, making it an orphan.
    */
    removeParent(): void
    
    /**
    * Sets the SceneObject's parent to `newParent` in the scene hierarchy.
    */
    setParent(newParent: SceneObject): void
    
    /**
    * Changes the parent of the SceneObject without altering its world position, rotation, or scale.
    */
    setParentPreserveWorldTransform(newParent: SceneObject): void
    
    /**
    * Sets the render layer of the SceneObject to `id`.
    
    * @deprecated
    */
    setRenderLayer(id: number): void
    
    /**
    * Get an array of the scene object's children.
    
    * @readonly
    */
    children: SceneObject[]
    
    /**
    * Whether the SceneObject, including its components and children, is enabled or disabled.
    */
    enabled: boolean
    
    /**
    * Check if a SceneObject is enabled in the hiearchy. It is enabled if both its own enabled property is `true`, and that of all of its parents to the root of the scene.
    
    * @deprecated
    
    * @readonly
    */
    isEnabledInHiearchy: boolean
    
    /**
    * Returns true if this SceneObject and all of its parents are enabled.
    
    * @readonly
    */
    isEnabledInHierarchy: boolean
    
    /**
    * Gets or sets the LayerSet of layers this SceneObject belongs to.
    * This is used to determine which {@link Camera} will render the SceneObject.
    */
    layer: LayerSet
    
    /**
    * The name of the SceneObject.
    */
    name: string
    
    /**
    * An event that will trigger when a SceneObject goes from enabled in the hiearchy to disabled in the hiearchy.
    
    * @readonly
    */
    onDisabled: event0<void>
    
    /**
    * An event that will trigger when a SceneObject goes from disabled in the hiearchy to enabled in the hiearchy.
    
    * @readonly
    */
    onEnabled: event0<void>
    
}

/**
* Base class for all object-based Event types in Lens Studio (ManipulateStartEvent, TapEvent, etc.).
*/
declare class SceneObjectEvent extends SceneEvent {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns the SceneObject this Event is associated with.
    */
    getSceneObject(): SceneObject
    
}

/**
* Overrides the settings on a local {@link ScreenTransform} to fit specific screen region on the device.

* @see [Screen Transform](https://developers.snap.com/lens-studio/lens-studio-workflow/scene-set-up/2d/screen-transform-overview) guide.

* @example
* ```js
* // @input Component.ScreenRegionComponent screenRegionComponent

* script.screenRegionComponent.region = ScreenRegionType.Capture;
* ```
*/
declare class ScreenRegionComponent extends Component {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The region of the screen the local {@link ScreenTransform} will be fit to.
    */
    region: ScreenRegionType
    
}

/**
* Types of screen regions that can be used with {@link ScreenRegionComponent}.

* @see Used By: {@link ScreenRegionComponent#region}

* @example
* ```js
* // @input Component.ScreenRegionComponent screenRegionComponent

* script.screenRegionComponent.region = ScreenRegionType.Capture;
* ```
*/
declare enum ScreenRegionType {
    /**
    * The entire screen area of the device.
    */
    FullFrame,
    /**
    * The screen area shown to the user when recording. On some devices, this region is a subset of FullFrame region.
    */
    Capture,
    /**
    * The screen area shown to the user when previewing a Snap. On some devices, this region is a subset of Capture region.
    */
    Preview,
    /**
    * A screen area that will be visible on all devices and won't overlap Snapchat UI. Safe area to place any UI elements.
    */
    SafeRender,
    /**
    * The screen area where the round "Snap" button is drawn.
    */
    RoundButton
}

/**
* Texture providing the current Render Target being rendered.
*/
declare class ScreenTextureProvider extends TextureProvider {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Used for positioning objects in 2D screen space. Modifies the position, size, and anchoring of a rectangle relatively to a parent {@link ScreenTransform}.

* @remarks
* It overrides the regular {@link Transform} component on the {@link SceneObject} it's attached to.

* @see [Screen Transform](https://developers.snap.com/lens-studio/lens-studio-workflow/scene-set-up/2d/screen-transform-overview) guide.

* @see Used By: {@link BaseMeshVisual#extentsTarget}

* @example
* ```js
* // @input Component.ScreenTransform screenTransform

* // Move the ScreenTransform's anchor center
* script.screenTransform.anchors.setCenter(new vec2(-0.25, 0.5));

* // Change the ScreenTransform's anchor size
* script.screenTransform.anchors.setSize(new vec2(0.25, 0.25));
* ```

* ```js
* // @input Component.ScreenTransform screenTransform

* // Move the ScreenTransform to match the position of touch events
* function onTouch(eventData) {
* 	var touchPos = eventData.getTouchPosition();
* 	var parentPos = script.screenTransform.screenPointToParentPoint(touchPos);
* 	script.screenTransform.anchors.setCenter(parentPos);
* }

* script.createEvent("TouchStartEvent").bind(onTouch);
* script.createEvent("TouchMoveEvent").bind(onTouch);
* ```

* ```js
* // @input Component.ScreenTransform screenTransform

* // Check if a touch starts inside the ScreenTransform
* script.createEvent("TouchStartEvent").bind(function(eventData) {
* 	var touchPos = eventData.getTouchPosition();
* 	if (script.screenTransform.containsScreenPoint(touchPos)) {
* 		print("contains screen point!");
* 	}
* });
* ```

* ```js
* // @input Component.ScreenTransform screenTransform
* // @input Component.ScreenTransform other

* // Move the ScreenTransform to match the world position of another ScreenTransform
* var worldPos = script.other.localPointToWorldPoint(new vec2(0, 0));
* var parentPos = script.screenTransform.worldPointToParentPoint(worldPos);
* script.screenTransform.anchors.setCenter(parentPos);
* ```
*/
declare class ScreenTransform extends Component {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns true if the local point is within the boundaries of this ScreenTransform--that is: its position is within `-1` and `1` in both the x and y coordinates.
    */
    containsLocalPoint(localPoint: vec2): boolean
    
    /**
    * Returns true if the screen position is within the boundaries of this ScreenTransform.
    * Useful for checking if a touch event overlaps with this object.
    * This function will calculate the ScreenPoint by heuristically looking for a camera: first checking for a camera in it's parent's hierarchy, then looking for a camera with the same render layer, and finally just choosing the first camera in the scene.
    */
    containsScreenPoint(screenPoint: vec2): boolean
    
    /**
    * Returns true if the world position is within the boundaries of this ScreenTransform.
    * The `z` value of the world position is ignored.
    */
    containsWorldPoint(worldPoint: vec3): boolean
    
    /**
    * Returns true if this ScreenTransform is in a valid screen hierarchy, which is required for anchoring to work.
    * To be in a valid screen hierarchy there must be a {@link Camera} component upward in the parent hierarchy, and every
    * object between the Camera and this one must also have a ScreenTransform.
    */
    isInScreenHierarchy(): boolean
    
    /**
    * Converts from a normalized (-1 to 1) position within this ScreenTransform's bounds to a screen position. This function will calculate the ScreenPoint by heuristically looking for a camera: first checking for a camera in it's parent's hierarchy, then looking for a camera with the same render layer, and finally just choosing the first camera in the scene.
    */
    localPointToScreenPoint(relativeLocalPoint: vec2): vec2
    
    /**
    * Converts from a normalized (-1 to 1) position within this ScreenTransform's bounds to a world position.
    */
    localPointToWorldPoint(relativeLocalPoint: vec2): vec3
    
    /**
    * Converts from a screen position to a normalized (-1 to 1) position within this ScreenTransform's bounds.
    */
    screenPointToLocalPoint(screenPoint: vec2): vec2 | undefined
    
    /**
    * Converts from a screen position to a normalized (-1 to 1) position within the parent object's bounds.
    * This value is useful because it can be used directly for this ScreenTransform's anchor positioning.
    */
    screenPointToParentPoint(screenPoint: vec2): vec2 | undefined
    
    /**
    * Converts from a world position to a normalized (-1 to 1) position within this ScreenTransform's bounds.
    */
    worldPointToLocalPoint(worldPoint: vec3): vec2
    
    /**
    * Converts from a world position to a normalized (-1 to 1) position within the parent object's bounds.
    * This value is useful because it can be used directly for this ScreenTransform's anchor positioning.
    */
    worldPointToParentPoint(worldPoint: vec3): vec2
    
    /**
    * The anchor rect positioning this ScreenTransform proportional to its parent's bounds.
    * For each field, a value of `0` equals the parent's center point, and value of `-1` or `1` (depending on the side) equals the parent's full boundary.
    
    * For example, a `top` value of `1.0` means this ScreenTransform's top edge will be exactly at the top edge of its parent.
    
    * A `bottom` value of `-0.5` means this ScreenTransform's bottom edge will be halfway between its parent's bottom edge and center.
    
    * A `right` value of `0` means this ScreenTransform's right edge will be exactly at its parent's center.
    
    * A `left` value of `-2` means this ScreenTransform's left edge will be twice as far from its parent's center as its parent's left edge is.
    */
    anchors: Rect
    
    /**
    * Display the debug view of the screen transform.
    */
    enableDebugRendering: boolean
    
    /**
    * This rect is applied after `anchors` to determine the final boundaries of the ScreenTransform.
    * It adds an offset in world units (based on the parent {@link Camera}'s to each edge of the ScreenTransform's boundaries.
    
    * For example, a value of `0` for any side will have no effect on boundaries.
    
    *  A value of `1.0` for any side will offset that edge by `1.0` world unit.
    */
    offsets: Rect
    
    /**
    * Normalized (x, y) position of the center point used in rotation. (-1, -1) being bottom left, (0, 0) being center, and (1, 1) being top right of the image.
    */
    pivot: vec2
    
    /**
    * Basic local position in world units relative to the parent's center. Useful for animating screen elements with a simple offset.
    */
    position: vec3
    
    /**
    * Basic local rotation applied to the SceneObject.
    */
    rotation: quat
    
    /**
    * Basic local scaling applied to the SceneObject.
    */
    scale: vec3
    
}

/**
* Represents a JavaScript or TypeScript script that can be used to add logic in your Lens.
*/
declare class ScriptAsset extends Asset {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Used to bind custom JavaScript or TypeScript code to specific Lens events for dynamic interactivity.

* @remarks These Script Components are attached to SceneObjects, providing the ability to modify properties and behaviors of those objects or others within the scene. Script Components expose input fields in the Inspector panel, allowing for customization of script behavior without altering code. Any script can access the ScriptComponent executing them through the variable `script`.

* @remarks
* @see [Scripting Overview](https://developers.snap.com/lens-studio/essential-skills/scripting/script-overview).
* @see [Script Events Guide](https://developers.snap.com/lens-studio/features/scripting/script-events).

* @example
* ```js
* // Bind a function to the MouthOpened event
* function onMouthOpen(eventData)
* {
*     print("mouth was opened");
* }
* var event = script.createEvent("MouthOpenedEvent");
* event.bind(onMouthOpen);
* ```
*/
declare class ScriptComponent extends Component {
    
    /** @hidden */
    protected constructor()
    
    [index:string]: any
    
    /**
    * Adds a new SceneEvent, triggered by `eventType` events, to the ScriptComponent.
    */
    createEvent<K extends keyof EventNameMap>(eventType: K): EventNameMap[K]
    
    /**
    * Removes a previously added SceneEvent from the ScriptComponent.
    */
    removeEvent(event: SceneEvent): void
    
    /**
    * Generic object accessible by other instances of ScriptComponent. Use this object to store references to properties and methods that need to be accessible from other ScriptComponents.
    
    * @deprecated
    
    * @readonly
    */
    api: Record<string, any>
    
}

/**
* Base class for objects representing Script data.

* @see Used By: {@link ScriptObject#isSame}

* @example
* Unassigned
*/
declare class ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns the name of this object's type.
    */
    getTypeName(): string
    
    /**
    * Returns true if the object matches or derives from the passed in type.
    */
    isOfType(type: string): boolean
    
    /**
    * Returns true if this object is the same as `other`. Useful for checking if two references point to the same thing.
    */
    isSame(other: ScriptObject): boolean
    
}

/**
* Represents the Lens scene. Accessible through `global.scene`.

* @example
* ```js
* if(global.scene.getCameraType() == "front")
* {
*     print("the front camera is active");
* }
* else if(global.scene.getCameraType() == "back")
* {
*     print("the back camera is active");
* }
* ```

* ```js
* var newObject = global.scene.createSceneObject("newObject");
* ```
*/
declare class ScriptScene extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Create a texture containing the DepthStencilRenderTargetProvider.
    */
    createDepthStencilRenderTargetTexture(): Texture
    
    /**
    * Creates a new Render Target texture with a {@link RenderTargetProvider} as its `control` property.
    */
    createRenderTargetTexture(): Texture
    
    /**
    * Adds a new {@link SceneObject} named `name` to the scene.
    */
    createSceneObject(name: string): SceneObject
    
    /**
    * Returns a string describing the currently active device camera.
    
    * Returns "back" if the rear-facing (aka World) camera is active.
    
    * Returns "front" if the front-facing (aka Selfie) camera is active.
    
    * Otherwise, the camera is not initialized.
    */
    getCameraType(): string
    
    /**
    * Returns the root {@link SceneObject} at index `index` in the current scene.
    */
    getRootObject(index: number): SceneObject
    
    /**
    * Returns the number of {@link SceneObject} in the current scene.
    */
    getRootObjectsCount(): number
    
    /**
    * Returns whether or not the scene is currently being recorded.
    */
    isRecording(): boolean
    
    /**
    * The output Render Target of the actual recorded video.
    */
    captureTarget: Texture
    
    /**
    * Returns true if the device supports Ray Tracing and Advanced Graphics Features is enabled in the project settings.
    
    * @readonly
    */
    isRayTracingSupported: boolean
    
    /**
    * Similar to `liveTarget`, but this RenderTarget will not have predictive motion adjustments applied to it (only applicable on supported devices). [Learn more](https://developers.snap.com/lens-studio/lens-studio-workflow/scene-set-up/camera#overlay-target)
    */
    liveOverlayTarget: Texture
    
    /**
    * The output Render Target that users will see in the live camera and during recording.
    */
    liveTarget: Texture
    
}

/**
* Segmentation model used for {@link SegmentationTextureProvider}.
*/
declare class SegmentationModel extends BinAsset {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Controls a segmentation texture resource.
* Can be accessed through {@link Texture.control} on a Segmentation texture.
* For more information, see the [Segmentation](https://developers.snap.com/lens-studio/features/ar-tracking/body/segmentation/fullscreen-segmentation) guide.
*/
declare class SegmentationTextureProvider extends TextureProvider {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Get the percentage of screen covered by the current segmentation mask, from 0 to 1.
    
    * @deprecated
    */
    getMaskPercentage(): number
    
    /**
    * Triggered when the area segmented changes.
    
    * @readonly
    */
    onMaskPercentageUpdated: event1<number, void>
    
}

/**
* Arguments used with the `InteractionComponent.onSelectEnd` event.

* @example
* ```js
* //@input Component.InteractionComponent interactionComponent

* // Subscribe to the selectEndEvent event
* var selectEndEvent = script.interactionComponent.onSelectEnd.add(function(selectEndEventArgs){
*     print("Select End!");
* });

* // Unsubscribe from the selectEndEvent event
* script.interactionComponent.onSelectEnd.remove(selectEndEvent);
* ```
*/
declare class SelectEndEventArgs extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Arguments used with the `InteractionComponent.onSelectStart` event.

* @example
* ```js
* //@input Component.InteractionComponent interactionComponent

* // Subscribe to the selectStartEvent event
* var selectStartEvent = script.interactionComponent.onSelectStart.add(function(selectStartEventArgs){
*     print("Select Start!");
* });

* // Unsubscribe from the selectStartEvent event
* script.interactionComponent.onSelectStart.remove(selectStartEvent);
* ```
*/
declare class SelectStartEventArgs extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Low-level data class.

* @example
* ```js
* // @input Component.MeshVisual visualA
* // @input Component.MeshVisual visualB

* // Check if two MeshVisuals are using the same texture
* var textureA = script.visualA.mainPass.baseTex;
* var textureB = script.visualB.mainPass.baseTex;

* if(textureA.isSame(textureB)) {
*     print("The textures match");
* }

* ```
*/
declare class SerializableWithUID extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * @readonly
    */
    uniqueIdentifier: string
    
}

/**
* Base type for collision shapes.

* @see Used By: {@link ColliderComponent#shape}, {@link Probe#shapeCast}, {@link Probe#shapeCastAll}
*/
declare class Shape extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Create a BoxShape with default settings.
    */
    static createBoxShape(): BoxShape
    
    /**
    * Create a CapsuleShape with default settings.
    */
    static createCapsuleShape(): CapsuleShape
    
    /**
    * Create a ConeShape with default settings.
    */
    static createConeShape(): ConeShape
    
    /**
    * Create a CylinderShape with default settings.
    */
    static createCylinderShape(): CylinderShape
    
    /**
    * Create a LevelsetShape with default settings.
    */
    static createLevelsetShape(): LevelsetShape
    
    /**
    * Creates a new MeshShape.
    */
    static createMeshShape(): MeshShape
    
    /**
    * Create a SphereShape with default settings.
    */
    static createSphereShape(): SphereShape
    
}

/**
* Allows the creation of Shopping Lenses with an integrated Product Catalog.

* @remarks
* The ShoppingModule includes several input fields for you to define as you create your Shopping Lens. The fields include:

**Domain:** name of the product line (e.g., Running Shoes).
**Description:** (of the domain): description of the domain (e.g., Winter Season Collection).
**State(s):** name of the single product displayed in that state (e.g., Shoe ABC).
**Description** (of each state): description of the product in the state (e.g., SKU ID 12345, red shoe).

* @see [Shopping Lens](https://developers.snap.com/lens-studio/sponsored/sponsored-lens-templates/shopping/surface-objects) guide.
*/
declare class ShoppingModule extends Asset {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns the status of whether the module has loaded.
    */
    loadingStarted(): any
    
    selectProduct(index: number): void
    
    /**
    * The domain within this shopping module.
    
    * @readonly
    */
    domains: DomainInfo[]
    
    /**
    * Triggered when an error has occured on the module.
    
    * @readonly
    */
    onError: event2<number, string, void>
    
    /**
    * Triggered when the client (e.g. Snapchat) has changed the product state.
    
    * @readonly
    */
    onProductStateUpdate: event1<string, void>
    
    /**
    * @readonly
    */
    onProductsLoaded: event1<string, void>
    
}

/**
* Represents skinning data for rigged meshes. See also: {@link MeshVisual}.

* @see Used By: {@link MeshShape#skin}, {@link RenderMeshVisual#setSkin}
*/
declare class Skin extends Component {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Remove all bones on the skin.
    */
    clearBones(): void
    
    /**
    * Get the bone on the skin.
    */
    getSkinBone(boneName: string): SceneObject
    
    /**
    * Get all the bone names on the skin.
    */
    getSkinBoneNames(): string[]
    
    /**
    * Associate the Scene Object `bone` with `boneName`
    */
    setSkinBone(boneName: string, bone: SceneObject): void
    
}

/**
* Triggered when a smile ends on the tracked face.

* @example
* ```js
* var event = script.createEvent("SmileFinishedEvent");
* event.faceIndex = 0;
* event.bind(function (eventData)
* {
* 	print("Smile ended on face 0");
* });
* ```
*/
declare class SmileFinishedEvent extends FaceTrackingEvent {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Triggered when a smile begins on the tracked face.

* @example
* ```js
* var event = script.createEvent("SmileStartedEvent");
* event.faceIndex = 0;
* event.bind(function (eventData)
* {
* 	print("Smile started on face 0");
* });
* ```
*/
declare class SmileStartedEvent extends FaceTrackingEvent {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Details about friendship between the active user and another user.

* @see Used By: {@link SnapchatUser#friendInfo}

* @example
* ```js
* async function getFriends() {
*     return new Promise(function (resolve, reject) {
*         global.userContextSystem.getAllFriends(function (friends) {
*             resolve(friends);
*         });
*     });
* }

* async function init(){
*     let friends = await getFriends();
*     friends.forEach(f => {
*         let friendshipStartDate = f.friendInfo.friendshipStart;
*         let lastInteractionTime = f.friendInfo.lastInteractionTime;
*         print(`Friendship with ${f.displayName} started on ${friendshipStartDate}, last interacted on ${lastInteractionTime}`)
*     });
* }

* init();
* ```
*/
declare class SnapchatFriendUserInfo {
    
    /** @hidden */
    protected constructor()
    
    /**
    * When this friendship started.
    
    * @readonly
    */
    friendshipStart?: Date
    
    /**
    * Last interaction between this user and the active one.
    
    * @readonly
    */
    lastInteractionTime?: Date
    
}

/**
* Represents a Snapchat user in order to pass to other APIs or to retrieve certain details about the user like display name.

* @see Used By: {@link Bitmoji2DOptions#user}, {@link Bitmoji3DOptions#user}, {@link Leaderboard.UserRecord#snapchatUser}, {@link SnapData.addUserMention}, {@link UserContextSystem#getCurrentUser}, {@link UserContextSystem#getMyAIUser}, {@link UserContextSystem#loadResourceAsSnapchatUser}
*/
declare class SnapchatUser extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * User's birth month and day. The month and day will start with the index of 1.
    
    * @readonly
    */
    birthday?: SnapchatUserBirthday
    
    /**
    * Name a Snapchat user has chosen to be shown as. Note that not all users set their display name, so you should treat this value as such.
    
    * @readonly
    */
    displayName?: string
    
    /**
    * Information about friendship between this friend and the active user like start date and streak.
    
    * @readonly
    */
    friendInfo?: SnapchatFriendUserInfo
    
    /**
    * Whether this user has a bitmoji that can be retireved via BitmojiModule.
    
    * @readonly
    */
    hasBitmoji: boolean
    
    /**
    * Snapchat user's unique userName. Should not be relied on as a key because it can be changed.
    
    * @readonly
    */
    userName: string
    
    /**
    * This user's zodiac.
    
    * @readonly
    */
    zodiac?: Zodiac
    
}

/**
* Set the bone on the skin.

* @see Used By: {@link SnapchatUser#birthday}
*/
declare class SnapchatUserBirthday {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The day of the month (1-31).
    
    * @readonly
    */
    day: number
    
    /**
    * The month of the year (1-12).
    
    * @readonly
    */
    month: number
    
}

/**
* Provides a marker for tracking Snapcodes.
* For more information, see the [Marker Tracking](https://developers.snap.com/lens-studio/features/ar-tracking/world/marker-tracking) guide.
*/
declare class SnapcodeMarkerProvider extends MarkerProvider {
    
    /** @hidden */
    protected constructor()
    
}

/**
* A handle to add automatic tags like mention stickers to the Snap.
*/
declare class SnapData {
    
    /** @hidden */
    protected constructor()
    
    /**
    * If a Snap is taken, the passed in user will be included in a mention sticker. Pass in `null` to remove any previously assigned users. 
    */
    static addUserMention(user: SnapchatUser): void
    
}

/**
* Called when the user taps on the capture button to record an image.

* @example
* ```js
* // @input SceneObject objectToShowOnCaptured
* script.createEvent("SnapImageCaptureEvent").bind(function() {
*     script.objectToShowOnCaptured.enabled = true;
* });
* ```
*/
declare class SnapImageCaptureEvent extends SceneEvent {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Gets called when the user starts long pressing the capture button to record a Snap.

* @example
* ```js
* // @input SceneObject objectToShowOnRecord
* script.createEvent("SnapRecordStartEvent").bind(function() {
*     script.objectToShowOnRecord.enabled = true;
* });
* ```
*/
declare class SnapRecordStartEvent extends SceneEvent {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Gets called when the user stops long pressing the Snap button to end recording of a Snap.

* @example
* ```js
* // @input SceneObject objectToShowOnRecordStop
* script.createEvent("SnapRecordStopEvent").bind(function() {
*     script.objectToShowOnRecordStop.enabled = true;
* });
* ```
*/
declare class SnapRecordStopEvent extends SceneEvent {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Order that inTensor will be sorted when applying `TensorMath.argSortMasked()`.

* @see Used By: {@link TensorMath.argSortMasked}

* @example
* ```js
* var size = TensorMath.argSortMasked(inTensor, mask, outTensor, order);
* print(outTensor.subarray(0, size));
* ```

* ```js
* var inTensor = [5.0, 2.0, 1.0, 3.0, 7.0, -1.0, -5.0];
* var mask = [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0];
* var outTensor = new Uint32Array(7);
* var order = SortOrder.Ascending;
* var size = TensorMath.argSortMasked(inTensor, mask, outTensor, order);

* print(outTensor.subarray(0, size)); // Expected: [2, 1, 3, 0, 4]
* ```

* ```js
* var inTensor = [5.0, 2.0, 1.0, 3.0, 7.0, -1.0, -5.0];
* var mask = [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0];
* var outTensor = new Uint32Array(7);
* var order = SortOrder.Descending;
* var size = TensorMath.argSortMasked(inTensor, mask, outTensor, order);

* print(outTensor.subarray(0, size)); // Expected: [4, 0, 3, 1, 2]
* ```
*/
declare enum SortOrder {
    /**
    * Applies ascending sorting order when returning indices in TensorMath.argSortMasked().
    */
    Ascending,
    Descending
}

/**
* Represents beat synchronization data for a music track.

* @see Used By: {@link SoundSyncTracker#fullSoundSync}
*/
declare class SoundSync extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Timestamps of all beats in the music track in milliseconds.
    
    * @readonly
    */
    allBeatsTimestampMS: number[]
    
    /**
    * The interval between consecutive beats in milliseconds.
    
    * @readonly
    */
    beatPeriodMS: number
    
    /**
    * Tempo of the music track expressed as beats per minute (BPM).
    
    * @readonly
    */
    beatsPerMinute: number
    
    /**
    * Timestamps of downbeats in milliseconds.
    
    * @readonly
    */
    downbeatsTimestampsMS: number[]
    
    /**
    * Number of beats in each musical measure of this music track.
    
    * @readonly
    */
    numBeatsInMeasure: number
    
    /**
    * Total duration of the track in seconds.
    
    * @readonly
    */
    trackDurationSeconds: number
    
}

/**
* Represents a single musical beat used to synchronize effects with music.
*/
declare class SoundSyncBeat extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The position of this beat within its measure, starting with 1.
    
    * @readonly
    */
    beatIndex: number
    
    /**
    * The time in seconds when this beat occurs in the music track.
    
    * @readonly
    */
    timestamp: number
    
}

/**
* Tracks music beats and provides events for beat occurrences and playback state changes.

* @see Returned By: {@link ExternalMusicModule#getSoundSyncTracker}
*/
declare class SoundSyncTracker extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The complete beat synchronization data for the current music track.
    
    * @readonly
    */
    fullSoundSync: SoundSync
    
    /**
    * Indicates whether music is currently playing.
    
    * @readonly
    */
    isPlaying: boolean
    
    /**
    * Event triggered when any beat occurs in the music.
    
    * @readonly
    */
    onBeat: event1<SoundSyncBeat, void>
    
    /**
    * Event triggered when a down beat (first beat within a measure) occurs in the music. 
    
    * @readonly
    */
    onDownBeat: event1<SoundSyncBeat, void>
    
    /**
    * Event triggered when music playback is stopped or reset.
    
    * @readonly
    */
    onPlaybackReset: event0<void>
    
    /**
    * Event triggered when music playback begins, providing information about the track.
    
    * @readonly
    */
    onPlaybackStarted: event1<ExternalMusicInfo, void>
    
    /**
    * Event triggered when beat synchronization data becomes available for the track.
    
    * @readonly
    */
    onSoundSyncAvailable: event0<void>
    
    /**
    * Event triggered when beat synchronization data is cleared, such as when changing tracks.
    
    * @readonly
    */
    onSoundSyncCleared: event0<void>
    
    /**
    * Current playback position (in seconds) of the tracked music.
    
    * @readonly
    */
    playbackPosition: number
    
}

/**
* Provides access to the SourceMaps utilities that maps TypeScript to JavaScript.
*/
declare class SourceMaps {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Applies the SourceMaps to the stack trace to map JavaScript to TypeScript.
    */
    static applyToStackTrace(trace: string): string
    
}

/**


* @see Used By: {@link AudioComponent#spatialAudio}
*/
declare class SpatialAudio extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Directivity effect settings.
    
    * @readonly
    */
    directivityEffect: DirectivityEffect
    
    /**
    * Distance effect settings.
    
    * @readonly
    */
    distanceEffect: DistanceEffect
    
    /**
    * Sets whether spatial audio is enabled or not.
    */
    enabled: boolean
    
    /**
    * Position effect settings.
    
    * @readonly
    */
    positionEffect: PositionEffect
    
}

/**
* @wearableOnly
*/
declare class SpectaclesHandSpecificData extends ObjectSpecificData {
    
    /** @hidden */
    protected constructor()
    
}

/**
* @wearableOnly
*/
declare class SpectaclesMobileKitSession extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * @wearableOnly
    */
    close(): void
    
    /**
    * @wearableOnly
    */
    sendData(data: string): void
    
    /**
    * @wearableOnly
    */
    sendRequest(data: string): Promise<string>
    
    /**
    * @wearableOnly
    */
    start(): void
    
    /**
    * @wearableOnly
    */
    startSubscription(data: string, onError: (error: string) => void): event1<string, void>
    
    /**
    * @wearableOnly
    */
    stopSubscription(id: event1<string, void>): void
    
    /**
    * @readonly
    
    * @wearableOnly
    */
    isConnected: boolean
    
    /**
    * @readonly
    
    * @wearableOnly
    */
    onConnected: event0<void>
    
    /**
    * @readonly
    
    * @wearableOnly
    */
    onDisconnected: event0<void>
    
}

/**
* Representation the signal strength over time at various frequencies present in a particular waveform. Created by applying Fast Fourier Transform (FFT) on the overlapping segments of the audio data.

* @see Returned By: {@link SpectrogramBuilder#build}

* @example
* ```js
* var spectrogram = spectrogramBuilder
*     .setFrameSize(frameSize)
*     .setHopSize(hopSize)
*     .setFFTSize(fftSize)
*     .build();
* ```
*/
declare class Spectrogram extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Processes audio data from the inTensor of inShape, writes result to the outTensor and returns the outTensor shape.
    */
    process(inTensor: Float32Array, inShape: vec3, outTensor: Float32Array): vec3
    
    /**
    * Returns the maximum buffer length.
    
    * @readonly
    */
    maxTensorSize: number
    
}

/**
* The builder class for Spectrogram.

* @see Returned By: {@link MachineLearning.createSpectrogramBuilder}, {@link SpectrogramBuilder#setFFTSize}, {@link SpectrogramBuilder#setFrameSize}, {@link SpectrogramBuilder#setHopSize}

* @example
* ```js
* var spectrogramBuilder = MachineLearning.createSpectrogramBuilder();
* ```
*/
declare class SpectrogramBuilder extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Creates a new Spectrogram.
    */
    build(): Spectrogram
    
    /**
    * Sets the length of the window.
    */
    setFFTSize(fftSize: number): SpectrogramBuilder
    
    /**
    * Sets the frame size.
    */
    setFrameSize(frameSize: number): SpectrogramBuilder
    
    /**
    * Sets the number of samples between successive FFT segments.
    */
    setHopSize(hopSize: number): SpectrogramBuilder
    
}

/**
* A sphere collision shape.

* @see Returned By: {@link Shape.createSphereShape}
*/
declare class SphereShape extends Shape {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Local radius of the sphere.
    */
    radius: number
    
}

/**
* Used by {@link HairVisual} to visualize hair strands.

* @deprecated
*/
declare class SplineComponent extends Component {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Represents transform data for deprecated SpriteVisual component. Use {@link ScreenTransform} in combination with {@link Image} component instead.

* @deprecated

* @example
* ```
* // Set the size of the Sprite Aligner
* //@input Component.SpriteAligner spriteAlign

* script.spriteAlign.size = new vec2(2.0,2.0);
* ```
*/
declare class SpriteAligner extends Component {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The location of the point this sprite is bound to.
    
    * @deprecated
    */
    bindingPoint: vec2
    
    /**
    * The width and height of the SpriteVisual.
    
    * @deprecated
    */
    size: vec2
    
}

/**
* This class has been **deprecated** and replaced with the [Image](https://developers.snap.com/api/classes/Image) component.

* Represents a renderable 2D visual in Lens Studio.

* @deprecated

* @example
* ```js
* // Flip the Sprite upside down
* //@input Component.SpriteVisual sprite
* script.sprite.flipY = true;
* ```

* ```js
* // Change the Sprite's fill mode to "stretch"
* //@input Component.SpriteVisual sprite
* script.sprite.fillMode = 2;
* ```

* ```js
* // Change the Sprite's pivot point to top right corner
* //@input Component.SpriteVisual sprite
* script.sprite.pivot = new vec2(1,1);
* ```
*/
declare class SpriteVisual extends MaterialMeshVisual {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns the width and height of the mesh the `SpriteVisual` is applied to.
    
    * @deprecated
    */
    getMeshSize(): vec2
    
    /**
    * Which type of fill the sprite uses.
    
    * Possible values:
    
    * Fit = 0
    * Fill = 1
    * Stretch = 2
    
    * @deprecated
    */
    fillMode: number
    
    /**
    * Whether the sprite is flipped over the Y-axis (sideways).
    
    * @deprecated
    */
    flipX: boolean
    
    /**
    * Whether the sprite is flipped over the X-axis (upside-down).
    
    * @deprecated
    */
    flipY: boolean
    
    /**
    * The location of the sprite's pivot point relative to its boundaries.
    * The pivot's `x` value is a relative horizontal position, -1 being the sprite's left border and 1 being the sprite's right border.
    * Similarly, the `y` value is a relative vertical position, -1 being the sprite's bottom border and 1 being the sprite's top border.
    
    * @deprecated
    */
    pivot: vec2
    
}

/**
* A state used in an {@link DomainInfo} used by a {@link ShoppingModule}.
*/
declare class StateInfo {
    
    /** @hidden */
    protected constructor()
    
    /**
    * @readonly
    */
    description: string
    
    /**
    * The name of the state.
    
    * @readonly
    */
    name: string
    
}

/**
* Stencil buffer clear option.

* @see Used By: {@link Camera.DepthStencilRenderTarget#stencilClearOption}, {@link DepthStencilRenderTargetProvider#stencilClearOption}
*/
declare enum StencilClearOption {
    /**
    * Stencil buffer clear operation will be skipped.
    */
    None,
    /**
    * Stencil buffer will be cleared by "clearStencilValue" property value.
    */
    CustomValue,
    /**
    * Stencil buffer will be cleared by texture from "maskTexture" property, if "maskTexture" is null then the stencil clear option will fallback to "CustomValue" mode.
    */
    CustomTexture
}

/**
* Specifies whether the front and/or back face stencil test will be applied. The initial value is "FrontAndBack"

* @see Used By: {@link StencilState#face}
*/
declare enum StencilFace {
    /**
    * The stencil test will be applied to the both front and back faces.
    */
    FrontAndBack,
    /**
    * The stencil test will be applied to the front face only.
    */
    Front,
    /**
    * The stencil test will be applied to the back face only.
    */
    Back
}

/**
* Specifies the stencil test function. The initial value is "Always".

* @see Used By: {@link StencilState#stencilCompareFunction}
*/
declare enum StencilFunction {
    /**
    * Always passes.
    */
    Always,
    /**
    * Always fails.
    */
    Never,
    /**
    * Passes if (referenceValue & readMask) < (stencil buffer value & readMask).
    */
    Less,
    /**
    * Passes if (referenceValue & readMask) <= (stencil buffer value & readMask).
    */
    LessEqual,
    /**
    * Passes if (referenceValue & readMask) > (stencil buffer value & readMask).
    */
    Greater,
    /**
    * Passes if (referenceValue & readMask) >= (stencil buffer value & readMask).
    */
    GreaterEqual,
    /**
    * Passes if (referenceValue & readMask) = (stencil buffer value & readMask).
    */
    Equal,
    /**
    * Passes if (referenceValue & readMask) != (stencil buffer value & readMask).
    */
    NotEqual
}

/**
* Options for specifying the action to take when stencil and depth tests resolve.

* @see Used By: {@link StencilState#depthFailureOperation}, {@link StencilState#depthStencilPassOperation}, {@link StencilState#stencilFailureOperation}
*/
declare enum StencilOperation {
    /**
    * Keep the current value.
    */
    Keep,
    /**
    * Set the stencil buffer value to 0.
    */
    Zero,
    /**
    * Sets the stencil buffer value to "referenceValue" property of StencilState.
    */
    Replace,
    /**
    * Increments the current stencil buffer value. Clamps to the maximum representable unsigned value.
    */
    IncrementClamp,
    /**
    * Increments the current stencil buffer value. Wraps stencil buffer value to zero when incrementing the maximum representable unsigned value.
    */
    IncrementWrap,
    /**
    * Decrements the current stencil buffer value. Clamps to 0.
    */
    DecrementClamp,
    /**
    * Decrements the current stencil buffer value. Wraps stencil buffer value to the maximum representable unsigned value when decrementing a stencil buffer value of zero.
    */
    DecrementWrap,
    /**
    * Bitwise inverts the current stencil buffer value.
    */
    Invert
}

/**
* The stencil test state for Pass.

* @see Used By: {@link Pass#stencilState}
*/
declare class StencilState extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Specifies the stencil action when the stencil test passes, but the depth test fails. The initial value is "Keep".
    */
    depthFailureOperation: StencilOperation
    
    /**
    * Specifies the stencil action when both the stencil test and the depth test pass, or when the stencil test passes and either there is no depth buffer or depth testing is not enabled. The initial value is "Keep".
    */
    depthStencilPassOperation: StencilOperation
    
    /**
    * Enable or disable the stencil test for pass. The initial value is false.
    */
    enabled: boolean
    
    /**
    * Specifies whether the front and/or back face stencil test will be applied. The initial value is "FrontAndBack"
    */
    face: StencilFace
    
    /**
    * Specifies a mask that is ANDed with both the reference value and the stored stencil value when the test is done. The initial value is 0xFF.
    */
    readMask: number
    
    /**
    * Specifies the reference value for the stencil test. It is clamped to the range [0..pow(2, n)1], where n is the number of bitplanes in the stencil buffer. The initial value is 0.
    */
    referenceValue: number
    
    /**
    * Specifies the stencil test function. The initial value is "Always".
    */
    stencilCompareFunction: StencilFunction
    
    /**
    * Specifies the action to take when the stencil test fails. The initial value is "Keep".
    */
    stencilFailureOperation: StencilOperation
    
    /**
    * Specifies a bit mask to enable and disable writing of individual bits in the stencil planes. Initially, the mask is 0xFF.
    */
    writeMask: number
    
}

/**
* Settings for saving values in a MultiplayerSession.

* @see Returned By: {@link StorageOptions.create}

* @example
* ```js
* var options = StorageOptions.create();
* options.scope = StorageScope.Session;

* // Get a persisted value stored with specified scope
* function setStoredValue(key, value, op) {
*     script.session.setStoredValue(key, value, op,
*         function onSuccess() {
*             screenLog("Value [" + key + "] was persisted in scope! " + op.scope);
*         },
*         function handleError(code, description) {
*             screenLog("Error code: " + code + " description: " + description);
*         });

* }

* // Get value stored with specified scope
* function getStoredValue(key, scope) {
*     script.session.getStoredValue(key, scope,
*         function onSuccess(key, value) {
*             screenLog("Value of " + key + " is: " + value);
*         },
*         function handleError(code, description) {
*             screenLog("Error code: " + code + " description: " + description);
*         });
* }

* // Delete value stored with specified scope
* function deleteStoredValue(key, scope) {
*     // Delete a persisted value from the collection
*     script.session.deleteStoredValue(key, scope,
*         function onSuccess() {
*             screenLog("Value " + key + " was deleted");
*         },
*         function handleError(code, description) {
*             screenLog("Error code: " + code + " description: " + description);
*         });
* }

* function listStoredValues(scope) {
*     var pageNumber = 0;
*     var limit = 10; //constant

*     function handleError(code, description) {
*         screenLog("Error code: " + code + " description: " + description);
*     };
*     function onListRetrieved(results, cursor) {
*         screenLog(" page " + (pageNumber++) + " contains " + results.length + " results " + "cursor " + cursor);

*         // Results are returned as a list of [key, value] tuples
*         for (var i = 0; i < results.length; ++i) {
*             const key = results[i][0];
*             const value = results[i][1];
*             screenLog(" - key: " + key + " value: " + value);
*         }

*         // Request the next page of results
*         if (results.length == limit) {
*             script.session.listStoredValues(options.scope, cursor, onListRetrieved, handleError);
*         }
*     }
*     script.session.listStoredValues(scope, "", onListRetrieved, handleError);
* }
* ```
*/
declare class StorageOptions extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The Storage Scope to set, get, or delete values from.
    */
    scope: StorageScope
    
    /**
    * Create a new StorageOptions object.
    */
    static create(): StorageOptions
    
}

/**
* Specifies a scope for storing or retrieving values from a MultiplayerSession.

* @see Used By: {@link CloudStorageListOptions#scope}, {@link CloudStorageReadOptions#scope}, {@link CloudStorageWriteOptions#scope}, {@link StorageOptions#scope}

* @example
* ```js
* var options = StorageOptions.create();
* options.scope = StorageScope.Session;
* ```
*/
declare enum StorageScope {
    /**
    * Stored variables are visible to the user only, and accessible until lens cache is cleared.
    */
    User,
    /**
    * Stored values are visible and editable by any user who is using the same session. Values are stored as long as the session that corresponds to chat exists.
    */
    Session
}

/**
* Options for stretching a mesh to fit a {@link ScreenTransform}

* Used in {@link RenderMeshVisual} `stretchMode` property, as long as the SceneObject has a {@link ScreenTransform} attached.

* Also used in {@link TextFill}

* See the [Image guide](https://developers.snap.com/lens-studio/assets-pipeline/2d/image) for more information about stretch modes.

* @see Used By: {@link BaseMeshVisual#stretchMode}, {@link TextFill#textureStretch}

* @example
* ```js
* // @input Component.Image image

* script.image.stretchMode = StretchMode.Stretch;
* ```
*/
declare enum StretchMode {
    /**
    * Scale uniformly so that both width and height fit within the bounds.
    */
    Fit,
    /**
    * Scale uniformly so that both width and height meet or exceed the bounds.
    */
    Fill,
    /**
    * Scale non-uniformly to match the exact width and height of the bounds.
    */
    Stretch,
    /**
    * Scale uniformly to match the same height as the bounds.
    */
    FitHeight,
    /**
    * Scale uniformly to match the same width as the bounds.
    */
    FitWidth,
    /**
    * Same as `Fill`, but when used with the {@link Image} any area outside of the bounds is cropped out.
    */
    FillAndCut
}

/**
* A point in the {@link FaceStretchVisual} feature.
*/
declare class StretchPoint {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The direction in which the stretch should occur.
    */
    delta: vec3
    
    /**
    * The index of the stretch point.
    
    * @readonly
    */
    index: number
    
    /**
    * The strength of the stretch effect.
    */
    weight: number
    
}

/**
* @example
* ```js
* // Show this message in the Lens Studio Logger window, even when running on device
* Studio.log("Send this message to Lens Studio!");
* ```
*/
declare class Studio {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Prints out a message to the Logger window in Studio. When running on a device paired to Lens Studio, it will send the message directly to the Logger window in Studio to help with debugging.
    */
    static log(message: any): void
    
}

/**
* Used with {@link DeviceTracking#surfaceOptions} to change settings for Surface tracking mode.

* @see Used By: {@link DeviceTracking#surfaceOptions}

* @example
* ```js
* // Tell the DeviceTracking component to enhance surface tracking with native AR
* //@input Component.DeviceTracking deviceTracking

* script.deviceTracking.surfaceOptions.enhanceWithNativeAR = true;
* ```
*/
declare class SurfaceOptions extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * If enabled, surface tracking will be improved using native AR tracking.
    */
    enhanceWithNativeAR: boolean
    
}

/**
* If a {@link DeviceTracking} component is present in the scene, this event is triggered when
* the tracking is restarted (typically when the Lens starts, or if the user taps the ground).

* @example
* ```js
* // Resets the local position to (0,0,0) when tracking resets
* function onSurfaceReset(eventData)
* {
*     script.getTransform().setLocalPosition(new vec3(0, 0, 0));
* }
* var worldTrackingResetEvent = script.createEvent("SurfaceTrackingResetEvent");
* worldTrackingResetEvent.bind(onSurfaceReset);
* ```
*/
declare class SurfaceTrackingResetEvent extends SceneEvent {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Used with `AnimationKeyFrame`.

* @see Used By: {@link AnimationKeyFrame#leftTangentType}, {@link AnimationKeyFrame#rightTangentType}
*/
declare enum TangentType {
    /**
    * A tangent with 0 slope.
    */
    Const,
    /**
    * A tangent where the angle and weight of both the left and right side can be changed freely. This is the default value.
    */
    Free,
    /**
    * A tangent where the left and right side are not connected.
    */
    Broken,
    /**
    * A slope defined by a linear tangent.
    */
    Linear,
    /**
    * A tangent which value is clamped, as in a spline.
    */
    Clamped
}

/**
* This event is triggered when the user taps on the screen.

* @example
* ```js
* function onTapped(eventData)
* {
*     print("Tap Position: (" + eventData.getTapPosition().x + ", " + eventData.getTapPosition().y + ")");
* }

* var event = script.createEvent("TapEvent");
* event.bind(onTapped);
* ```
*/
declare class TapEvent extends SceneObjectEvent {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Arguments used with the `InteractionComponent.onTap` event.

* @example
* ```js
* //@input Component.InteractionComponent interactionComponent

* // Subscribe to the onTap event
* var onTapEvent = script.interactionComponent.onTap.add(function(tapEventArgs){
*     print("onTap! Touched at: " + tapEventArgs.position);
* });

* // Unsubscribe from the onTap event
* script.interactionComponent.onTap.remove(onTapEvent);
* ```
*/
declare class TapEventArgs extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The position of the touch on the screen. [0,0] being top left, and [1,1] being bottom right.
    
    * @readonly
    */
    position: vec2
    
}

/**
* @wearableOnly

* @example
* ```js
* // For public use
* script.gestureModule = require("LensStudio:GestureModule");

* function onTargetingDataEvent(args) {
*     print("The hand intends to target: " + args.handIntendsToTarget);
* }

* script.gestureModule
*     .getTargetingDataEvent(GestureModule.HandType.Right)
*     .add(onTargetingDataEvent);
* ```
*/
declare class TargetingDataArgs extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Whether the hand intends to target.
    
    * @readonly
    
    * @wearableOnly
    */
    handIntendsToTarget: boolean
    
    /**
    * @readonly
    
    * @wearableOnly
    */
    isValid: boolean
    
    /**
    * @readonly
    
    * @wearableOnly
    */
    rayDirectionInWorld: vec3
    
    /**
    * @readonly
    
    * @wearableOnly
    */
    rayOriginInWorld: vec3
    
}

/**
* Namespace for mathematical operations on tensors. Useful with MLComponent.

**Tensor broadcasting rules**

* Tensor (channels, width, height) op Tensor (1, 1, 1)  =  the same as applying op with scalar

* Tensor (channels, width, height) op Tensor (channels, 1, 1)  =  the same as applying op per channel

* @example
* ```js
* // Convert a texture to an array of grayscale values
* // @input Asset.Texture texture

* // Desired width and height of the grayscale texture
* // You can instead use Texture.getWidth() and Texture.getHeight() for original size
* var width = 64;
* var height = 64;

* var data = new Uint8Array(height * width);
* var shape = new vec3(width, height, 1);

* // Get the texture pixels as grayscale values (0-255)
* TensorMath.textureToGrayscale(script.texture, data, shape);
* ```
*/
declare class TensorMath {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Adds a scalar value to each element of `inTensor` and puts the result into `outTensor`.
    */
    static addScalar(inTensor: Float32Array, scalar: number, outTensor: Float32Array): void
    
    /**
    * Adds `inTensorA` to `inTensorB` and puts the result into `outTensor`. See the broadcasting rules for elementwise operations.
    */
    static addTensors(inTensorA: Float32Array, inShapeA: vec3, inTensorB: Float32Array, inShapeB: vec3, outTensor: Float32Array): void
    
    /**
    * Converts inTensor from the amplitude scale to the decibel scale.
    */
    static amplitudeToDb(inTensor: Float32Array, outTensor: Float32Array): void
    
    /**
    * Blurs an image tensor using the box filter.
    */
    static applyBoxFilter(inTensor: Float32Array, inShape: vec3, kernelSize: vec2, anchor: vec2, normalize: boolean, borderType: TensorMath.BorderType, outTensor: Float32Array): void
    
    static applyNMS(inTensor: Float32Array, inShape: vec3, scores: Float32Array, scoreThreshold: number, iouThreshold: number, outTensor: Uint32Array): number
    
    /**
    * Applies a fixed-level threshold to each array element.
    */
    static applyThreshold(inTensor: Float32Array, threshold: number, maxValue: number, type: TensorMath.ThresholdMethod, outTensor: Float32Array): void
    
    /**
    * Approximates a polygonal curve with the specified precision.
    */
    static approximatePolygonalCurve(inTensor: Float32Array, inShape: vec3, epsilon: number, closed: boolean, outTensor: Float32Array): number
    
    /**
    * Returns the indices of the maximum values along an the channels of `inTensor`, with the specified `inShape`. The result is put into `outTensor`.
    
    * If `inShape` = {width, height, channels}, then the shape of `outTensor` should be {1, 2, channels}.
    */
    static argMax(inTensor: Float32Array, inShape: vec3, outTensor: Uint32Array): void
    
    /**
    * Similar to `numpy.argsort()`, but in TensorMath, we have no kind and order parameters. Also, tensor is always 3D, and axis can be equal to 0(x), 1(y) or 2(z).
    */
    static argSort(inTensor: Float32Array, shape: vec3, axis: number, outTensor: Uint32Array): void
    
    /**
    * Applies a list of indices of a tensor in sorted order of their corresponding values in the tensor to the given outTensor. Only indices whose corresponding values are not equal to 0 in the provided mask will be returned. Indices from the original tensor, before applying the mask, will be returned in the provided array. Returns the size of the list of indices applied to the outTensor (note: this will be equal to the number of non-zero values provided in the mask). eg:
    
    * ```js
    * var inTensor = [5.0, 2.0, 1.0, 3.0, 7.0, -1.0, -5.0];
    * var mask = [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0];
    * var outTensor = new Uint32Array(7);
    * var order = SortOrder.Ascending;
    * var size = TensorMath.argSortMasked(inTensor, mask, outTensor, order);
    * print(outTensor.subarray(size)); // Expected: [2, 1, 3, 0, 4]
    * ```
    */
    static argSortMasked(inTensor: Float32Array, mask: Float32Array, outTensor: Uint32Array, order: SortOrder): number
    
    /**
    * Clamps all values of `inTensor` between `minVal` and `maxVal`, and puts the result into `outTensor`.
    */
    static clamp(inTensor: Float32Array, minVal: number, maxVal: number, outTensor: Float32Array): void
    
    /**
    * Concatenates inTensorA and inTensorB along the specified axis and puts the result to the outTensor. Input arrays have to have same dimension along other 2 axes.
    */
    static concat(inTensorA: Float32Array, inShapeA: vec3, inTensorB: Float32Array, inShapeB: vec3, axis: number, outTensor: Float32Array): void
    
    /**
    * Dilates an image by using a specific structuring element.
    */
    static dilate(inTensor: Float32Array, inShape: vec3, kernelTensor: Float32Array, kernelShape: vec3, anchor: vec2, iterations: number, borderType: TensorMath.BorderType, borderValue: vec4, outTensor: Float32Array): void
    
    /**
    * Divides `inTensorA` by `inTensorB` and puts the result into `outTensor`. See broadcasting rules for elementwise operations.
    */
    static divTensors(inTensorA: Float32Array, inShapeA: vec3, inTensorB: Float32Array, inShapeB: vec3, outTensor: Float32Array): void
    
    /**
    * Draws a line segment connecting two points.
    */
    static drawLine(imgTensor: Float32Array, imgShape: vec3, point1: vec2, point2: vec2, color: vec4, thickness: number, lineType: number, shift: number): void
    
    /**
    * Erodes an image by using a specific structuring element.
    */
    static erode(inTensor: Float32Array, inShape: vec3, kernelTensor: Float32Array, kernelShape: vec3, anchor: vec2, iterations: number, borderType: TensorMath.BorderType, borderValue: vec4, outTensor: Float32Array): void
    
    /**
    * Fills a convex polygon.
    */
    static fillConvexPoly(imgTensor: Float32Array, imgShape: vec3, pointsTensor: Int32Array, pointsShape: vec3, color: vec4, lineType: number, shift: number): void
    
    /**
    * Fills a polygon. Note that you should pass an Array of Int32Array-s (polygonsTensors).
    */
    static fillPoly(imgTensor: Float32Array, imgShape: vec3, polygonsTensors: Int32Array[], color: vec4, lineType: number, shift: number, offset: vec2): void
    
    /**
    * Returns contours. Note that: contours are sorted from the largest to the smallest; 2) We cannot return Array of TypedArray-s, so all contours data is stored in single outTensor and Array with sizes of each contour is returned.
    */
    static findContours(inTensor: Uint8Array, inShape: vec3, mode: number, method: number, offset: vec2, outTensor: Int32Array): number[]
    
    /**
    * Finds minimum distances between each 2D point from one array, to 2D points in another array.
    
    * `from` - Float32Array of size ("from" point count * 2). 2D points from which min distances should be found
    
    * `fromShape` - Should be {2, "from" point count, 1}
    
    * `to` - Float32Array of size ("to" point count * 2). 2D points to which min distances should be found
    
    * `toShape` - Should be {2, "to" point count, 1}
    
    * `output` - Float32Array of size ("from" point count). For each point in the `from` array, the minimum distance to points from the `to` array will be written to this array.
    */
    static findMinDistancesBetween(from: Float32Array, fromShape: vec3, to: Float32Array, toShape: vec3, output: Float32Array): void
    
    /**
    * Looks for contour points in a grayscale texture, or any one-channel tensor.
    
    * `grayscaledTexture` - Float32Array of size (width * height). Grayscale texture or one-channel tensor to evaluate
    
    * `textureShape` - Should be {width, height, 1}
    
    * `threshold` - Quality value threshold for found contour points
    
    * Each contour point found must satisfy these conditions:
    
    * 1. The point's quality value should be >= `threshold`
    
    * 2. The number of points which have values < `threshold` and lie in the rectangle with left corner vec2(x-`winSize`, y-`winSize`) and right corner vec2(x+`winSize`-1, y+`winSize`-1) should be <= `maxNearCount`
    
    * `outTensor` - Float32Array where results are written. Found contour points are written in the format: x0, y0, x1, y1, etc.
    * The number of points found will not exceed outTensor's size / 2.
    */
    static getContour(grayscaledTexture: Float32Array, textureShape: vec3, threshold: number, winSize: number, maxNearCount: number, outTensor: Float32Array): number
    
    static getRotatedRectPoints(rotatedRect: RotatedRect, outTensor: Float32Array): void
    
    /**
    * Calculates the length of all vectors in an array. Vectors can be of any dimension count.
    
    * `vectors` - Float32Array of size (vector dimension count * vector count). Vectors to measure the length of
    
    * `vectorsShape` - Should be {vector dimension count, vector count, 1}
    
    * `output` - Float32Array of size (vector count). For each vector in `vectors`, its length will be written to this array
    */
    static getVectorsLength(vectors: Float32Array, vectorsShape: vec3, output: Float32Array): void
    
    /**
    * Checks for each 2D point whether it is inside of a rectangle.
    
    * `points` - Float32Array of size (point count * 2). 2D points to check
    
    * `pointsShape` - Should be {2, point count, 1}
    
    * `rect` - 2D rectangle points will be checked against
    
    * `output` - Uint8Array of size (point count). For each point, this will be filled with 1 if the point is inside the rectangle, or 0 otherwise.
    */
    static isInRectangle(points: Float32Array, pointsShape: vec3, rect: Rect, output: Uint8Array): void
    
    /**
    * Places the maximum values of `inTensor` into `outTensor`.
    
    * `outTensor` should have the shape {1, 1, channels}.
    */
    static max(inTensor: Float32Array, inShape: vec3, outTensor: Float32Array): void
    
    /**
    * From the input points, finds the two points with the maximum distance between them and returns this distance. Works with points of any dimension count.
    
    * `points` - Float32Array of size (point dimension count * point count). Points to find the max distance between.
    
    * `pointsShape` - Should be {point dimension count, point count, 1}
    */
    static maxDistanceBetweenPoints(points: Float32Array, pointsShape: vec3): number
    
    /**
    * If we consider the tensor as a 3D array, this function finds the indexes of the maximum element in each subarray of size `window`.
    
    * `tensor` - Float32Array of size (width * height * depth). 3D array of input data
    
    * `tensorShape` - Should be {width, height, depth}
    
    * `window` - Size of each subarray, in each of which will be found the index of the max element
    
    * `output` - Float32Array of size (width * height * depth). The index of the max value will be written into this array for each subarray.
    */
    static maxInSlideWindow(tensor: Float32Array, tensorShape: vec3, window: vec3, output: Uint32Array): void
    
    /**
    * Places the minimum values of `inTensor` into `outTensor`.
    
    * `outTensor` should have the shape {1, 1, channels}.
    */
    static min(inTensor: Float32Array, inShape: vec3, outTensor: Float32Array): void
    
    /**
    * Return TensorMath's format of RotatedRect.
    */
    static minAreaRect(inTensor: Float32Array, inShape: vec3): RotatedRect
    
    /**
    * If we consider the tensor as a 3D array, this function finds the indexes of the minimum element in each subarray of size `window`.
    
    * `tensor` - Float32Array of size (width * height * depth). 3D array of input data
    
    * `tensorShape` - Should be {width, height, depth}
    
    * `window` - Size of each subarray, in each of which will be found the index of the min element
    
    * `output` - Float32Array of size (width * height * depth). The index of the min value will be written into this array for each subarray.
    */
    static minInSlideWindow(tensor: Float32Array, tensorShape: vec3, window: vec3, output: Uint32Array): void
    
    /**
    * Multiply a matrix by a set of points through an efficient batch operation.
    */
    static mulMatToPoints(pointsTensor: Float32Array, pointsShape: vec3, matrix: mat4, outTensor: Float32Array): void
    
    /**
    * Multiplies each element of `inTensor` by a scalar value and puts the result into `outTensor`.
    */
    static mulScalar(inTensor: Float32Array, scalar: number, outTensor: Float32Array): void
    
    /**
    * Multiplies `inTensorA` and `inTensorB` and puts the result into `outTensor`. See broadcasting rules for elementwise operations.
    */
    static mulTensors(inTensorA: Float32Array, inShapeA: vec3, inTensorB: Float32Array, inShapeB: vec3, outTensor: Float32Array): void
    
    /**
    * Stabilizes image objects between two consecutive frames caused by camera or object movement.
    * Results are written to the `points` array.
    
    * `prevGrayscale` - Uint8Array of size (width * height). Grayscale data of previous frame
    
    * `grayscale` - Uint8Array of size (width * height). Grayscale data of current frame
    
    * `textureShape` - Should be {width, height, 1}
    
    * `prevPoints` - Float32Array of size (point count * 2). Positions of 2D points on the previous frame
    
    * `points` - Float32Array of size (point count * 2). Results are written here - positions of 2D points on the current frame
    
    * `pointsShape` - Should be {2, point count, 1}
    
    * `winSize` - Size of the search window for each pyramid level
    
    * `maxLevel` - Maximal pyramid level number, with zero based index
    
    * `maxCount` - Terminate if iteration count exceeds maxCount
    
    * `epsilon` - Terminate if window movement is less than epsilon
    */
    static opticalFlow(prevGrayscale: Uint8Array, grayscale: Uint8Array, textureShape: vec3, prevPoints: Float32Array, points: Float32Array, pointsShape: vec3, winSize: vec2, maxLevel: number, maxCount: number, epsilon: number): void
    
    /**
    * Rearranges the inTensor of inShape according to the desired ordering and puts the result into outTensor.
    */
    static permute(inTensor: Float32Array, inShape: vec3, permuteAxis: vec3, outTensor: Float32Array): void
    
    /**
    * Sorts 2D points by polar angle relative to the `center` point.
    
    * `inTensor` - Float32Array with size (point count * 2). 2D points to sort
    
    * `tensorShape` - Should be {2, point count, 1}
    
    * `center` - Center point to use for polar angle sorting
    */
    static polarSort2d(inTensor: Float32Array, tensorShape: vec3, center: vec2): void
    
    /**
    * Raises elements of `inTensor` to the power of `val`, and puts the results into `outTensor`.
    */
    static power(inTensor: Float32Array, val: number, outTensor: Float32Array): void
    
    /**
    * Converts inTensor from the power scale to the decibel scale.
    */
    static powerToDb(inTensor: Float32Array, topDb: number, outTensor: Float32Array): void
    
    /**
    * Project 3D points into 2D space using an efficient batch operation.
    */
    static projectPoints(pointsTensor: Float32Array, pointsShape: vec3, projectionMatrix: mat4, outTensor: Float32Array): void
    
    /**
    * Duplicates inTensor elements and store result in the outTensor.
    
    * Axis specifies the number of repeats along the axis e.g:
    
    * axis(1, 1, 1): outTensor will be the same as inTensor
    
    * axis(2, 1, 2): inTesnor = [1 2 3 | 4 5 6], inShape = [3 2 1] =>
    
    * outTensor = [[1 1 2 2 3 3 | 4 4 5 5 6 6] [1 1 2 2 3 3 | 4 4 5 5 6 6]] outShape = [6 2 2]
    */
    static repeat(inTensor: Float32Array, inShape: vec3, axis: vec3, outTensor: Float32Array): void
    
    /**
    * Applies a rotation to each point in a set of 3D points, and places the results in `outPoints`.
    
    * `points` - Float32Array of size (point count * 3). Points to rotate
    
    * `pointsShape` - Should be {3, point count, 1}
    
    * `rotation` - Quaternion rotation to apply
    
    * `outPoints` - Float32Array of size (point count * 3). Resulting rotated points are placed here
    */
    static rotatePoints3d(points: Float32Array, pointsShape: vec3, rotation: quat, outPoints: Float32Array): void
    
    /**
    * Smooths a polygon formed by input points. Works with points of any dimension count, for example `2` for 2D points or `3` for 3D points.
    * Results are written to `outTensor`.
    
    * `inTensor` - Float32Array of size (point dimension count * point count). Points of polygon to smooth out
    
    * `tensorShape` - Should be {point dimension count, point count, 1}
    
    * `step` - Smoothing value, higher value meaning higher smoothness
    
    * `outTensor` - Float32Array of size (point dimension count * point count). Smoothed points are written to this array
    */
    static smoothPoints(inTensor: Float32Array, tensorShape: vec3, step: number, outTensor: Float32Array): void
    
    /**
    * Applies softArgMax function to the `inTensor`, with the specified `inShape`. The result is put into `outTensor`.
    
    * If `inShape` = {width, height, channels}, then the shape of `outTensor` should be {1, 2, channels}.
    */
    static softArgMax(inTensor: Float32Array, inShape: vec3, outTensor: Float32Array, normalized: boolean): void
    
    /**
    * Applies softMax function to `inTensor`, with the specified `inShape`. The result is put into `outTensor`.
    
    * If `inShape` = {width, height, channels}, then the shape of `outTensor` should be {1, 1, channels}.
    */
    static softMax(inTensor: Float32Array, inShape: vec3, outTensor: Float32Array): void
    
    /**
    * Estimates the object pose given a set of object points (`inObjectPoints`), their corresponding image projections (`inImagePoints`), and the intrinsic camera matrix (`transform`).
    
    * `inObjectPoints` - Float32Array of size (point count * 3). 3D points of the object
    
    * `inImagePoints` - Float32Array of size (point count * 2). 2D points which are projections of the points in `inObjectPoints`, with some applied transformation we are attempting to find
    
    * `imagePointsShape` - Should be {2, point count, 1}
    
    * `transform` - Intrinsic camera matrix
    
    * `flags` - Currently unused, can be left as 0. Further functionality may be added in the future.
    
    * `outRotTrans` - Float32Array of size 6 where results are written. Describes object transformation:
    
    **vec3(outRotTrans[0], outRotTrans[1], outRotTrans[2])** - object rotation in Rodrigues format
    
    **vec3(outRotTrans[3], outRotTrans[4], outRotTrans[5])** - object position in 3D space
    */
    static solvePnP(inObjectPoints: Float32Array, inImagePoints: Float32Array, imagePointsShape: vec3, transform: mat3, flags: number, outRotTrans: Float32Array): boolean
    
    /**
    * Estimates the object pose given a set of object points (inObjectPoints), their corresponding image projections (inImagePoints), and the intrinsic camera matrix (cameraIntrinsicsMatrix). Similar to solvePnP but has additional camera distortion coefficient settings.
    
    * `distortionCoeff` - Input vector of distortion coefficients. If the vector is null - the zero distortion coefficients are assumed.
    
    * `distortionCoeffShape` - {numElements, 1, 1} where numElements can be 4, 5, 8 or 12.
    
    * `useExtrinsicGuess` - The function uses the provided outRotTrans values as initial approximations of the rotation and translation vectors and further optimizes them.
    */
    static solvePnPExtended(inObjectPoints: Float32Array, inImagePoints: Float32Array, imagePointsShape: vec3, cameraIntrinsicsMatrix: mat3, distortionCoeff: Float32Array, distortionCoeffShape: vec3, useExtrinsicGuess: boolean, flags: number, outRotTrans: Float32Array): boolean
    
    /**
    * `solvePnPRansac` is very similar to `solvePnPExtended` except that it uses Random Sample Consensus ( RANSAC ) for robustly estimating the pose.
    
    * `iterationsCount` - The number of times the minimum number of points are picked and the parameters estimated
    
    * `reprojectionError` - As mentioned earlier in RANSAC the points for which the predictions are close enough are called inliers. This parameter value is the maximum allowed distance between the observed and computed point projections to consider it an inlier.
    
    * `confidence` -  Number of inliers. If the algorithm at some stage finds more inliers than confidence, it finishes.
    * `outInliers` - Output array that contains indices of inliers in objectPoints and imagePoints .
    * outRotTrans
    */
    static solvePnPRansac(inObjectPoints: Float32Array, inImagePoints: Float32Array, imagePointsShape: vec3, cameraIntrinsicsMatrix: mat3, distortionCoeff: Float32Array, distortionCoeffShape: vec3, useExtrinsicGuess: boolean, iterationsCount: number, reprojectionError: number, confidence: number, flags: number, outInliers: Uint8Array, outRotTrans: Float32Array): boolean
    
    /**
    * Subtracts `inTensorB` from `inTensorA` and puts the result into `outTensor`. See the broadcasting rules for elementwise operations.
    */
    static subTensors(inTensorA: Float32Array, inShapeA: vec3, inTensorB: Float32Array, inShapeB: vec3, outTensor: Float32Array): void
    
    /**
    * Applies subpixelArgMax function to the `inTensor`, with the specified `inShape` and kernel size. The result is put into `outTensor`.
    
    * If `inShape` = {width, height, channels}, then the shape of `outTensor` should be {1, 2, channels}.
    */
    static subpixelArgMax(inTensor: Float32Array, inShape: vec3, outTensor: Float32Array, kernelSize: number): void
    
    /**
    * Calculate the sum of the inTensor elements and store result in the outTensor.
    
    * Axis specifies axis along which a sum is performed., e.g:
    
    * axis(0, 0, 0): the sum will be performed on the whole tensor
    
    * axis(0, 0, 1): the sum will be performed along the z axis. outTensor will store inShape.z values
    
    * axis(1, 1, 0): the sum will be performed along x and y axes. outTensor will store
    
    * inShape.x * inShape.y values, where outTensor[y][x] is the sum of all inTensor[0..inShape.z-1][y][x] values
    */
    static sum(inTensor: Float32Array, inShape: vec3, axis: vec3, outTensor: Float32Array): void
    
    /**
    * Converts the texture to a set of 0-255 grayscale values, and outputs the result into `outTensor`.
    
    * `outTensor` should be a Uint8Array of shape {width, height, 1}.
    
    * @exposesUserData
    */
    static textureToGrayscale(texture: Texture, grayscaleBuffer: Uint8Array, grayscaleBufferShape: vec3): void
    
}

declare namespace TensorMath {
    /**
    
    
    * @see Used By: {@link TensorMath.applyBoxFilter}, {@link TensorMath.dilate}, {@link TensorMath.erode}
    */
    enum BorderType {
        Constant,
        Replicate,
        Reflect,
        Reflect101
    }

}

declare namespace TensorMath {
    /**
    
    
    * @see Used By: {@link TensorMath.applyThreshold}
    */
    enum ThresholdMethod {
        Binary,
        BinaryInv,
        Trunc,
        ToZero,
        ToZeroInv
    }

}

/**
* Renders 2D text with specific style and layout.

* @remarks
* Supports Dynamic Text.

* @see [Text](https://developers.snap.com/lens-studio/features/text/2d-text) guide.

* @example
* ```js
* //@input Component.Text textComponent
* //@input Asset.Font customFont
* //@input vec4 color {"widget":"color"}

* script.textComponent.text = "Hello there";
* script.textComponent.font = script.customFont;
* script.textComponent.size = 30;
* script.textComponent.verticalOverflow = VerticalOverflow.Truncate;
* script.textComponent.textFill.color = script.color;
* ```

* ```js
* // @input Component.Text textComponent
* // @input Component.ScreenTransform background

* // Reparent the background object to be a child of the Text object
* var textObj = script.textComponent.getSceneObject();
* script.background.getSceneObject().setParent(textObj);

* // Set the background object to be the extentsTarget of the Text object, so it matches the rendered area
* script.textComponent.extentsTarget = script.background;
* ```
*/
declare class Text extends BaseMeshVisual {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Gets the bounding box encompassing the characters in the range from `start` to `end`. The range is inclusive of `start` and exclusive of `end`.
    
    */
    getBoundingBox(start?: number, end?: number): Rect
    
    /**
    * Settings for drawing a background behind the text.
    */
    backgroundSettings: BackgroundSettings
    
    /**
    * How the text should be blended during rendering.
    */
    blendMode: BlendMode
    
    /**
    * Overrides the capitalization of the text rendered.
    */
    capitilizationOverride: CapitilizationOverride
    
    /**
    * Controls the masking of color channels with a vec4b representing each channel with a boolean.
    */
    colorMask: vec4b
    
    /**
    * If enabled, the text material will use Depth Testing. Useful when Text exists in 3D space.
    */
    depthTest: boolean
    
    /**
    * Settings for how dropshadow is used in text drawing.
    */
    dropshadowSettings: DropshadowSettings
    
    /**
    * Makes the Text component editable. When this is enabled the Text can be clicked to open up the device keyboard and edit the contents.
    */
    editable: boolean
    
    /**
    * Font asset used.
    */
    font: Font
    
    /**
    * Controls how text should be handled when it goes past the horizontal boundaries defined by the world space rect or ScreenTransform.
    */
    horizontalOverflow: HorizontalOverflow
    
    /**
    * Modifies the spacing between letters. Set to 0 by default, which uses the font's normal letter spacing.
    * Negative values will remove space between letters, and positive values will add more space between letters.
    */
    letterSpacing: number
    
    /**
    * Modifies the vertical spacing between lines, as a multiple of lines. 1 will be single spacing, 2 will be double spaced, and 0.5 would be half the normal line height.
    */
    lineSpacing: number
    
    /**
    * This event will notify you when the user finishes editing the text.
    
    * @readonly
    */
    onEditingFinished: event1<string, void>
    
    /**
    * This event will notify you when the input keyboard opens.
    
    * @readonly
    */
    onEditingStarted: event0<void>
    
    /**
    * This event will notify you on every edit to the text while the user is typing.
    
    * @readonly
    */
    onEditingUpdated: event1<string, void>
    
    /**
    * Settings for how text outline is used in text drawing.
    */
    outlineSettings: OutlineSettings
    
    /**
    * Use this property to control whether to show the input preview the keyboard. Note this preview also enables cursor movement.
    */
    showEditingPreview: boolean
    
    /**
    * Font size used.
    */
    size: number
    
    /**
    * If enabled, the rendered text will always scale to fit the boundaries defined by the world space rect or ScreenTransform.
    */
    sizeToFit: boolean
    
    /**
    * Text string to be drawn.
    */
    text: string
    
    /**
    * Settings for how the text is drawn, such as fill color or texture.
    */
    textFill: TextFill
    
    /**
    * Use this property to override the touch handling for when to open the device keyboard when the Text is editable.
    */
    touchHandler: InteractionComponent
    
    /**
    * Whether the text should be visible in both front and back.
    */
    twoSided: boolean
    
    /**
    * Controls how text should be handled when it goes past the vertical boundaries defined by the world space rect or ScreenTransform.
    */
    verticalOverflow: VerticalOverflow
    
    /**
    * Controls the boundaries the text is aligned and wraps within when not using ScreenTransform. Referred to as the Layout Rect in the Inspector panel.
    */
    worldSpaceRect: Rect
    
}

/**
* Renders 3D text with specific style, layout and material.

* @see [3D Text](https://developers.snap.com/lens-studio/features/text/3d-text) guide.
*/
declare class Text3D extends MaterialMeshVisual {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Gets the bounding box encompassing the characters in the range from `start` to `end`. The range is inclusive of `start` and exclusive of `end`.
    
    */
    getBoundingBox(start?: number, end?: number): Rect
    
    /**
    * Splits the Text3D into individual RenderMeshVisuals. This destroys the Text3D component and you can no longer edit its properties like text.
    */
    split(): RenderMeshVisual[]
    
    /**
    * Overrides the capitalization of the text rendered.
    */
    capitilizationOverride: CapitilizationOverride
    
    /**
    * Makes the Text component editable. When this is enabled the Text can be clicked to open up the device keyboard and edit the contents.
    */
    editable: boolean
    
    /**
    * Optimizes Text3D by combining glyph meshes.
    */
    enableBatching: boolean
    
    /**
    * Starting from the Text3D's local position control whether the meshes are extruded forwards, backwards, or both directions
    */
    extrudeDirection: number
    
    /**
    * How deep the text meshes should be relative to the line height. 0-1 portion of the basic height which will be used for mesh depth. Values > 1 are valid.
    */
    extrusionDepth: number
    
    /**
    * Font asset used.
    */
    font: Font
    
    /**
    * Controls how text should be handled when it goes past the horizontal boundaries defined by the world space rect or ScreenTransform.
    */
    horizontalOverflow: HorizontalOverflow
    
    /**
    * Modifies the spacing between letters. Set to 0 by default, which uses the fonts normal letter spacing. Negative values will remove space between letters, and positive values will add more space between letters.
    */
    letterSpacing: number
    
    /**
    * Modifies the vertical spacing between lines, as a multiple of lines. 1 will be single spacing, 2 will be double spaced, and 0.5 would be half the normal line height.
    */
    lineSpacing: number
    
    /**
    * This event will notify you when the user finishes editing the text.
    
    * @readonly
    */
    onEditingFinished: event1<string, void>
    
    /**
    * This event will notify you when the user starts typing.
    
    * @readonly
    */
    onEditingStarted: event0<void>
    
    /**
    * This event will notify you on every edit to the text while the user is typing.
    
    * @readonly
    */
    onEditingUpdated: event1<string, void>
    
    /**
    * Use this property to control whether to show the input preview the keyboard. Note this preview also enables cursor movement.
    */
    showEditingPreview: boolean
    
    /**
    * Font size used.
    */
    size: number
    
    /**
    * If enabled, the rendered text will always scale to fit the boundaries defined by the world space rect or ScreenTransform.
    */
    sizeToFit: boolean
    
    /**
    * Text string to be drawn.
    */
    text: string
    
    /**
    * Use this property to override the touch handling for when to open the device keyboard when the Text is editable.
    */
    touchHandler: InteractionComponent
    
    /**
    * Controls how text should be handled when it goes past the vertical boundaries defined by the world space rect or ScreenTransform.
    */
    verticalOverflow: VerticalOverflow
    
    /**
    * Controls the boundaries the text is aligned and wraps within when not using ScreenTransform. Referred to as the Layout Rect in the Inspector panel.
    */
    worldSpaceRect: Rect
    
}

/**
* Contains methods for Text decoding

* @example
* ```js var decoder = new TextDecoder()  let decodedStr = decoder.decode([226,136,190,32,69,226,139,133,100,97]) print("decodedString="+decodedStr) ```
*/
declare class TextDecoder extends ScriptObject {
    /**
    * Create a new instance of TextDecoder with specified encoding (optional, defaults to utf-8 if not provided).
    */
    constructor(encoding?: string)
    
    /**
    * Decodes a Uint8Array as a string. utf8 by default.
    */
    decode(data: Uint8Array): string
    
    /**
    * The encoding format set in the TextDecoder.
    
    * @readonly
    */
    encoding: string
    
}

/**
* Contains methods for Text encoding

* @example
* ```js var encoder = new TextEncoder()  let originalStr = "\u223e\u0020\u0045\u22c5\u0064\u0061\u0020\u003d\u0020\u0051" let encodedArr = encoder.encode(originalStr) print("encodedArr="+encodedArr) // result is [226,136,190,32,69,226,139,133,100,97] ```
*/
declare class TextEncoder extends ScriptObject {
    /**
    * Create a new instance of TextEncoder with utf-8 encoding.
    */
    constructor()
    
    /**
    * Encodes a string as Uint8Array. utf-8 by default.
    */
    encode(value: string): Uint8Array
    
    /**
    * Encodes and saves the `value` into the `result` array.
    */
    encodeInto(value: string, result: Uint8Array): void
    
    /**
    * The encoding format set in the TextEncoder.
    
    * @readonly
    */
    encoding: string
    
}

/**
* Fill settings used by several text related classes.

* Used in {@link Text}'s `textFill` property,
* {@link DropshadowSettings}' `fill` property, and
* {@link OutlineSettings}' `fill` property.

* @see Used By: {@link BackgroundSettings#fill}, {@link DropshadowSettings#fill}, {@link OutlineSettings#fill}, {@link Text#textFill}

* @example
* ```js
* // @input Component.Text textComponent

* script.textComponent.textFill.mode = TextFillMode.Solid;

* script.textComponent.textFill.color = new vec4(1, 0, 0, 1);
* ```
*/
declare class TextFill extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * If `mode` is set to `TextFillMode.Solid`, this will be used as the solid color used in drawing.
    */
    color: vec4
    
    /**
    * The tint that should be applied on top of the text. Useful when using texture fill mode.
    */
    colorTint: vec4
    
    /**
    * Controls which drawing method is used. Can switch between `Texture` mode (for drawing using a tiled texture) or `Solid` mode (for drawing a solid color).
    */
    mode: TextFillMode
    
    /**
    * If `mode` is set to `TextFillMode.Texture`, this will be used as the texture asset used in drawing.
    */
    texture: Texture
    
    /**
    * If `mode` is set to `TextFillMode.Texture`, this defines what type of stretching is used when the
    * Texture's aspect ratio doesn't match the drawing area's aspect ratio.
    */
    textureStretch: StretchMode
    
    /**
    * If `mode` is set to `TextFillMode.Texture`, this defines how many times the texture will tile across its drawing zone.
    */
    tileCount: number
    
    /**
    * If `mode` is set to `TextFillMode.Texture`, this defines what area should be used for tiling the texture.
    */
    tileZone: TileZone
    
}

/**
* Used in {@link TextFill}'s `mode` property. Controls which drawing method is used for the TextFill.

* @see Used By: {@link TextFill#mode}

* @example
* ```js
* // @input Component.Text textComponent
* // @input Asset.Texture texture

* script.textComponent.textFill.mode = TextFillMode.Texture;
* script.textComponent.textFill.texture = script.texture;
* ```
*/
declare enum TextFillMode {
    /**
    * Solid color will be used for drawing.
    */
    Solid,
    /**
    * Tiled texture will be used for drawing.
    */
    Texture
}

/**
* Declares the Input Framework (Text) permission for your Lens project.
*/
declare class TextInputModule extends Asset {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Provides access to the device's input system. Accessible through `global.textInputSystem`.

* @example
* ```
* var options = new TextInputSystem.KeyboardOptions();
* options.enablePreview = false;
* options.keyboardType = TextInputSystem.KeyboardType.Text;
* options.returnKeyType = TextInputSystem.ReturnKeyType.Done;

* global.textInputSystem.requestKeyboard(options);
* ```
*/
declare class TextInputSystem extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Requests the client to hide the keyboard and clear any existing text in the inivisible text view.
    */
    dismissKeyboard(): void
    
    /**
    * Requests the client to pop up the keyboard with specific type and return key type, fill the inivisible text view with the initial text, and set the selected text range. When user starts editing the text, the new text string and the new selected range are returned to the lens by invoking the onTextChanged function.
    */
    requestKeyboard(options: TextInputSystem.KeyboardOptions): void
    
    /**
    * Requests the client to set the editing ("cursor") position on the invisible text view.
    */
    setEditingPosition(position: number): void
    
    /**
    * Requests the client to set the selected text range on the invisible text view.
    */
    setSelectedTextRange(range: vec2): void
    
}

declare namespace TextInputSystem {
    /**
    * The settings used for how the input keyboard should work.
    
    * @see Used By: {@link TextInputSystem#requestKeyboard}
    
    * @example
    * ```
    * var options = new TextInputSystem.KeyboardOptions();
    * options.enablePreview = false;
    * options.keyboardType = TextInputSystem.KeyboardType.Text;
    * options.returnKeyType = TextInputSystem.ReturnKeyType.Done;
    
    * // Maintain the state of the keyboard
    * options.onTextChanged = function(text, range) {
    *     currText = text;
    * };
    
    * // When the keyboard returns, print the current text
    * options.onKeyboardStateChanged = function(isOpen) {
    *     if (!isOpen) {
    *         print(currText);
    *     }
    * };
    
    * script.createEvent("TapEvent").bind(function(){
    *     global.textInputSystem.requestKeyboard(options);
    * })
    * ```
    */
    class KeyboardOptions {
        /**
        * The type of keyboard to be used for input.
        */
        constructor()
        
        /**
        * Sets whether a preview should be visible above the input keyboard.
        */
        enablePreview: boolean
        
        /**
        * Initial selected text range to set on the inivisible text view.
        */
        initialSelectedRange: vec2
        
        /**
        * Initial text to fill into the invisible text view.
        */
        initialText: string
        
        /**
        * Requested keyboard type.
        */
        keyboardType: TextInputSystem.KeyboardType
        
        /**
        * Callback to be invoked when there is an error.
        */
        onError: (error: number, description: string) => void
        
        /**
        * Callback to be invoked when keyboard is shown or dismissed.
        */
        onKeyboardStateChanged: (keyboardIsOpen: boolean) => void
        
        /**
        * Callback to be invoked when the user presses the return key.
        */
        onReturnKeyPressed: () => void
        
        /**
        * Callback to be invoked every time the user presses a (non-return) key.
        */
        onTextChanged: (text: string, range: vec2) => void
        
        /**
        * Requested return key type.
        */
        returnKeyType: TextInputSystem.ReturnKeyType
        
    }

}

declare namespace TextInputSystem {
    /**
    * The different input style of keyboard input.
    
    * @see Used By: {@link TextInputSystem.KeyboardOptions#keyboardType}
    
    * @example
    * ```
    * var options = new TextInputSystem.KeyboardOptions();
    * options.keyboardType = TextInputSystem.KeyboardType.Text;
    
    * script.createEvent("TapEvent").bind(function(){
    *     global.textInputSystem.requestKeyboard(options);
    * })
    * ```
    */
    enum KeyboardType {
        /**
        * A keyboard type for entering normal text
        */
        Text,
        /**
        * A keyboard type for entering digits
        */
        Num,
        /**
        * A keyboard type for entering phone numbers
        */
        Phone,
        /**
        * A keyboard type for entering URLs
        */
        Url,
        /**
        * A keyboard type for entering passwords.
        
        * @wearableOnly
        */
        Password,
        /**
        * A keyboard type for entering pins.
        
        * @wearableOnly
        */
        Pin
    }

}

declare namespace TextInputSystem {
    /**
    * The return key style of keyboard input.
    
    * @see Used By: {@link TextInputSystem.KeyboardOptions#returnKeyType}
    
    * @example
    * ```
    * var options = new TextInputSystem.KeyboardOptions();
    * options.returnKeyType = TextInputSystem.ReturnKeyType.Done;
    
    * script.createEvent("TapEvent").bind(function(){
    *     global.textInputSystem.requestKeyboard(options);
    * })
    * ```
    */
    enum ReturnKeyType {
        /**
        * Return key label shows "Done"
        */
        Done,
        /**
        * Return key label shows "Go"
        */
        Go,
        /**
        * Return key label shows "Next"
        */
        Next,
        /**
        * Return key label shows "return"
        */
        Return,
        /**
        * Return key label shows "Search"
        */
        Search,
        /**
        * Return key label shows "Send"
        */
        Send
    }

}

/**
* Controls a text rendering texture. Can be accessed through the main rendering pass on a {@link Text} component.
* The properties here mirror those on Text.

* @example
* ```js
* // Sets the text to be used on a label's text rendering texture

* //@input Component.Label label

* var control = script.label.mainPass.baseTex.control;
* control.text = "New Text";
* ```
*/
declare class TextProvider extends TextureProvider {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The font used for rendering text.
    */
    fontAsset: Font
    
    /**
    * The color used for the outline effect.
    */
    outlineColor: vec4
    
    /**
    * The strength of the outline effect.
    */
    outlineSize: number
    
    /**
    * The color used for dropshadow.
    */
    shadowColor: vec4
    
    /**
    * The horizontal and vertical offset used for dropshadow.
    */
    shadowOffset: vec2
    
    /**
    * The font size being used.
    */
    size: number
    
    /**
    * The text being rendered.
    */
    text: string
    
    /**
    * The color for rendering text.
    */
    textColor: vec4
    
    /**
    * If enabled, adds a dropshadow to the text.
    */
    useDropshadow: boolean
    
    /**
    * If enabled, renders an outline around the text.
    */
    useOutline: boolean
    
}

declare class TextToSpeech {
    
    /** @hidden */
    protected constructor()
    
}

declare namespace TextToSpeech {
    /**
    * Provides the configuration for the {@link TextToSpeechModule}.   It is used to control the language of the generated voice, the voices style and pace.
    
    * @see Used By: {@link TextToSpeechModule#synthesize}
    * @see Returned By: {@link TextToSpeech.Options.create}
    
    * @example
    * ```js
    * var onTTSComplete = function(audioTrackAsset, eventArgs) {
    *     print("Synthesis complete!");
    * }
    
    * var onTTSError = function(error, description) {
    *     print("Synthesis error!");
    * }
    
    * var options = TextToSpeech.Options.create();
    * options.voiceName = TextToSpeech.VoiceNames.Sam;
    * options.voicePace = 100;
    * options.voiceStyle = TextToSpeech.VoiceStyles.Two;
    
    * script.ttsModule.synthesize("Text to speech example", options, onTTSComplete, onTTSError);
    * ```
    */
    class Options extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * The name of the voice. If not set, the service will choose the default voice based on the chosen language.  Supported voice names: `Sasha`
        */
        voiceName: string
        
        /**
        * Creates options for the {@link TextToSpeechModule}.
        */
        static create(): TextToSpeech.Options
        
    }

}

declare namespace TextToSpeech {
    /**
    * Provides a map of what phoneme is said at what time in the synthesized speech.
    
    * @example
    * ```
    * //@input Asset.TextToSpeechModule ttsModule
    
    * var onTTSComplete = function(audioTrackAsset, wordInfos, phonemeInfos) {
    *      for (var i = 0; i < wordInfos.length; i++) {
    *         print("word" + wordInfos[i].word);
    *         print("startTime" + wordInfos[i].startTime.toString());
    *         print("endTime" + wordInfos[i].endTime.toString());
    *      }
    *     for (var j = 0; j < phonemeInfos.length; j++) {
    *         print("phoneme" + phonemeInfos[j].phoneme);
    *         print("startTime" + phonemeInfos[j].startTime.toString());
    *         print("endTime" + phonemeInfos[j].endTime.toString());
    *      }
    *      print(audioTrackAsset);
    *      audioComponent = script.getSceneObject().createComponent('Component.AudioComponent');
    *      audioComponent.audioTrack = audioTrackAsset;
    *      audioComponent.play(1);
    * }
    
    * var onTTSError = function(error, description) {
    *     print(error);
    *     print(description);
    * }
    
    * var options = TextToSpeech.TextToSpeechOptions.create();
    * options.languageCode = "en_US";
    * options.voiceName = TextToSpeech.VoiceNames.Sasha;
    * options.voiceStyle = TextToSpeech.VoiceStyles.One;
    * options.voicePace = 75;
    
    * script.ttsModule.synthesize("show me you love dogs, without telling me you love dogs",
    *     options, onTTSComplete, onTTSError)
    * ```
    */
    class PhonemeInfo extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * Phoneme end time in milliseconds.
        
        * @readonly
        */
        endTime: number
        
        /**
        * Is the phonemes part of an abusive word.
        
        * @readonly
        */
        isAbusive: boolean
        
        /**
        * The phoneme found for the alloted time.
        
        * @readonly
        */
        phoneme: string
        
        /**
        * Phoneme start time in milliseconds.
        
        * @readonly
        */
        startTime: number
        
    }

}

declare namespace TextToSpeech {
    /**
    * The voice style on which the TextToSpeech will be synthesized, Varying from neutral style, to a more elaborated styles depending on the voice. Note that Sams voice still doesnt have more than the neutral style, this will be added throughout the coming releases.
    
    * @example
    * ```js
    * //@input Asset.TextToSpeechModule ttsModule
    
    * var onTTSComplete = function(audioTrackAsset, eventArgs) {
    *     print("Synthesis complete!");
    * }
    
    * var onTTSError = function(error, description) {
    *     print("Synthesis error!");
    * }
    
    * // Change to the voice name you want to use.
    * var options = TextToSpeech.Options.create();
    * options.voiceName = TextToSpeech.VoiceNames.Sam;
    
    * script.ttsModule.synthesize("Text to speech example", options, onTTSComplete, onTTSError);
    * ```
    */
    class VoiceNames {
        
        /** @hidden */
        protected constructor()
        
    }

}

declare namespace TextToSpeech {
    /**
    * Parameter returned in the `onTTSCompleteHandler` callback providing timing details for word pronunciation.
    
    * @example
    * ```js
    * var onTTSCompleteHandler = function(audioTrackAsset, wordInfos){
    * var wordInfosTxt = "Word Infos:";
    * for (var i = 0; i < wordInfos.length; i++) {
    *     wordInfosTxt += "\nWord: '" + wordInfos[i].word + "', startTime: " + wordInfos[i].startTime.toString() + ", endTime: " + wordInfos[i].endTime.toString();
    * });
    * ```
    */
    class WordInfo extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * The time in milliseconds when the word ended in the audio.
        
        * @readonly
        */
        endTime: number
        
        /**
        * The time in milliseconds when the word started in the audio.
        
        * @readonly
        */
        startTime: number
        
        /**
        * The words the synthesized audio was generated for (as text might be expanded during the synthesize process, there might be a slight variation between the input text and the words returned).
        
        * @readonly
        */
        word: string
        
    }

}

/**
* Allows generation of audio from a given text with a variety of options.

* @remarks

* You can use only one `TextToSpeechModule` in a Lens. However, its methods can be called multiple times in parallel if needed.

* @see {@link TextToSpeech.Options}
* @see [Text To Speech](https://developers.snap.com/lens-studio/features/voice-ml/text-to-speech) guide.

* @example
* ```js
* //@input Asset.TextToSpeechModule textToSpeech

* var onTTSErrorHandler = function(error, description){
*     print('error: ' + error + " desc: " + description);
* }

* var onTTSCompleteHandler = function(audioTrackAsset, wordInfos){
*     var additionalInformation = "Successfully synthesized audio\n\n";
*     var wordInfosTxt = "Word Infos:";
*     for (var i = 0; i < wordInfos.length; i++) {
*         wordInfosTxt += "\nWord: '" + wordInfos[i].word + "', startTime: " + wordInfos[i].startTime.toString() + ", endTime: " + wordInfos[i].endTime.toString();
*     }
*     print(additionalInformation + wordInfosTxt);
*     var audioComponent = script.getSceneObject().createComponent('Component.AudioComponent');
*     audioComponent.audioTrack = audioTrackAsset;
*     audioComponent.play(1);
* }

* script.textToSpeech.synthesize("show me you love cats, without telling me you love cats!" , TextToSpeech.Options.create(), onTTSCompleteHandler, onTTSErrorHandler);
* ```
*/
declare class TextToSpeechModule extends Asset {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Generates speech {@link AudioTrackAsset} of a given text.
    
    * `Input` should be the text to synthesize (Currently supports text in English only. Non English characters will be stripped).
    * `Options` should be a {@link TextToSpeech.Options}.
    * `onTTsComplete` should be a callback function which will be called once the audio generation is completed.
    
    * The callback will receive two parameters: the generated audio file ({@link AudioTrackAsset}) and maps of word/timing ({@link TextToSpeech.WordInfo}).
    
    * `onTTSError` should be a callback function which will be called if there is an error. This callback will receive a message of the error code and its description.
    */
    synthesize(input: string, options: TextToSpeech.Options, onTTSComplete: (audioTrackAsset: AudioTrackAsset, wordInfo: TextToSpeech.WordInfo[], phonemeInfo: TextToSpeech.PhonemeInfo[], voiceStyle: any) => void, onTTSError: (error: number, description: string) => void): void
    
}

/**
* Represents a texture file asset.

* @see Used By: {@link Base64.decodeTextureAsync}, {@link Base64.encodeTextureAsync}, {@link Camera#inputTexture}, {@link Camera#maskTexture}, {@link Camera#renderTarget}, {@link Camera.BaseRenderTarget#inputTexture}, {@link Camera.BaseRenderTarget#maskTexture}, {@link Camera.BaseRenderTarget#targetTexture}, {@link ConnectedLensModule#shareSession}, {@link CropTextureProvider#inputTexture}, {@link DepthStencilRenderTargetProvider#inputTexture}, {@link DepthStencilRenderTargetProvider#maskTexture}, {@link FaceMaskVisual#customMaskOnMouthClosed}, {@link ImageFrame#texture}, {@link InputBuilder#setInputTexture}, {@link InputPlaceholder#texture}, {@link InternetModule#createWebView}, {@link LightSource#diffuseEnvmapTexture}, {@link LightSource#specularEnvmapTexture}, {@link OutputPlaceholder#texture}, {@link Pass#baseTex}, {@link PassPropertyOverrides#baseTex}, {@link ProceduralTextureProvider.createFromTexture}, {@link RectangleSetter#cropTexture}, {@link RemoteMediaModule#loadResourceAsImageTexture}, {@link RemoteMediaModule#loadResourceAsVideoTexture}, {@link RenderTargetProvider#inputTexture}, {@link SamplerWrapper#texture}, {@link ScanModule#scanTarget}, {@link ScriptScene#captureTarget}, {@link ScriptScene#liveOverlayTarget}, {@link ScriptScene#liveTarget}, {@link TensorMath.textureToGrayscale}, {@link TextFill#texture}, {@link TextureTrackingScope#texture}, {@link VideoRecorder.Options#sourceTexture}
* @see Returned By: {@link CameraModule#requestCamera}, {@link LocationTextureProvider.create}, {@link MapModule#createMapTextureProvider}, {@link ProceduralTextureProvider.createFromTexture}, {@link ProceduralTextureProvider.createWithFormat}, {@link ScriptScene#createDepthStencilRenderTargetTexture}, {@link ScriptScene#createRenderTargetTexture}, {@link Texture#copyFrame}

* @example
* ```
* // Print the texture's height, width
* // Change the texture's name and print it out
* //@input Asset.Texture texture

* print("Width = " + script.texture.getWidth().toString());
* print("Height = " + script.texture.getHeight().toString());

* ```

* ```js
* // Takes a screenshot of device camera input and applies it to meshVisual
* //@input Asset.Texture deviceCameraTexture
* //@input Component.MeshVisual meshVisual
* var textureCopy = script.deviceCameraTexture.copyFrame();
* script.meshVisual.mainPass.baseTex = textureCopy;
* ```
*/
declare class Texture extends Asset {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns a Texture that captures the current state of this Texture Asset.
    */
    copyFrame(): Texture
    
    /**
    * Creates a marker asset from the texture.
    */
    createMarkerAsset(): MarkerAsset
    
    /**
    * Returns the Colorspace of the Texture.
    
    * @deprecated
    */
    getColorspace(): Colorspace
    
    getFormat(): TextureFormat
    
    /**
    * Returns the height of the texture.
    */
    getHeight(): number
    
    /**
    * Returns the width of the texture.
    */
    getWidth(): number
    
    /**
    * The TextureProvider for the texture, which may control things like animation depending on the texture type.
    * See also: {@link AnimatedTextureFileProvider}.
    */
    control: TextureProvider
    
}

/**


* @see Used By: {@link ProceduralTextureProvider.createWithFormat}
* @see Returned By: {@link Texture#getFormat}
*/
declare enum TextureFormat {
    R8Unorm,
    RG8Unorm,
    RGB8Unorm,
    RGBA8Unorm,
    R16Unorm,
    RG16Unorm,
    RGBA16Unorm,
    R8Snorm,
    RG8Snorm,
    RGBA8Snorm,
    R16Snorm,
    RG16Snorm,
    RGBA16Snorm,
    R8Uint,
    RG8Uint,
    RGBA8Uint,
    R16Uint,
    RG16Uint,
    RGBA16Uint,
    R32Uint,
    RG32Uint,
    RGBA32Uint,
    R8Sint,
    RG8Sint,
    RGBA8Sint,
    R16Sint,
    RG16Sint,
    RGBA16Sint,
    R32Sint,
    RG32Sint,
    RGBA32Sint,
    R16Float,
    RG16Float,
    RGBA16Float,
    R32Float,
    RG32Float,
    RGBA32Float,
    BGRA8Unorm,
    RGBA8Srgb,
    RGB10A2Unorm,
    RGB10A2Uint,
    RG11B10Float
}

/**
* The base class for specialized Texture providers. Can be accessed through {@link Texture.control}.

* @see Used By: {@link MediaPickerTextureProvider#imageControl}, {@link Texture#control}
*/
declare class TextureProvider extends Provider {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns the texture's aspect ratio, which is calculated as width / height.
    */
    getAspect(): number
    
    /**
    * Returns the height of the texture in pixels.
    */
    getHeight(): number
    
    /**
    * Returns the width of the texture in pixels.
    */
    getWidth(): number
    
}

/**
* @deprecated
*/
declare enum TextureStretchMode {
    /**
    * @deprecated
    */
    Fit,
    /**
    * @deprecated
    */
    Fill,
    /**
    * @deprecated
    */
    Stretch,
    /**
    * @deprecated
    */
    FitHeight,
    /**
    * @deprecated
    */
    FitWidth
}

/**
* Provides a {@link Texture} to do face tracking on for entities like {@link Head}, {@link FaceStretch}, and {@link FaceMesh}.

* @see Used By: {@link Camera#devicePropertiesSource}, {@link FaceTrackingScope#parentScope}, {@link PersonTrackingScope#parentScope}
*/
declare class TextureTrackingScope extends TrackingScope {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The texture on which tracking should be done.
    */
    texture: Texture
    
}

/**
* Defines the bounding area used for texture tiling with {@link TextFill}'s `tileZone` property.

* @see Used By: {@link TextFill#tileZone}

* @example
* ```js
* // @input Component.Text text
* // @input Asset.Texture texture

* script.text.textFill.mode = TextFillMode.Texture;
* script.text.textFill.texture = script.texture;
* script.text.textFill.tileZone = TileZone.Character;
* script.text.textFill.tileCount = 1;
* ```
*/
declare enum TileZone {
    /**
    * The attached {@link ScreenTransform} bounding rectangle is used for texture tiling
    */
    Rect,
    /**
    * The Text component's drawn area (extents) is used for texture tiling
    */
    Extents,
    /**
    * Each character uses its own drawn area for texture tiling
    */
    Character,
    /**
    * The position of each character in screen space is used for tiling
    */
    Screen
}

/**
* This provider is returned by `global.touchSystem` and allows your lens to handle any touches on the screen, and optionally let certain touch types to pass through (let Snapchat handle the touch).

* Possible TouchType exception values:

* "TouchTypeNone"
* "TouchTypeTouch"
* "TouchTypeTap"
* "TouchTypeDoubleTap"
* "TouchTypeScale"
* "TouchTypePan"
* "TouchTypeSwipe"

* @example
* ```js
* // Enable full screen touches
* global.touchSystem.touchBlocking = true;

* // Allow double-tap to be passed through to Snapchat to flip the camera.
* global.touchSystem.enableTouchBlockingException("TouchTypeDoubleTap", true);

* // Or alternatively enable "TouchTypeDoubleTap" using mask
* var mask = global.touchSystem.touchBlockingExceptionMask;
* mask = global.touchSystem.composeTouchBlockingExceptionMask(mask, "TouchTypeDoubleTap");
* global.touchSystem.touchBlockingExceptionMask = mask;
* ```
*/
declare class TouchDataProvider extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns a copy of `currentMask` with the `newException` flag set to true.
    */
    composeTouchBlockingExceptionMask(currentMask: number, newException: string): number
    
    /**
    * Allow or stop allowing a certain `TouchType` to pass through your lens. Useful for allowing Snapchat to handle certain `TouchType`, e.g. allowing `TouchTypeDoubleTap` to flip the camera.
    */
    enableTouchBlockingException(exception: string, enable: boolean): void
    
    /**
    * Set your lens to handle touches on the screen, preventing default Snapchat touch behavior from occuring. Useful for enabling full screen touches without any touch components. It is similar to creating a plane the size of the screen in front of the camera.
    */
    touchBlocking: boolean
    
    /**
    * The current touch mask.
    */
    touchBlockingExceptionMask: number
    
}

/**
* Triggered when a touch event ends.

* @example
* ```
* // Prints "touch ended" when the touch input ends
* var event = script.createEvent("TouchEndEvent");
* event.bind(function(eventData)
* {
*     print("touch ended");
* });
* ```
*/
declare class TouchEndEvent extends SceneObjectEvent {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns the ID of this specific touch. Useful for distinguishing between touches when multiple are occurring simultaneously.
    */
    getTouchId(): number
    
    /**
    * Returns the normalized 2D screen position of the users touch.
    * The normalized coordinates range from ([0-1], [0-1]), (0,0) being top-left and (1,1) being bottom-right.
    */
    getTouchPosition(): vec2
    
}

/**
* @example
* ```js
* //@input Component.InteractionComponent interactionComponent

* // Subscribe to the onTouchEnd event
* var onTouchEndEvent = script.interactionComponent.onTouchEnd.add(function(touchEndEventArgs){
*     print("Touch End! Touched at: " + touchEndEventArgs.position + " for touchId: " + touchEndEventArgs.touchId);
* });

* // Unsubscribe from the onTouchEnd event
* script.interactionComponent.onTouchEnd.remove(onTouchEndEvent);
* ```
*/
declare class TouchEndEventArgs extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The position of the touch on the screen. [0,0] being top left, and [1,1] being bottom right.
    
    * @readonly
    */
    position: vec2
    
    /**
    * Returns the ID of this specific touch. Useful for distinguishing between touches when multiple are occurring simultaneously.
    
    * @readonly
    */
    touchId: number
    
}

/**
* Triggered when a touch position on the screen is moved.

* @example
* ```
* // Prints "touch move" when the touch position on the screen has moved
* var event = script.createEvent("TouchMoveEvent");
* event.bind(function(eventData)
* {
*     print("touch move");
* });
* ```
*/
declare class TouchMoveEvent extends SceneObjectEvent {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns the ID of this specific touch. Useful for distinguishing between touches when multiple are occurring simultaneously.
    */
    getTouchId(): number
    
    /**
    * Returns the normalized 2D screen position of the users touch.
    * The normalized coordinates range from ([0-1], [0-1]), (0,0) being top-left and (1,1) being bottom-right.
    */
    getTouchPosition(): vec2
    
}

/**
* Arguments used with the `InteractionComponent.onTouchMove` event.

* @example
* ```js
* //@input Component.InteractionComponent interactionComponent

* // Subscribe to the onTouchMove event
* var onTouchMoveEvent = script.interactionComponent.onTouchMove.add(function(touchMoveEventArgs){
*     print("Touch Move! Touched at: " + touchMoveEventArgs.position + " for touchId: " + touchMoveEventArgs.touchId);
* });

* // Unsubscribe from the onTouchMove event
* script.interactionComponent.onTouchMove.remove(onTouchMoveEvent);
* ```
*/
declare class TouchMoveEventArgs extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The position of the touch on the screen. [0,0] being top left, and [1,1] being bottom right.
    
    * @readonly
    */
    position: vec2
    
    /**
    * Returns the ID of this specific touch. Useful for distinguishing between touches when multiple are occurring simultaneously.
    
    * @readonly
    */
    touchId: number
    
}

/**
* Triggered when a touch event starts.

* @example
* ```js
* // Prints the touch position when a screen touch has started
* var event = script.createEvent("TouchStartEvent");
* event.bind(function(eventData)
* {
*     var touchedPos = eventData.getTouchPosition();
*     print("touch started at " + touchedPos.x + " " + touchedPos.y);
* });
* ```
*/
declare class TouchStartEvent extends SceneObjectEvent {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns the ID of this specific touch. Useful for distinguishing between touches when multiple are occurring simultaneously.
    */
    getTouchId(): number
    
    /**
    * Returns the normalized 2D screen position of the users touch.
    * The normalized coordinates range from ([0-1], [0-1]), (0,0) being top-left and (1,1) being bottom-right.
    */
    getTouchPosition(): vec2
    
}

/**
* Arguments used with the `InteractionComponent.onTouchStart` event.

* @example
* ```js
* //@input Component.InteractionComponent interactionComponent

* // Subscribe to the onTouchStart event
* var onTouchStartEvent = script.interactionComponent.onTouchStart.add(function(touchStartEventArgs){
*     print("Touch Start! Touched at: " + touchStartEventArgs.position + " for touchId: " + touchStartEventArgs.touchId);
* });

* // Unsubscribe from the onTouchStart event
* script.interactionComponent.onTouchStart.remove(onTouchStartEvent);
* ```
*/
declare class TouchStartEventArgs extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The position of the touch on the screen. [0,0] being top left, and [1,1] being bottom right.
    
    * @readonly
    */
    position: vec2
    
    /**
    * Returns the ID of this specific touch. Useful for distinguishing between touches when multiple are occurring simultaneously.
    
    * @readonly
    */
    touchId: number
    
}

/**


* @see Used By: {@link WebPageTextureProvider#touch}
*/
declare enum TouchState {
    Began,
    Moved,
    Ended,
    Cancelled
}

/**
* Represents a mesh generated by world tracking. Only available when world mesh tracking is supported and enabled.

* @see Used By: {@link TrackedMeshHitTestResult#mesh}

* @example
* ```javascript
* script.createEvent("WorldTrackingMeshesAddedEvent").bind(onMeshesAdded);

* function onMeshesAdded(eventData) {
*     var trackedMeshes = eventData.getMeshes();
*     print(trackedMeshes.length + " meshes were added");

*     for (var i = 0; i < trackedMeshes.length; i++) {
*         print("TrackedMesh is valid " + trackedMeshes[i].isValid);
*         print("Transformation matrix " + trackedMeshes[i].transform);
*         var m = trackedMeshes[i].transform;

*         var pos = m.column3;
*         print("Position " + pos);

*         var rot = quat.lookAt(
*             new vec3(m.column2.x,m.column2.y,m.column2.z),
*             new vec3(m.column1.x,m.column1.y,m.column1.z)
*         );
*         print("Rotation " + rot);
*         var s = new vec3(
*             m.column0.length,
*             m.column1.length,
*             m.column2.length
*         );
*         print("Scale " + s);
*     }
* }
* ```
*/
declare class TrackedMesh extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns whether the tracked mesh is valid.
    
    * @readonly
    */
    isValid: boolean
    
    /**
    * Returns the World Transformation matrix of the detected mesh.
    
    * @readonly
    */
    transform: mat4
    
}

/**
* Classifications of mesh face. See {@link TrackedMeshHitTestResult}.

* @see Used By: {@link TrackedMeshHitTestResult#classification}
*/
declare enum TrackedMeshFaceClassification {
    /**
    * None
    */
    None,
    /**
    * Wall
    */
    Wall,
    /**
    * Floor
    */
    Floor,
    /**
    * Ceiling
    */
    Ceiling,
    /**
    * Table
    */
    Table,
    /**
    * Seat
    */
    Seat,
    /**
    * Window
    */
    Window,
    /**
    * Door
    */
    Door
}

/**
* Provides histogram information about tracked world mesh faces in a given area. Returned by `DeviceTracking.calculateWorldMeshHistogram()`.

* @see Returned By: {@link DeviceTracking#calculateWorldMeshHistogram}
*/
declare class TrackedMeshHistogramResult extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Average normal direction, in world space, of the mesh faces.
    
    * @readonly
    */
    avgNormal: vec3
    
    /**
    * Array of relative proportions for each classification, in the order described below. The values all add up to a total of 1.0.
    
    * The classification value order is:
    * 0: None
    * 1: Wall
    * 2: Floor
    * 3: Ceiling
    * 4: Table
    * 5: Seat
    * 6: Window
    * 7: Door
    
    * @readonly
    */
    histogram: number[]
    
}

/**
* Provides information about a TrackedMesh surface hit during a raycast. Is returned in an array when calling `DeviceTracking.hitTestWorldMesh()` or `DeviceTracking.raycastWorldMesh()`.
*/
declare class TrackedMeshHitTestResult extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns the classification of the mesh face at the intersection point.
    
    * @readonly
    */
    classification: TrackedMeshFaceClassification
    
    /**
    * Returns the TrackedMesh that was hit.
    
    * @readonly
    */
    mesh: TrackedMesh
    
    /**
    * Returns the world space normal direction of the intersection point.
    
    * @readonly
    */
    normal: vec3
    
    /**
    * Returns the world space position of the intersection point.
    
    * @readonly
    */
    position: vec3
    
}

/**
* A representation for plane detected by native tracking. Can be used with TrackedPoint.

* @example
* ```
* // @input Component.RenderMeshVisual renderMeshVisual

* script.createEvent("WorldTrackingPlanesAddedEvent").bind(function (eventData) {
*     var eventPlanes = eventData.getPlanes();

*     for (var i = 0; i < eventPlanes.length; i++) {
*       var eventPlane = eventPlanes[i];

*       script.renderMeshVisual.mesh = eventPlane.mesh;
*     }
* });
* ```
*/
declare class TrackedPlane extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Whether the detected plane is still available to utilize
    
    * @readonly
    */
    isValid: boolean
    
    /**
    * A coarse mesh describing the shape of the tracked plane. Can be displayed by RenderMeshVisual
    
    * @readonly
    */
    mesh: RenderMesh
    
    /**
    * Returns the orientation of the detected plane, either vertical or horizontal.
    
    * @readonly
    */
    orientation: TrackedPlaneOrientation
    
    /**
    * The center point of the detected plane
    
    * @readonly
    */
    pivot: vec3
    
    /**
    * The size of the detected plane, where it is described as width, 0, depth
    
    * @readonly
    */
    size: vec3
    
    /**
    * The position, rotation, and scale of the detected plane
    
    * @readonly
    */
    transform: mat4
    
}

/**


* @see Used By: {@link TrackedPlane#orientation}
*/
declare enum TrackedPlaneOrientation {
    Horizontal,
    Vertical
}

/**
* A point on the real world not attached to any detected plane.

* @see Used By: {@link TrackedPointComponent#trackedPoint}
* @see Returned By: {@link DeviceTracking#createTrackedWorldPoint}
*/
declare class TrackedPoint extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Provides the orientation of the tracked point.
    
    * @readonly
    */
    orientation: quat
    
    /**
    * Provides the world position of the tracked point.
    
    * @readonly
    */
    position: vec3
    
}

/**
* Allows you to bind the position and rotation of an object with this component to a {@link TrackedPoint}.
*/
declare class TrackedPointComponent extends Component {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns whether the trackedPoint is valid.
    
    * @readonly
    */
    isValid: boolean
    
    /**
    * The {@link TrackedPoint} which this component will copy the position and rotation from.
    */
    trackedPoint: TrackedPoint
    
}

/**
* The scope, such as the texture it should track against, in which a tracker, such as {@link FaceMaskVisual}, {@link EyeColorVisual}, {@link BodyDepthTextureProvider}, should be applied.
*/
declare class TrackingScope extends Asset {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Controls the position, rotation, and scale of a {@link SceneObject}.  Every SceneObject automatically has a Transform attached.

* @see Returned By: {@link Component#getTransform}, {@link SceneObject#getTransform}

* @example
* ```js
* // Move attached Transform 1 unit up
* var transform = script.getTransform();
* var pos = transform.getWorldPosition();
* pos.y += 1;
* transform.setWorldPosition(pos);
* ```

* ```js
* // Bind to "Frame Updated" to rotate the object every frame
* var transform = script.getTransform();
* var rotation = transform.getLocalRotation();
* var rotateBy = quat.angleAxis(Math.PI*getDeltaTime(), vec3.up());
* rotation = rotation.multiply(rotateBy);
* transform.setLocalRotation(rotation);
* ```
*/
declare class Transform extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns the Transform's world-to-local transformation matrix.
    */
    getInvertedWorldTransform(): mat4
    
    /**
    * Returns the Transform's position relative to its parent.
    */
    getLocalPosition(): vec3
    
    /**
    * Returns the Transform's rotation relative to its parent.
    */
    getLocalRotation(): quat
    
    /**
    * Returns the Transform's scale relative to its parent.
    */
    getLocalScale(): vec3
    
    /**
    * Returns the SceneObject the Transform is attached to.
    */
    getSceneObject(): SceneObject
    
    /**
    * Returns the Transform's position relative to the world.
    */
    getWorldPosition(): vec3
    
    /**
    * Returns the Transform's rotation relative to the world.
    */
    getWorldRotation(): quat
    
    /**
    * Returns the Transform's scale relative to the world.
    */
    getWorldScale(): vec3
    
    /**
    * Returns the Transform's local-to-world transformation matrix.
    */
    getWorldTransform(): mat4
    
    /**
    * Sets the Transform's position relative to its parent.
    */
    setLocalPosition(pos: vec3): void
    
    /**
    * Sets the Transform's rotation relative to its parent.
    */
    setLocalRotation(rotation: quat): void
    
    /**
    * Sets the Transform's scale relative to its parent.
    */
    setLocalScale(scale: vec3): void
    
    /**
    * Sets the Transform's local transformation matrix.
    */
    setLocalTransform(transformMat: mat4): void
    
    /**
    * Sets the Transform's position relative to the world.
    */
    setWorldPosition(pos: vec3): void
    
    /**
    * Sets the Transform's rotation relative to the world.
    */
    setWorldRotation(rotation: quat): void
    
    /**
    * Sets the Transform's scale relative to the world.
    * This may produce lossy results when parent objects are rotated, so use `setLocalScale()` instead if possible.
    */
    setWorldScale(scale: vec3): void
    
    /**
    * Sets the Transform's transformation matrix.
    */
    setWorldTransform(transformMat: mat4): void
    
    /**
    * Returns the Transform's back directional vector.
    
    * @readonly
    */
    back: vec3
    
    /**
    * Returns the Transform's down directional vector.
    
    * @readonly
    */
    down: vec3
    
    /**
    * Returns the Transform's forward directional vector.
    
    * @readonly
    */
    forward: vec3
    
    /**
    * Returns the Transform's left directional vector.
    
    * @readonly
    */
    left: vec3
    
    /**
    * Returns the Transform's right directional vector.
    
    * @readonly
    */
    right: vec3
    
    /**
    * When scaling a parent with segment scale enabled, instead of scaling child objects, it creates position offsets. This setting is used when exporting from certain 3d authoring tools.
    */
    segmentScaleCompensate: boolean
    
    /**
    * Returns the Transform's up directional vector.
    
    * @readonly
    */
    up: vec3
    
}

/**
* Applies additional transform processing on data for InputPlaceholders and OutputPlaceholders used with MLComponent.
* For more information, see the [MLComponent Overview](https://developers.snap.com/lens-studio/features/snap-ml/ml-component/ml-component-overview).

* @see Used By: {@link BasePlaceholder#transformer}, {@link InputBuilder#setTransformer}, {@link OutputBuilder#setTransformer}
* @see Returned By: {@link TransformerBuilder#build}

* @example
* ```js
* var transformer = MachineLearning.createTransformerBuilder()
*     .setVerticalAlignment(VerticalAlignment.Center)
*     .setHorizontalAlignment(HorizontalAlignment.Center)
*     .setRotation(TransformerRotation.Rotate180)
*     .setFillColor(new vec4(0, 0, 0, 1))
*     .build();

* var outputBuilder = MachineLearning.createOutputBuilder();
* outputBuilder.setName("probs");
* outputBuilder.setShape(new vec3(1, 1, 200));
* outputBuilder.setOutputMode(MachineLearning.OutputMode.Data);
* outputBuilder.setTransformer(transformer);
* var outputPlaceholder = outputBuilder.build();
* ```

* ```js
* //@input Component.MLComponent mlComponent
* //@input string outputName


* script.mlComponent.onLoadingFinished = onLoadingFinished;

* function onLoadingFinished(){
*     script.createEvent("UpdateEvent").bind(onUpdate);
* }

* function onUpdate() {
*     var outputTransformer = script.mlComponent.getOutput(script.outputName).transformer;
*     if (outputTransformer != null) {
*         var transformMatrix = outputTransformer.matrix;
*         print(transformMatrix);
*     }
* }
* ```
*/
declare class Transformer extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Inverse transformation matrix of this Transformer.
    
    * @readonly
    */
    inverseMatrix: mat3
    
    /**
    * Transformation matrix of this Transformer.
    
    * @readonly
    */
    matrix: mat3
    
}

/**
* Builds Transformer objects for MLComponent.

* @see Returned By: {@link MachineLearning.createTransformerBuilder}, {@link TransformerBuilder#setFillColor}, {@link TransformerBuilder#setFlipX}, {@link TransformerBuilder#setFlipY}, {@link TransformerBuilder#setHorizontalAlignment}, {@link TransformerBuilder#setRotation}, {@link TransformerBuilder#setStretch}, {@link TransformerBuilder#setVerticalAlignment}

* @example
* ```js
* var transformer = MachineLearning.createTransformerBuilder()
*     .setVerticalAlignment(VerticalAlignment.Center)
*     .setHorizontalAlignment(HorizontalAlignment.Center)
*     .setRotation(TransformerRotation.Rotate180)
*     .setFillColor(new vec4(0, 0, 0, 1))
*     .build();

* var outputBuilder = MachineLearning.createOutputBuilder();
* outputBuilder.setName("probs");
* outputBuilder.setShape(new vec3(1, 1, 200));
* outputBuilder.setOutputMode(MachineLearning.OutputMode.Data);
* outputBuilder.setTransformer(transformer);
* var outputPlaceholder = outputBuilder.build();
* ```
*/
declare class TransformerBuilder extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Builds and returns a Transformer object based on the current settings.
    */
    build(): Transformer
    
    /**
    * Sets the fill value used.
    */
    setFillColor(color: vec4): TransformerBuilder
    
    /**
    * Enables or disables horizontal flipping.
    */
    setFlipX(value: boolean): TransformerBuilder
    
    /**
    * Enables or disables vertical flipping.
    */
    setFlipY(value: boolean): TransformerBuilder
    
    /**
    * Sets the horizontal alignment type used.
    */
    setHorizontalAlignment(mode: HorizontalAlignment): TransformerBuilder
    
    /**
    * Sets the rotation type used.
    */
    setRotation(mode: TransformerRotation): TransformerBuilder
    
    /**
    * Sets the stretching type used.
    */
    setStretch(value: boolean): TransformerBuilder
    
    /**
    * Sets the vertical alignment type used.
    */
    setVerticalAlignment(mode: VerticalAlignment): TransformerBuilder
    
}

/**
* Rotation types used by TransformBuilder.

* @see Used By: {@link TransformerBuilder#setRotation}

* @example
* ```js
* var transformer = MachineLearning.createTransformerBuilder()
*     .setVerticalAlignment(VerticalAlignment.Center)
*     .setHorizontalAlignment(HorizontalAlignment.Center)
*     .setRotation(TransformerRotation.Rotate180)
*     .setFillColor(new vec4(0, 0, 0, 1))
*     .build();
* ```
*/
declare enum TransformerRotation {
    /**
    * No rotation
    */
    None,
    /**
    * Rotates by 90 degrees
    */
    Rotate90,
    /**
    * Rotates by 180 degrees
    */
    Rotate180,
    /**
    * Rotates by 270 degrees
    */
    Rotate270
}

/**
* Triangle hit information, available when a ray cast intersects a collision mesh.

* @see Used By: {@link RayCastHit#triangle}

* @example
* ```js
* var probe = Physics.createGlobalProbe();

* function printHit(hit) {
*     var tri = hit.triangle;
*     if (tri) {
*         print("triangle: " + tri);
*         print("mesh: " + tri.mesh);
*         print("index: " + tri.index);
*         print("vertexIndices: " + tri.vertexIndices);
*         print("vertexPositions: " + tri.vertexPositions);
*         print("barycentricCoordinate: " + tri.barycentricCoordinate);
*     }
* }

* probe.rayCast(fromPos, toPos, function onHitNearest(hit) {
*     if (hit) {
*         printHit(hit);
*     }
* });
* ```
*/
declare class TriangleHit extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Barycentric coordinate of the hit on the triangle. This is used to interpolate values over the triangle surface. Essentially, these are weights for each of the 3 triangle vertices. For example, you can compute the hit position from vertex positions as: (V0\*b0 + V1\*b1 + V2\*b2). We already have the hit position in RayCastHit, but 'V' can be any interpolated value, such as color or texture coordinate.
    
    * @readonly
    */
    barycentricCoordinate: vec3
    
    /**
    * Index of the triangle in the mesh. Note, this is the index of the triangle in the collision mesh, which won't necessarily correspond to the same index on the render mesh (depending on type and bake settings).
    
    * @readonly
    */
    index: number
    
    /**
    * Collision mesh containing the triangle.
    
    * @readonly
    */
    mesh: CollisionMesh
    
    /**
    * Vertex indices in the mesh. Note, these are the indices of the vertices in the collision mesh, which won't necessarily correspond to the same indices in the render mesh (depending on type and bake settings).
    
    * @readonly
    */
    vertexIndices: number[]
    
    /**
    * World-space vertex positions.
    
    * @readonly
    */
    vertexPositions: vec3[]
    
}

/**
* Gets called when the user triggers the primary input on their device. For example touch on touch screens.
*/
declare class TriggerPrimaryEvent extends SceneObjectEvent {
    
    /** @hidden */
    protected constructor()
    
    /**
    * @readonly
    */
    position: vec2
    
}

/**
* Arguments used with the `InteractionComponent.onTriggerPrimary` event.

* @example
* ```js
* //@input Component.InteractionComponent interactionComponent

* // Subscribe to the onTriggerPrimary event
* var onTriggerPrimaryEvent = script.interactionComponent.onTriggerPrimary.add(function(triggerPrimaryEventArgs){
*     print("Trigger Primary!");
* });

* // Unsubscribe from the onTriggerPrimary event
* script.interactionComponent.onTriggerPrimary.remove(onTriggerPrimaryEvent);
* ```
*/
declare class TriggerPrimaryEventArgs extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Triggered when the lens turns off.

* @example
* ```js
* // Prints "turn off" when the lens turns off
* var event = script.createEvent("TurnOffEvent");
* event.bind(function (eventData)
* {
*     print("turn off");
* });
* ```
*/
declare class TurnOffEvent extends SceneEvent {
    
    /** @hidden */
    protected constructor()
    
}

/**
* This event has been **deprecated**, please use the [OnStartEvent](https://developers.snap.com/api/classes/OnStartEvent) instead.

* Triggered when the lens turns on.

* @deprecated

* @example
* ```js
* // Prints "turn on" when the lens starts
* var event = script.createEvent("TurnOnEvent");
* event.bind(function (eventData)
* {
* 	print("turn on");
* });
* ```
*/
declare class TurnOnEvent extends SceneEvent {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Triggered every frame.

* @example
* ```js
* // Move this SceneObject up every frame with a speed of 5 units per second
* var transform = script.getTransform();
* var event = script.createEvent("UpdateEvent");
* event.bind(function (eventData)
* {
* 	var pos = transform.getLocalPosition();
* 	pos.y += eventData.getDeltaTime() * 5.0;
* 	transform.setLocalPosition(pos);
* });
* ```
*/
declare class UpdateEvent extends SceneEvent {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns the time elapsed (in seconds) between the current frame and previous frame.
    */
    getDeltaTime(): number
    
}

/**
* Provides a render object of the upper body, without the head. Unlike `BodyMesh` which handles the whole body, this model is optimized to work better with `FaceMesh` and selfie use cases.

* @example
* ```js
* function createUpperBodyRenderObjectProvider() {
*    var upperBodyProvider = global.scene.createResourceProvider("Provider.UpperBodyRenderObjectProvider");
*    upperBodyProvider.faceIndex = script.faceIndex;
*    return upperBodyProvider;
* }

* // 3D Upper Body Mesh
* var meshVisual = script.getSceneObject().getComponent("Component.RenderMeshVisual");
* meshVisual.mesh = global.assetSystem.createAsset("Asset.RenderMesh");
* meshVisual.mesh.control = createUpperBodyRenderObjectProvider();
* meshVisual.addMaterial(script.material);
* ```
*/
declare class UpperBodyRenderObjectProvider extends MeshRenderObjectProvider {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The index of the face whose upper body you want to render. The first face detected is `0`, the second `1`, and so on.
    */
    faceIndex: number
    
}

/**
* Configures 3D Upper Body Tracking for the {@link ObjectTracking3D} component.

* @remarks
* It is optimized to track with the face and in selfie use cases.

* @see [Upper Body Tracking](https://developers.snap.com/lens-studio/features/ar-tracking/body/upper-body-tracking) guide.

* @example
* ```js
* function createUpperBodyTrackingAsset() {
*    var asset = global.assetSystem.createAsset("Asset.UpperBodyTrackingAsset");
*    return asset;
* }

* // 3D Upper Body Tracking (skeleton)
* var object3D = script.getSceneObject().getComponent("Component.ObjectTracking3D");
* object3D.trackingAsset = createUpperBodyTrackingAsset();
* object3D.objectIndex = script.faceIndex;
* ```
*/
declare class UpperBodyTrackingAsset extends Object3DAsset {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Provides information about the user such as display name, birthday, and current weather. Accessible through `global.userContextSystem`.

* All callbacks will execute as soon as the requested information is available. In some rare cases, the requested information may be completely unavailable, and the callback will never occur.

* Note that formatted or localized strings may appear differently to users depending on their region.

* @example
* ```js
* global.userContextSystem.requestDisplayName(function(displayName) {
*    print("Hello " + displayName);
* });

* global.userContextSystem.requestWeatherLocalized(function(weatherText) {
*     print("The weather outside is: " + weatherText);
* });

* global.userContextSystem.requestAltitudeFormatted(function(altitude) {
*    print("Your altitude is: " + altitude);
* });


* global.userContextSystem.requestCity(function(city) {
*    print("Your city is: " + city);
* });

* global.userContextSystem.requestBirthdateFormatted(function(birthdate) {
*    print("Your birthday is: " + birthdate);
* });
* ```
*/
declare class UserContextSystem extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Retrieve the Snapchatter's friends list in order to access details like display name, birthdate, or Bitmoji
    
    * @exposesUserData
    */
    getAllFriends(callback: (data: SnapchatUser[]) => void): void
    
    /**
    * Retrieve the Snapchatter's best friends in order to access details like display name, birthdate, or Bitmoji.
    
    * @exposesUserData
    */
    getBestFriends(callback: (data: SnapchatUser[]) => void): void
    
    /**
    * Retrieve a SnapchatUser representing the current user.
    
    * @exposesUserData
    */
    getCurrentUser(callback: (data: SnapchatUser) => void): void
    
    /**
    * Retrieve a SnapchatUser object for MyAI which you can use to access the MyAI Bitmoji or other details.
    
    * @exposesUserData
    */
    getMyAIUser(callback: (data: SnapchatUser) => void): void
    
    /**
    * Retrieve the Snapchatter's pinned best friends in order to access details like display name, birthdate, or Bitmoji.
    
    * @exposesUserData
    */
    getPinnedBestFriends(callback: (data: SnapchatUser[]) => void): void
    
    /**
    * Gets the list of friends in the current context, such as 1:1 chats, 1:many chats, and group chats.
    
    * @exposesUserData
    */
    getUsersInCurrentContext(callback: (data: SnapchatUser[]) => void): void
    
    /**
    * Get a Snapchat User information, given a DynamicResource.
    */
    loadResourceAsSnapchatUser(resource: DynamicResource, onSuccess: (snapchatUser: SnapchatUser) => void, onFailure: (errorMessage: string) => void): void
    
    /**
    * Provides the user's current altitude as a localized string.
    */
    requestAltitudeFormatted(callback: (formattedData: string) => void): void
    
    /**
    * Provides the user's current altitude in meters.
    */
    requestAltitudeInMeters(callback: (data: number) => void): void
    
    /**
    * Provides the user's birth date as a Date object.
    
    * @exposesUserData
    */
    requestBirthdate(callback: (data: Date) => void): void
    
    /**
    * Provides the user's birth date as localized string.
    
    * @exposesUserData
    */
    requestBirthdateFormatted(callback: (formattedData: string) => void): void
    
    /**
    * Provides the name of the city the user is currently located in.
    
    * @exposesUserData
    */
    requestCity(callback: (data: string) => void): void
    
    /**
    * Provides the user's display name.
    */
    requestDisplayName(callback: (data: string) => void): void
    
    /**
    * Provides the user's current temperature in celsius.
    */
    requestTemperatureCelsius(callback: (data: number) => void): void
    
    /**
    * Provides the user's current temperature in fahrenheit.
    */
    requestTemperatureFahrenheit(callback: (data: number) => void): void
    
    /**
    * Provides the user's current temperature as a localized string.
    */
    requestTemperatureFormatted(callback: (formattedData: string) => void): void
    
    /**
    * Provides the user's username.
    
    * @exposesUserData
    */
    requestUsername(callback: (data: string) => void): void
    
    /**
    * Provides the user's current weather condition.
    */
    requestWeatherCondition(callback: (data: WeatherCondition) => void): void
    
    /**
    * Provides the user's current weather condition as a localized string.
    */
    requestWeatherLocalized(callback: (formattedData: string) => void): void
    
}

/**
* Contains methods for Utf8 encoding or decoding

* @example
* Encode Example:
* ```js
* var utf16String = "\u223e\u0020\u0045\u22c5\u0064\u0061\u0020\u003d\u0020\u0051"
* const encodedUtf8Array = Utf8.encode(utf16String);
* // result is [226,136,190,32,69,226,139,133,100,97]
* ```

* Decode Example:
* ```js
* var utf8Array = new Uint8Array([226,136,190,32,69,226,139,133,100,97])
* const decodedUtf16String = Utf8.decode(utf8Array);
* // result is the same as utf16String
* ```
*/
declare class Utf8 {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Decodes a utf-8 Uint8Array to a utf-16 string
    */
    static decode(data: Uint8Array): string
    
    /**
    * Encodes a utf-16 string as a utf-8 Uint8Array
    */
    static encode(value: string): Uint8Array
    
}

/**
* A two dimensional vector.
* Vectors can only store finite numbers in the range Number.MIN_VALUE to Number.MAX_VALUE.

* @example
* ```
* // Set the scale of a sprite using vec2 methods
* //@input Component.SpriteAligner spriteAligner
* //@input float spriteScale

* script.spriteAligner.size = vec2.one().uniformScale(script.spriteScale);
* ```
*/
declare class vec2 {
    /**
    * Creates a new instance of a vec2.
    */
    constructor(x: number, y: number)
    
    /**
    * Returns the vector plus `vec`.
    */
    add(vec: vec2): vec2
    
    /**
    * Returns the angle between the vector and `vec`.
    */
    angleTo(vec: vec2): number
    
    /**
    * Returns a copy of the vector with its length clamped to `length`.
    */
    clampLength(length: number): vec2
    
    /**
    * Returns the distance between the vector and the vector `vec`.
    */
    distance(vec: vec2): number
    
    /**
    * Like `distance()`, but returns the squared distance between vectors.
    */
    distanceSquared(vec: vec2): number
    
    /**
    * Returns the division of the vector by the vector `vec`.
    */
    div(vec: vec2): vec2
    
    /**
    * Returns the dot product of the vector and `vec`.
    */
    dot(vec: vec2): number
    
    /**
    * Returns whether this is equal to `vec`.
    */
    equal(vec: vec2): boolean
    
    /**
    * Returns a copy of the vector moved towards the point `point` by the amount `magnitude`.
    */
    moveTowards(point: vec2, magnitude: number): vec2
    
    /**
    * Returns the component-wise multiplication product of the vector and `vec`.
    */
    mult(vec: vec2): vec2
    
    /**
    * Returns a copy of the vector with its length scaled to 1.
    */
    normalize(): vec2
    
    /**
    * Returns a copy of the vector projected onto the vector `vec`.
    */
    project(vec: vec2): vec2
    
    /**
    * Projects the vector onto the plane represented by the normal `normal`.
    */
    projectOnPlane(normal: vec2): vec2
    
    /**
    * Returns a copy of the vector reflected across the plane defined by the normal `vec`.
    */
    reflect(vec: vec2): vec2
    
    /**
    * Returns the component-wise multiplication product of the vector and `vec`.
    */
    scale(vec: vec2): vec2
    
    /**
    * Returns the vector minus `vec`.
    */
    sub(vec: vec2): vec2
    
    /**
    * Returns a string representation of the vector.
    */
    toString(): string
    
    /**
    * Multiplies the components by the number `scale`.
    */
    uniformScale(scale: number): vec2
    
    /**
    * Alternate name for the y component.
    */
    g: number
    
    /**
    * Returns the length of the vector.
    */
    length: number
    
    /**
    * Returns the squared length of the vector.
    */
    lengthSquared: number
    
    /**
    * Alternate name for the x component.
    */
    r: number
    
    /**
    * x component of the vec2.
    */
    x: number
    
    /**
    * y component of the vec2.
    */
    y: number
    
    /**
    * Returns the vector (0, -1).
    */
    static down(): vec2
    
    /**
    * Returns the vector (-1, 0).
    */
    static left(): vec2
    
    /**
    * Linearly interpolates between the two vectors `vecA` and `vecB` by the factor `t`.
    */
    static lerp(vecA: vec2, vecB: vec2, t: number): vec2
    
    /**
    * Returns a new vector containing the largest value of each component in the two vectors.
    */
    static max(vecA: vec2, vecB: vec2): vec2
    
    /**
    * Returns a new vector containing the smallest value of each component in the two vectors.
    */
    static min(vecA: vec2, vecB: vec2): vec2
    
    /**
    * Returns the vector (1, 1).
    */
    static one(): vec2
    
    /**
    * Generate a random 2D direction vector. This is equivalent to a random point on a unit-radius circle.
    */
    static randomDirection(): vec2
    
    /**
    * Returns the vector (1, 0).
    */
    static right(): vec2
    
    /**
    * Returns the vector (0, 1).
    */
    static up(): vec2
    
    /**
    * Returns the vector (0, 0).
    */
    static zero(): vec2
    
}

/**
* Represents an animation track using vec2 value keyframes.

* @deprecated
*/
declare class Vec2AnimationTrack extends AnimationTrack {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Represents an animation track using vec2 value keyframes.

* @deprecated
*/
declare class Vec2AnimationTrackKeyFramed extends Vec2AnimationTrack {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Adds a keyframe value `value` at time `time`.
    
    * @deprecated
    */
    addKey(time: number, value: vec2): void
    
    /**
    * Removes all keyframes.
    
    * @deprecated
    */
    removeAllKeys(): void
    
    /**
    * Removes the keyframe at `index`.
    
    * @deprecated
    */
    removeKeyAt(index: number): void
    
}

/**
* A three dimensional vector.
* Vectors can only store finite numbers in the range Number.MIN_VALUE to Number.MAX_VALUE.

* @example
* ```
* // Check the distance between two objects
* //@input SceneObject objectA
* //@input SceneObject objectB
* //@input float distCheck

* var pointA = script.objectA.getTransform().getWorldPosition();
* var pointB = script.objectB.getTransform().getWorldPosition();
* var distance = pointA.distance(pointB);
* if(distance > script.distCheck)
* {
*     print("Too Far.");
* }
* ```
*/
declare class vec3 {
    /**
    * Creates a new instance of a vec3.
    */
    constructor(x: number, y: number, z: number)
    
    /**
    * Returns the vector plus `vec`.
    */
    add(vec: vec3): vec3
    
    /**
    * Returns the angle in radians between the vector and `vec`.
    */
    angleTo(vec: vec3): number
    
    /**
    * Returns a copy of the vector with its length clamped to `length`.
    */
    clampLength(length: number): vec3
    
    /**
    * Returns the cross product of the vector and `vec`
    */
    cross(vec: vec3): vec3
    
    /**
    * Returns the distance between the vector and the vector `vec`.
    */
    distance(vec: vec3): number
    
    /**
    * Like `distance()`, but returns the squared distance between vectors.
    */
    distanceSquared(vec: vec3): number
    
    /**
    * Returns the division of the vector by the vector `vec`.
    */
    div(vec: vec3): vec3
    
    /**
    * Returns the dot product of the vector and `vec`.
    */
    dot(vec: vec3): number
    
    /**
    * Returns whether this is equal to `vec`.
    */
    equal(vec: vec3): boolean
    
    /**
    * Returns a copy of the vector moved towards the point `point` by the amount `magnitude`.
    */
    moveTowards(point: vec3, magnitude: number): vec3
    
    /**
    * Returns the component-wise multiplication product of the vector and `vec`.
    */
    mult(vec: vec3): vec3
    
    /**
    * Returns a copy of the vector with its length scaled to 1.
    */
    normalize(): vec3
    
    /**
    * Returns a copy of the vector projected onto the vector `vec`.
    */
    project(vec: vec3): vec3
    
    /**
    * Projects the vector onto the plane represented by the normal `normal`.
    */
    projectOnPlane(normal: vec3): vec3
    
    /**
    * Returns a copy of the vector reflected across the plane defined by the normal `vec`.
    */
    reflect(vec: vec3): vec3
    
    /**
    * Returns a copy of the vector rotated towards the `target` vector by `step` radians.
    
    * The vectors may be non-normalized. The function always returns a vector with the source vector's magnitude.
    * This prevents overshoot. If `step` exceeds the angle between vectors, it stops at the `target` direction.
    * If `step` is negative, this rotates the source vector away from `target`. It stops when the direction is precisely opposite to `target`.
    * If the vectors are in opposite directions, the result is rotated along an arbitrary (but consistent) axis.
    * If either vector is zero magnitude, it returns the source vector.
    */
    rotateTowards(target: vec3, step: number): vec3
    
    /**
    * Returns the component-wise multiplication product of the vector and `vec`.
    */
    scale(vec: vec3): vec3
    
    /**
    * Returns the vector minus `vec`.
    */
    sub(vec: vec3): vec3
    
    /**
    * Returns a string representation of the vector.
    */
    toString(): string
    
    /**
    * Multiplies the components by the number `scale`.
    */
    uniformScale(scale: number): vec3
    
    /**
    * Alternate name for the z component.
    */
    b: number
    
    /**
    * Alternate name for the y component.
    */
    g: number
    
    /**
    * Returns the length of the vector.
    */
    length: number
    
    /**
    * Returns the squared length of the vector.
    */
    lengthSquared: number
    
    /**
    * Alternate name for the x component.
    */
    r: number
    
    /**
    * x component of the vec3.
    */
    x: number
    
    /**
    * y component of the vec3.
    */
    y: number
    
    /**
    * z component of the vec3.
    */
    z: number
    
    /**
    * Returns the vector (0, 0, -1).
    */
    static back(): vec3
    
    /**
    * Returns the vector (0, -1, 0).
    */
    static down(): vec3
    
    /**
    * Returns the vector (0, 0, 1).
    */
    static forward(): vec3
    
    /**
    * Returns the vector (-1, 0, 0).
    */
    static left(): vec3
    
    /**
    * Linearly interpolates between the two vectors `vecA` and `vecB` by the factor `t`.
    */
    static lerp(vecA: vec3, vecB: vec3, t: number): vec3
    
    /**
    * Returns a new vector containing the largest value of each component in the two vectors.
    */
    static max(vecA: vec3, vecB: vec3): vec3
    
    /**
    * Returns a new vector containing the smallest value of each component in the two vectors.
    */
    static min(vecA: vec3, vecB: vec3): vec3
    
    /**
    * Returns the vector (1, 1, 1).
    */
    static one(): vec3
    
    /**
    * Makes the vectors `vecA` and `vecB` normalized and orthogonal to each other.
    */
    static orthonormalize(vecA: vec3, vecB: vec3): void
    
    /**
    * Generate random 3D direction vector. This is equivalent to a random point on a unit-radius sphere.
    */
    static randomDirection(): vec3
    
    /**
    * Returns the vector (1, 0, 0).
    */
    static right(): vec3
    
    /**
    * Spherically interpolates between the two vectors `vecA` and `vecB` by the factor `t`.
    */
    static slerp(vecA: vec3, vecB: vec3, t: number): vec3
    
    /**
    * Returns the vector (0, 1, 0).
    */
    static up(): vec3
    
    /**
    * Returns the vector (0, 0, 0).
    */
    static zero(): vec3
    
}

/**
* Represents an animation track using vec3 value keyframes.

* @deprecated
*/
declare class Vec3AnimationTrack extends AnimationTrack {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Represents an animation track using vec3 value keyframes.

* @deprecated
*/
declare class Vec3AnimationTrackKeyFramed extends Vec3AnimationTrack {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Adds a keyframe value `value` at time `time`.
    
    * @deprecated
    */
    addKey(time: number, value: vec3): void
    
    /**
    * Removes all keyframes.
    
    * @deprecated
    */
    removeAllKeys(): void
    
    /**
    * Removes the keyframe at `index`.
    
    * @deprecated
    */
    removeKeyAt(index: number): void
    
}

/**
* Represents an animation track using vec3 animation tracks.

* @deprecated
*/
declare class Vec3AnimationTrackXYZ extends Vec3AnimationTrack {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns the child track at index `index`
    
    * @deprecated
    */
    getChildTrackByIndex(index: number): AnimationTrack
    
    /**
    * Sets the child track at index `index` to `track`
    
    * @deprecated
    */
    setChildTrackByIndex(index: number, track: AnimationTrack): void
    
}

/**
* A four dimensional vector.
* Vectors can only store finite numbers in the range Number.MIN_VALUE to Number.MAX_VALUE.

* @example
* ```
* // Lightens and darkens a color with vec4 methods
* //@input Asset.Material material
* //@input vec4 myColor = {1,0,0,1} {"widget":"color"}
* //@input float value

* script.material.mainColor = script.myColor.uniformScale(script.value);
* ```
*/
declare class vec4 {
    /**
    * Creates a new instance of a vec4.
    */
    constructor(x: number, y: number, z: number, w: number)
    
    /**
    * Returns the vector plus `vec`.
    */
    add(vec: vec4): vec4
    
    /**
    * Returns the angle between the vector and `vec`.
    */
    angleTo(vec: vec4): number
    
    /**
    * Returns a copy of the vector with its length clamped to `length`.
    */
    clampLength(length: number): vec4
    
    /**
    * Returns the distance between the vector and the vector `vec`.
    */
    distance(vec: vec4): number
    
    /**
    * Like `distance()`, but returns the squared distance between vectors.
    */
    distanceSquared(vec: vec4): number
    
    /**
    * Returns the division of the vector by the vector `vec`.
    */
    div(vec: vec4): vec4
    
    /**
    * Returns the dot product of the vector and `vec`.
    */
    dot(vec: vec4): number
    
    /**
    * Returns whether this is equal to `vec`.
    */
    equal(vec: vec4): boolean
    
    /**
    * Returns a copy of the vector moved towards the point `point` by the amount `magnitude`.
    */
    moveTowards(point: vec4, magnitude: number): vec4
    
    /**
    * Returns the component-wise multiplication product of the vector and `vec`.
    */
    mult(vec: vec4): vec4
    
    /**
    * Returns a copy of the vector with its length scaled to 1.
    */
    normalize(): vec4
    
    /**
    * Returns a copy of the vector projected onto the vector `vec`.
    */
    project(vec: vec4): vec4
    
    /**
    * Projects the vector onto the plane represented by the normal `normal`.
    */
    projectOnPlane(normal: vec4): vec4
    
    /**
    * Returns a copy of the vector reflected across the plane defined by the normal `vec`.
    */
    reflect(vec: vec4): vec4
    
    /**
    * Returns the component-wise multiplication product of the vector and `vec`.
    */
    scale(vec: vec4): vec4
    
    /**
    * Returns the vector minus `vec`.
    */
    sub(vec: vec4): vec4
    
    /**
    * Returns a string representation of the vector.
    */
    toString(): string
    
    /**
    * Multiplies the components by the number `scale`.
    */
    uniformScale(scale: number): vec4
    
    /**
    * Alternate name for the w component.
    */
    a: number
    
    /**
    * Alternate name for the z component.
    */
    b: number
    
    /**
    * Alternate name for the y component.
    */
    g: number
    
    /**
    * Returns the length of the vector.
    */
    length: number
    
    /**
    * Returns the squared length of the vector.
    */
    lengthSquared: number
    
    /**
    * Alternate name for the x component.
    */
    r: number
    
    /**
    * w component of the vec4.
    */
    w: number
    
    /**
    * x component of the vec4.
    */
    x: number
    
    /**
    * y component of the vec4.
    */
    y: number
    
    /**
    * z component of the vec4.
    */
    z: number
    
    /**
    * Linearly interpolates between the two vectors `vecA` and `vecB` by the factor `t`.
    */
    static lerp(vecA: vec4, vecB: vec4, t: number): vec4
    
    /**
    * Returns a new vector containing the largest value of each component in the two vectors.
    */
    static max(vecA: vec4, vecB: vec4): vec4
    
    /**
    * Returns a new vector containing the smallest value of each component in the two vectors.
    */
    static min(vecA: vec4, vecB: vec4): vec4
    
    /**
    * Returns the vector (1, 1, 1, 1).
    */
    static one(): vec4
    
    /**
    * Returns the vector (0, 0, 0, 0).
    */
    static zero(): vec4
    
}

/**
* Represents an animation track using vec4 value keyframes.

* @deprecated
*/
declare class Vec4AnimationTrack extends AnimationTrack {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Represents an animation track using vec4 value keyframes.

* @deprecated
*/
declare class Vec4AnimationTrackKeyFramed extends Vec4AnimationTrack {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Adds a key with value `value` at time `time`.
    
    * @deprecated
    */
    addKey(time: number, value: vec4): void
    
    /**
    * Removes all keys.
    
    * @deprecated
    */
    removeAllKeys(): void
    
    /**
    * Removes key at index `index`.
    
    * @deprecated
    */
    removeKeyAt(index: number): void
    
}

/**
* A vector containing 4 boolean values.

* @see Used By: {@link ClothVisual#getPointIndicesByColor}, {@link ClothVisual#getPointIndicesByMask}, {@link Pass#colorMask}, {@link PassWrapper#colorMask}, {@link Text#colorMask}
*/
declare class vec4b {
    /**
    * Creates a new instance of a vec4b.
    */
    constructor(x: boolean, y: boolean, z: boolean, w: boolean)
    
    /**
    * Returns a string representation of the vector.
    */
    toString(): string
    
    /**
    * Alternate name for the w component.
    */
    a: boolean
    
    /**
    * Alternate name for the z component.
    */
    b: boolean
    
    /**
    * Alternate name for the y component.
    */
    g: boolean
    
    /**
    * Alternate name for the x component.
    */
    r: boolean
    
    /**
    * w component of the vec4b.
    */
    w: boolean
    
    /**
    * x component of the vec4b.
    */
    x: boolean
    
    /**
    * y component of the vec4b.
    */
    y: boolean
    
    /**
    * z component of the vec4b.
    */
    z: boolean
    
}

/**
* Used to help control vertex animations on the SceneObject.
*/
declare class VertexCache extends Component {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The current time of vertex animations on this SceneObject.
    */
    currentTime: number
    
    /**
    * The weight applied to vertex animations on this SceneObject.
    */
    weight: number
    
}

/**
* Provides settings for vertex physics in the ClothVisual component.

* @see Used By: {@link ClothVisual#setVertexSettings}
* @see Returned By: {@link ClothVisual#getVertexSettings}, {@link ClothVisual.createVertexSettings}

* @example
* ```js
* //Create Cloth Visual Component
* var clothVisual = script.getSceneObject().createComponent("Component.ClothVisual");

* // yellow vertex colors
* var yellow = new vec4(1.0, 1.0, 0.0, 1.0);

* // different color mask (picking which color channel to compare)
* var colorMask = new vec4b(true, true, true, true);
* var yellowIndices = [];

* //Initialize ClothVisual Component
* clothVisual.onInitialized = clothInitCallback;
* clothVisual.updatePriority = 0;

* function clothInitCallback(clothVisualArg) {
*     var yellowIndices = clothVisualArg.getPointIndicesByColor(yellow, colorMask);

*     // create a vertex settings and set specific simulation settings value on it
*     var vertexSettings = ClothVisual.createVertexSettings();
*     vertexSettings.stretchStiffness = 0.75;
*     vertexSettings.bendStiffness = 0.75;
*     vertexSettings.stretchStiffnessGlobalWeight = 0.0;
*     vertexSettings.bendStiffnessGlobalWeight = 0.0;

*     // set this vertex simulation settings to all the yellow vertices with
*     for (var i = 0; i < yellowIndices.length; i++) {
*         clothVisual.setVertexSettings(yellowIndices[i], vertexSettings);
*     }

*     // we can also use getVertexSettings(vertexIndex) and modify vertex settings.
*     clothVisual.resetSimulation();
* }
* ```
*/
declare class VertexSimulationSettings extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Bend Stiffness value.
    */
    bendStiffness: number
    
    /**
    * BendStiffness value weight on Global Simulation Settings for this vertex.
    */
    bendStiffnessGlobalWeight: number
    
    /**
    * Friction value.
    */
    friction: number
    
    /**
    * Friction value weight on Global Simulation Settings for this vertex.
    */
    frictionGlobalWeight: number
    
    /**
    * Mass value.
    */
    mass: number
    
    /**
    * Mass value weight on Global Simulation Settings for this vertex.
    */
    massGlobalWeight: number
    
    /**
    * Stretch Stiffness value.
    */
    stretchStiffness: number
    
    /**
    * StretchStiffness value weight on Global Simulation Settings for this vertex.
    */
    stretchStiffnessGlobalWeight: number
    
}

/**
* Used by the `verticalAlignment` property in {@link MeshVisual}.
* When a {@link ScreenTransform} is attached to the same SceneObject, this determines how the mesh will be positioned vertically.

* @see Used By: {@link BaseMeshVisual#verticalAlignment}, {@link TransformerBuilder#setVerticalAlignment}

* @example
* ```js
* // @input Component.MeshVisual meshVisual
* script.meshVisual.verticalAlignment = VerticalAlignment.Top;
* ```
*/
declare enum VerticalAlignment {
    /**
    * The mesh will be aligned to the bottom side.
    */
    Bottom,
    /**
    * The mesh will be centered.
    */
    Center,
    /**
    * The mesh will be aligned to the top side.
    */
    Top
}

/**
* Options for handling vertical text overflow. Used by {@link Text}'s `verticalOverflow` property.

* @see Used By: {@link Text#verticalOverflow}, {@link Text3D#verticalOverflow}

* @example
* ```js
* //@input Component.Text textComponent

* script.textComponent.verticalOverflow = VerticalOverflow.Truncate;
* ```
*/
declare enum VerticalOverflow {
    /**
    * Text will continue to draw past the end of the vertical boundaries.
    */
    Overflow,
    /**
    * Text will be clipped at the end of the vertical boundaries.
    */
    Truncate,
    /**
    * Text will shrink to fit within the vertical boundaries.
    */
    Shrink
}

/**
* Defines a VFX to use with {@link VFXComponent}.
* @see [VFX](https://developers.snap.com/lens-studio/learning-lens-studio/graphics/particles/vfx-editor/introduction-and-concepts) guide.

* @see Used By: {@link VFXComponent#asset}
* @see Returned By: {@link VFXAsset#clone}
*/
declare class VFXAsset extends Asset {
    
    /** @hidden */
    protected constructor()
    
    [index:string]: any
    
    /**
    * Clones the VFX asset and returns a copy.
    */
    clone(): VFXAsset
    
    /**
    * Returns a PassWrapper containing all feedback passes in a VFX asset
    
    * @readonly
    */
    feedbacks: PassWrappers
    
    /**
    * When `useFixedDeltaTime` is true, this value is used for Delta Time for the asset.
    */
    fixedDeltaTime: number
    
    /**
    * When `Mesh` is selected as a Geometry Type in the VFX Output Container, the system will render particles using this mesh, otherwise particles will be rendered as quads. Refer to the Custom Mesh Emitter built-in asset as a starting point when working with custom meshes.
    */
    mesh: RenderMesh
    
    /**
    * Returns a PassWrapper containing all output passes in a VFX asset
    
    * @readonly
    */
    outputs: PassWrappers
    
    /**
    * A multiplier on the delta time for this VFX Asset. If `useFixedDeltaTime` is true, `Component Time += fixedDeltaTime * playRate`, otherwise `Component Time += fixedDeltaTime * playRate`.
    */
    playRate: number
    
    /**
    * Controls properties for the VFXAsset. Any scriptable properties on a VFX Graph will automatically become properties of this Properties class. For example, if the VFX Graph defines a variable named `baseColor`, a script would be able to access that property as `vfxAsset.properties.baseColor`.
    
    * @readonly
    */
    properties: Properties
    
    /**
    * Returns a PassWrapper containing all simulation passes in a VFX asset
    
    * @readonly
    */
    simulations: PassWrappers
    
    /**
    * Toggles the use of `fixedDeltaTime` for Component Time accumulation and particle simulation. When false, the real delta time of the Lens is used.
    */
    useFixedDeltaTime: boolean
    
}

/**
* Renders {@link VFXAsset} in scene.
*/
declare class VFXComponent extends BaseMeshVisual {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Clear the VFX system.
    */
    clear(): void
    
    /**
    * Restart the VFX system.
    */
    restart(): void
    
    /**
    * Returns the time in seconds since the VFX component was enabled, adjusted by the VFX Asset's delta time configuration. Component Time differs from Elapsed Time in that Elapsed Time is the total time since the Lens started.
    */
    time(): number
    
    /**
    * The {@link VFXAsset} that describes the VFX simulation.
    */
    asset: VFXAsset
    
    /**
    * Enables or disables emission logic for emitters on this component.
    */
    emitting: boolean
    
    /**
    * When true, stops the VFX simulation by setting delta time to 0.
    */
    paused: boolean
    
}

/**
* Allows you to record a new texture (i.e video), based on on another texture. 

* @see Returned By: {@link VideoRecorder.create}

* @example
* ```js
* // @input Component.Image displayImage

* let target = requireAsset("./Render Target.renderTarget");
* let recorder = null;
* let isRecording = false;
* let isProcessingStop = false;

* // Function to validate components
* function validateComponents() {
*     print("=== Component Validation ===");
*     
*     if (!target) {
*         print("ERROR: Render Target asset is null or undefined");
*         return false;
*     }
*     print(" Render Target asset loaded");
*     
*     if (!script.displayImage) {
*         print("ERROR: displayImage component is not assigned in Inspector");
*         return false;
*     }
*     print(" displayImage component found");
*     
*     if (!script.displayImage.mainMaterial) {
*         print("ERROR: displayImage.mainMaterial is null");
*         return false;
*     }
*     print(" displayImage.mainMaterial found");
*     
*     if (!script.displayImage.mainMaterial.mainPass) {
*         print("ERROR: displayImage.mainMaterial.mainPass is null");
*         return false;
*     }
*     print(" displayImage.mainMaterial.mainPass found");
*     
*     print("=== All components validated successfully ===");
*     return true;
* }

* // Function to cleanup and reset recorder
* function cleanupRecorder() {
*     try {
*         if (recorder) {
*             // Cancel any ongoing recording
*             if (isRecording) {
*                 recorder.cancelRecording();
*                 print("Recording cancelled during cleanup");
*             }
*             recorder = null;
*         }
*         isRecording = false;
*         isProcessingStop = false;
*         print("Recorder cleaned up successfully");
*     } catch (error) {
*         print("Error during cleanup: " + error);
*     }
* }

* // Function to initialize VideoRecorder
* function initializeRecorder() {
*     try {
*         // Clean up any existing recorder first
*         cleanupRecorder();
*         
*         if (!validateComponents()) {
*             print("ERROR: Component validation failed - cannot initialize VideoRecorder");
*             return false;
*         }
*         
*         // Set up a new VideoRecorder instance
*         const options = new VideoRecorder.Options(target);
*         options.saveThumbnail = true;
*         recorder = VideoRecorder.create(options);
*         print("VideoRecorder initialized successfully");
*         return true;
*     } catch (error) {
*         print("Error initializing VideoRecorder: " + error);
*         cleanupRecorder();
*         return false;
*     }
* }

* // Initialize the recorder
* if (!initializeRecorder()) {
*     print("FATAL: Cannot initialize VideoRecorder - script will not function");
* }

* // Start recording
* script.createEvent("TouchStartEvent").bind(function(eventData) {
*     if (!recorder) {
*         print("VideoRecorder not initialized - attempting to reinitialize");
*         if (!initializeRecorder()) {
*             print("Failed to reinitialize VideoRecorder");
*             return;
*         }
*     }
*     
*     if (isRecording) {
*         print("Already recording, ignoring start request");
*         return;
*     }
*     
*     if (isProcessingStop) {
*         print("Still processing previous stop, ignoring start request");
*         return;
*     }
*     
*     try {
*         recorder.startRecording();
*         isRecording = true;
*         print("Recording started");
*     } catch (error) {
*         print("Error starting recording: " + error);
*         isRecording = false;
*     }
* });

* // Stop recording and display the result
* script.createEvent("TouchEndEvent").bind(function(eventData) {
*     if (!recorder) {
*         print("VideoRecorder not initialized");
*         return;
*     }
*     
*     if (!isRecording) {
*         print("Not recording, ignoring stop request");
*         return;
*     }
*     
*     if (isProcessingStop) {
*         print("Already processing stop, ignoring request");
*         return;
*     }
*     
*     try {
*         print("Attempting to stop recording...");
*         isProcessingStop = true;
*         
*         // Fixed: properly handle the Promise returned by stopRecording()
*         recorder.stopRecording().then(function(texture) {
*             print("stopRecording() promise resolved");
*             
*             // Debug the texture
*             if (!texture) {
*                 print("ERROR: stopRecording() returned null/undefined texture");
*                 print("This might indicate the recording was too short or failed");
*                 isRecording = false;
*                 isProcessingStop = false;
*                 return;
*             }
*             
*             print(" Texture received from stopRecording()");
*             print("Texture type: " + typeof texture);
*             
*             // Re-validate components before assignment
*             if (!script.displayImage) {
*                 print("ERROR: displayImage component is null during assignment");
*                 isRecording = false;
*                 isProcessingStop = false;
*                 return;
*             }
*             
*             if (!script.displayImage.mainMaterial) {
*                 print("ERROR: displayImage.mainMaterial is null during assignment");
*                 isRecording = false;
*                 isProcessingStop = false;
*                 return;
*             }
*             
*             if (!script.displayImage.mainMaterial.mainPass) {
*                 print("ERROR: displayImage.mainMaterial.mainPass is null during assignment");
*                 isRecording = false;
*                 isProcessingStop = false;
*                 return;
*             }
*             
*             // Try to assign the texture
*             try {
*                 script.displayImage.mainMaterial.mainPass.baseTex = texture;
*                 print(" Recording stopped and texture assigned successfully");
*             } catch (assignError) {
*                 print("ERROR: Failed to assign texture: " + assignError);
*             }
*             
*             // Reset state
*             isRecording = false;
*             isProcessingStop = false;
*             
*             // Clean up and reinitialize for next recording
*             cleanupRecorder();
*             if (!initializeRecorder()) {
*                 print("WARNING: Failed to reinitialize recorder for next use");
*             }
*             
*         }).catch(function(error) {
*             print("ERROR: stopRecording() promise rejected: " + error);
*             isRecording = false;
*             isProcessingStop = false;
*             
*             // Clean up and reinitialize on error
*             cleanupRecorder();
*             if (!initializeRecorder()) {
*                 print("WARNING: Failed to reinitialize recorder after error");
*             }
*         });
*         
*     } catch (error) {
*         print("Error in stop recording process: " + error);
*         isRecording = false;
*         isProcessingStop = false;
*         
*         // Clean up and reinitialize on error
*         cleanupRecorder();
*         if (!initializeRecorder()) {
*             print("WARNING: Failed to reinitialize recorder after error");
*         }
*     }
* });
* ```

*/
declare class VideoRecorder extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Cancels any ongoing recording and discards what has been recorded so far.
    */
    cancelRecording(): void
    
    /**
    * Starts to record the target texture on the next frame.
    */
    startRecording(): void
    
    /**
    * Stops recording and returns a Promise which will be fulfilled with a Texture containing a VideoTextureProvider that is the recorded data.
    */
    stopRecording(): Promise<Texture>
    
    /**
    * Create a VideoRecorder instance given Options. Options cannot be changed afterwards.
    */
    static create(options: VideoRecorder.Options): VideoRecorder
    
}

declare namespace VideoRecorder {
    /**
    * Configuration options for {@link VideoRecorder}.
    
    * @see Used By: {@link VideoRecorder.create}
    */
    class Options {
        /**
        * Create a new option using the {@link Texture} that should be recorded.
        */
        constructor(sourceTexture: Texture)
        
        /**
        * Whether to also save the first recorded frame as a still image to use as a placeholder when the VideoTextureProvider is not playing. 
        */
        saveThumbnail: boolean
        
        /**
        * Read back the Texture used to construct the Options.
        
        * @readonly
        */
        sourceTexture: Texture
        
        /**
        * A scalar on the resolution of the input Texture to the video. Can be used to scale down a large Texture during recording to avoid extra overhead.
        */
        textureScale: number
        
    }

}

/**
* Describes the current status of a {@link VideoTextureProvider}.

* @see Used By: {@link VideoTextureProvider#status}

* @example
* ```js
* // Check if the video is playing
* //@input Asset.Texture videoTexture
* var provider = script.videoTexture.control;
* if(provider.getStatus() == VideoStatus.Playing){
*     print("Video is playing");
* }
* ```
*/
declare enum VideoStatus {
    /**
    * The video playback has stopped
    */
    Stopped,
    /**
    * The video is being prepared
    */
    Preparing,
    /**
    * The video is playing
    */
    Playing,
    /**
    * The video playback is paused
    */
    Paused
}

/**
* Controls a video texture resource. Can be accessed through {@link Texture.control}.

* @see Used By: {@link MediaPickerTextureProvider#videoControl}

* @example
* ```js
* // Plays a video texture
* //@input Asset.Texture movie

* var loops = 100;

* var provider = script.movie.control;
* provider.play(loops);
* ```

* ```js
* //@input Asset.Texture movie
* var provider = script.movie.control;
* var seekTime = 5.0; // Seek to 5 seconds into the video
* if (provider) {
*     // The video automatically plays from 5 seconds
*     provider.seek(seekTime);
*     print("Video resumed from " + seekTime + " seconds");
* } else {
*     print("Video control not available");
* }
* ```
*/
declare class VideoTextureProvider extends TextureProvider {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns the number of times the video has played consecutively.
    
    * @deprecated
    */
    getCurrentPlayCount(): number
    
    /**
    * Returns the status of the video resource.
    
    * @deprecated
    */
    getStatus(): VideoStatus
    
    /**
    * Pauses the video playback.
    */
    pause(): void
    
    /**
    * Plays the video `playCount` times. If `playCount` is less than 0, it loops infinitely.
    */
    play(playCount: number): void
    
    /**
    * Resumes the video playback.
    */
    resume(): void
    
    /**
    * Sets the current playback time to the specified time in seconds.
    */
    seek(value: number): boolean
    
    /**
    * Sets `callback` as the function invoked when the video resource stops playing.
    
    * @deprecated
    */
    setOnFinish(callback: () => void): void
    
    /**
    * Sets `callback` as the function invoked when the video resource is ready to be played.
    
    * @deprecated
    */
    setOnReady(onReadyCallback: () => void): void
    
    /**
    * Stops the video playback.
    */
    stop(): void
    
    /**
    * Returns the number of played cycles.
    
    * @readonly
    */
    currentPlayCount: number
    
    /**
    * Returns the current time in seconds, or zero if accessed during playback being in unprepared state.
    
    * @readonly
    */
    currentTime: number
    
    /**
    * Returns the duration of playback range in seconds, or zero if accessed while playback is in an unprepared state.
    
    * @readonly
    */
    duration: number
    
    /**
    * Returns true if video file has been loaded and is ready for decoding and false otherwise.
    
    * @readonly
    */
    isPlaybackReady: boolean
    
    /**
    * Returns the time of the last acquired texture in seconds, or zero if accessed during playback being in unprepared state.
    
    * @readonly
    */
    lastFrameTime: number
    
    /**
    * The event for being reported about playback finished. When this event is triggered, lens developers can evict this texture from material slots to avoid disrupting user's experience.
    
    * @readonly
    */
    onPlaybackDone: event0<void>
    
    /**
    * The event for being reported about playback start. When this event is triggered, lens developers can set video texture to material slots and see actual video frames.
    
    * @readonly
    */
    onPlaybackReady: event0<void>
    
    /**
    * The playback rate of the video. The rate is set when starting playback, meaning that updates to this property does not take effect until the video is stopped and started again. Defaults to 1.
    */
    playbackRate: number
    
    /**
    * Sets or returns playback end time in unit range.
    */
    relativeEndTime: number
    
    /**
    * Sets or returns playback start time in unit range.
    */
    relativeStartTime: number
    
    /**
    * A read-only property that returns the status of provider. Suggested as a substitution for the existing getStatus()
    
    * @readonly
    */
    status: VideoStatus
    
    /**
    * Returns the duration of loaded video file in seconds, or zero if accessed during playback being in unprepared state.
    
    * @readonly
    */
    totalDuration: number
    
    /**
    * The audio volume of the video resource, normalized from 0 to 1.
    */
    volume: number
    
}

/**
* Base class for all visual Components.
*/
declare class Visual extends Component {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns the order of this Visual in the render queue.
    */
    getRenderOrder(): number
    
    /**
    * Sets the order of this Visual in the render queue.
    */
    setRenderOrder(value: number): void
    
    /**
    * The order in which the visual should be rendered.
    */
    renderOrder: number
    
}

declare class VoiceML {
    
    /** @hidden */
    protected constructor()
    
}

declare namespace VoiceML {
    /**
    * Additional parameters are used to provide additional data for NlpModels and NlpResponses.
    */
    class AdditionalParam extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * The key of the additional parameter.
        
        * @readonly
        */
        key: string
        
        /**
        * The value of the additional parameter.
        
        * @readonly
        */
        value: string
        
    }

}

declare namespace VoiceML {
    /**
    * NlpModels are used to provide the VoiceML NLP engine information about how the transcript of the input audio should be processed. BaseNlpModels is the abstract base class all NlpModels ({@link NlpKeywordModel}, {@link NlpIntentModel}) inherit from.
    
    * You can specify multiple NlpModels to process the same audio, all of their results will be returned in {@link VoiceML.ListeningUpdateEventArgs}.
    */
    class BaseNlpModel extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * Adds additional params to set in the ML engine.
        */
        addParam(key: string, value: string): void
        
    }

}

declare namespace VoiceML {
    /**
    * The abstract base class all NlpResponses inherit from. NlpResponses are used as the result from the VoiceML NLP engine with information after processing the transcript of the input audio.
    */
    class BaseNlpResponse extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * Additional params to send in the response.
        
        * @readonly
        */
        additionalParams: VoiceML.AdditionalParam[]
        
        /**
        * Name of the model that sent the response.  In current studio release, you can expect the result to contain either  `VOICE_ENABLED_UI` like described in {@link VoiceML.NlpIntentModel}.  If using `enableSystemCommands()`, `SYSTEM_COMMANDS` will be returned as well.
        
        * @readonly
        */
        modelName: string
        
        /**
        * The status of the NLP response. in case of successful result, the value should be `VoiceMLModule.NlpResponsesStatusCodes.OK`, in case of failure, the value should be `VoiceMLModule.NlpResponsesStatusCodes.ERROR`.
        
        * @readonly
        */
        status: VoiceML.NlpResponseStatus
        
    }

}

declare namespace VoiceML {
    /**
    * An NLP model used to detect keywords in the transcript of the input audio.  For example, you can have keyword detection which will trigger every time the word "red" is said, and another trigger for the word "yellow".
    
    * @example
    * ```js
    * var options = VoiceML.ListeningOptions.create();
    
    * var nlpKeywordModel = VoiceML.NlpKeywordModelOptions.create();
    * nlpKeywordModel.addKeywordGroup("Fruit", ["orange", "apple"]);
    * nlpKeywordModel.addKeywordGroup("Vegetable", ["carrot", "tomato"]);
    
    * options.nlpModels = [ nlpKeywordModel ];
    * ```
    */
    class KeywordModelGroup extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * Array of keywords that are being sent to the ML model for detection with similar meaning. For example, putting the word "yellow" would trigger a response in the sentence "I saw a yellow bird". The keyword detection model supports slight variation in the word transcription. We recommend adding possible synonyms of the same keyword. So for example, should you want to detect "movie", you might want to consider adding "video" and possibly "feature" and "flick" as well to the list resulting in `["movie", "video", "feature","flick"]` as different people might express themselves differently.
        
        * @readonly
        */
        keywords: string[]
        
        /**
        * Name of the keyword group.
        
        * @readonly
        */
        name: string
        
    }

}

declare namespace VoiceML {
    /**
    * ListeningErrorEventArgs object returns in onListeningError callback. It contains the error code and description of the error.
    
    * @example
    * ```js
    * //@input Asset.VoiceMLModule vmlModule
    * //@input Component.Text screenTextTranscription
    
    * var onListeningErrorHandler = function(eventErrorArgs){
    *     script.screenTextTranscription.text = 'error: ' + eventErrorArgs.error + " desc: "+ eventErrorArgs.description;
    * }
    
    * script.vmlModule.onListeningError.add(onListeningErrorHandler);
    * ```
    */
    class ListeningErrorEventArgs extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * Returns the description of the error.
        
        * @readonly
        */
        description: string
        
        /**
        * Returns the Error code number.
        
        * @readonly
        */
        error: number
        
    }

}

declare namespace VoiceML {
    /**
    * Provides the configuration for the audio input processing output. This can either include NLP processing using the {@link VoiceML.BaseNlpModel} or directly retrieving the transcription.  `speechContext` provides the ability to further improve the transcription accuracy given an assumed context.
    
    * @see Used By: {@link VoiceMLModule#startListening}
    * @see Returned By: {@link VoiceML.ListeningOptions.create}
    
    * @example
    * ```js
    * var options = VoiceML.ListeningOptions.create();
    * var nlpKeywordModel = VoiceML.NlpKeywordModelOptions.create();
    * nlpKeywordModel.addKeywordGroup("fruit", ["orange", "apple"]);
    * nlpKeywordModel.addKeywordGroup("vegetable", ["carrot", "tomato"]);
    
    * var nlpIntentsModel = VoiceML.NlpIntentsModelOptions.create("VOICE_ENABLED_UI");
    * nlpIntentsModel.possibleIntents = [];
    
    * var nlpIntentsModel2 = VoiceML.NlpIntentsModelOptions.create("VOICE_ENABLED_UI");
    * nlpIntentsModel2.possibleIntents = ["next", "back"];
    
    * options.nlpModels = [ nlpIntentsModel, nlpKeywordModel, nlpIntentsModel2];
    
    * options.addSpeechContext(["orange", "apple"], 2);
    * options.speechRecognizer = VoiceMLModule.SpeechRecognizer.Default;
    * ```
    */
    class ListeningOptions extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * In cases where specific words are expected from the users, the transcription accuracy of these words can be improved, by strengthening their likelihood in context. The strength is scaled `1-10` (10 being the strongest increase) the default value is `5`.
        */
        addSpeechContext(phrases: string[], boost: number): void
        
        /**
        * The language which VoiceML should listen to.
        */
        languageCode: string
        
        /**
        * Options for the ML model to be used.
        */
        nlpModels: VoiceML.BaseNlpModel[]
        
        /**
        * An array of VoiceML.QnaAction elements. It is used to pass the context in each QnaAction to the DialogML.
        */
        postProcessingActions: VoiceML.PostProcessingAction[]
        
        /**
        * Should complete transcription returned. Such transcriptions after the user stopped speaking. This transcription is marked with `isFinalTranscription=true` in the `OnListeningUpdate`.
        */
        shouldReturnAsrTranscription: boolean
        
        /**
        * Should interim transcription returned. Such transcriptions are returned while the user still speaks, however they may be less accurate, and can be changed on following transcriptions. This interim results are marked with `isFinalTranscription=false` in the `OnListeningUpdate`.
        */
        shouldReturnInterimAsrTranscription: boolean
        
        /**
        * Supports multiple speech contexts for increased transcription accuracy.
        */
        speechContexts: VoiceML.SpeechContext[]
        
        /**
        * An optional attribute to specify which speech recognizer ML model to use when transcribing.  When creating a new `ListeningOptions` the value of this attrbute is defaulted to `SPEECH_RECOGNIZER`.   The supported values are: `SPEECH_RECOGNIZER`.
        */
        speechRecognizer: string
        
        /**
        * Creates voice command options.
        */
        static create(): VoiceML.ListeningOptions
        
    }

}

declare namespace VoiceML {
    /**
    * The parameter when the callback registered on from `VoiceMLModule.onListeningUpdate` is called. This is to mark the input audio transcription (and possibly NlpModels as a result) was updated.
    
    * @example
    * ```js
    * //@input Asset.VoiceMLModule vmlModule
    * //@input Component.Text screenTextTranscription
    
    * var onUpdateListeningEventHandler = function(eventArgs){
    *     var keywordResponses = eventArgs.getKeywordResponses();
    *     var intentResponses = eventArgs.getIntentResponses();
    *     var commandResponses = eventArgs.getCommandResponses();
    
    *     if(eventArgs.isFinalTranscription || eventArgs.transcription.trim() != ""){
    *         script.screenTextTranscription.text = eventArgs.transcription;
    *     }
    * }
    
    * script.vmlModule.onListeningUpdate.add(onUpdateListeningEventHandler);
    * ```
    */
    class ListeningUpdateEventArgs extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * In case `enableSystemCommands()` function used, returns the command executed by the system.
        */
        getCommandResponses(): VoiceML.NlpCommandResponse[]
        
        /**
        * In case an NLP intent model used, returns intent response from the ML engine.
        */
        getIntentResponses(): VoiceML.NlpIntentResponse[]
        
        /**
        * In case a keyword detection model used, returns results from the keywords model.
        */
        getKeywordResponses(): VoiceML.NlpKeywordResponse[]
        
        /**
        * Returns an array of QnaResponse elements, each representing an answer to the question in the eventArgs.transcript. It has properties answer(a string with the ML's response) and answerStatusCode, which can take values
        */
        getQnaResponses(): VoiceML.QnaResponse[]
        
        /**
        * Specifies whether the transcription returned is final, or partial (interim) which can be updated later as the sentence continues.
        
        * @readonly
        */
        isFinalTranscription: boolean
        
        /**
        * Returns the transcription from the Automatic Speech Recognition.
        
        * @readonly
        */
        transcription: string
        
    }

}

declare namespace VoiceML {
    /**
    * May be returned when [enableSystemCommands()](https://developers.snap.com/api/classes/VoiceMLModule#enableSystemCommands) API is used.
    * The NlpCommandResponse contains the results of the servers' NLP command model classification on the last sentence and indicates that a predetermined system command was detected.
    * Current supported commands:
    
    * Take a Snap: takes a snapshot.
    * Start Recording: starts a video recording if not already started.
    * "Stop Recording: stops an ongoing video recording, if applicable.
    
    * @example
    * ```js
    * //@input Asset.VoiceMLModule vmlModule
    
    * var parseAdditionalParams = function(additionalParams) {
    *     if (additionalParams.length == 0) return "";
    *     var retParams = "Additional params";
    *     for (var i = 0; i < additionalParams.length; i++) {
    *         retParams += " Key: " + additionalParams[i].key + " Value: " + additionalParams[i].value;
    *     }
    *     return retParams;
    * }
    
    * var parseStatusCode = function(status) {
    *     var code = "";
    *     switch(status.code) {
    *         case VoiceMLModule.NlpResponsesStatusCodes.OK:
    *             code = "OK";
    *         break;
    *         case VoiceMLModule.NlpResponsesStatusCodes.ERROR:
    *             code = "ERROR";
    *         break;
    *     }
    *     return "\nStatus Code: " + code + " Description: " + status.description;
    * }
    
    * var onUpdateListeningEventHandler = function(eventArgs) {
    *     var commandResponses = eventArgs.getCommandResponses();
    *     var nlpResponseText = "";
    
    *     for (var iIterator = 0; iIterator < commandResponses.length; iIterator++) {
    *         var commandResponse = commandResponses[iIterator];
    *         nlpResponseText += "Command Response: " + commandResponse.modelName + "\n command: " + commandResponse.command;
    *         nlpResponseText += parseAdditionalParams(commandResponse.additionalParams);
    *         nlpResponseText += parseStatusCode(commandResponse.status);
    *         nlpResponseText += "\n\n";
    *     }
    * }
    
    * script.vmlModule.enableSystemCommands();
    * script.vmlModule.onListeningUpdate.add(onUpdateListeningEventHandler);
    * ```
    */
    class NlpCommandResponse extends VoiceML.BaseNlpResponse {
        
        /** @hidden */
        protected constructor()
        
        /**
        * The classification result of the NLP command model for the sentence. For example, for the sentence "Take a snap!" with the {@link VoiceMLModule.enableSystemCommands} API enabled, the result will be a {@link VoiceML.NlpCommandResponse} holding a `take a snap` {@link VoiceML.NlpCommandResponse#command}. In addition, the following values could be returned:
        
        * - `#SNAP_ERROR_INCONCLUSIVE`: two or more commands detected.
        * - `#SNAP_ERROR_INDECISIVE`: no command detected.
        * - `#SNAP_ERROR_NONVERBAL`: no transcribable human voice was detected.
        * - `#SNAP_ERROR_SILENCE`: silence was detected.
        
        * Anything starting with `#SNAP_ERROR_`: Errors that are not currently defined in this documentation and should be ignored.
        
        * @readonly
        */
        command: string
        
    }

}

declare namespace VoiceML {
    /**
    * Specifies which NLP Intent model should run to classify the transcription of the input audio. NLP Intent classification meant to extract the meaning of a sentence rather than detecting certain keywords. Multiple intent models can be used on the same transcription, and will run only on complete sentences (`isFinalTranscription = True`). Supported intent models: `VOICE_ENABLED_UI`.
    
    * @see Returned By: {@link VoiceML.NlpIntentsModelOptions.create}
    
    * @example
    * ```js
    * var options = VoiceML.ListeningOptions.create();
    
    * var nlpIntentsModel = VoiceML.NlpIntentsModelOptions.create("VOICE_ENABLED_UI");
    * nlpIntentsModel.possibleIntents = [];
    
    * var nlpIntentsModel2 = VoiceML.NlpIntentsModelOptions.create("VOICE_ENABLED_UI");
    * nlpIntentsModel2.possibleIntents = ["next", "back"]
    * ```
    */
    class NlpIntentModel extends VoiceML.BaseNlpModel {
        
        /** @hidden */
        protected constructor()
        
        /**
        * A list of the intents the `Intent ML` engine should use when classifying the sentence, for example `["back", "next"]`, in case no list provided, all possible intents of the model are used.
        
        * Supported intents for `VOICE_ENABLED_UI`: `next`, `back`, `left`, `right`, `up`, `down`, `first`, `second`, `third`, `fourth`, `fifth`, `sixth`, `seventh`, `eighth`, `ninth`, `tenth`.
        */
        possibleIntents: string[]
        
    }

}

declare namespace VoiceML {
    /**
    * Returned when {@link NlpIntentModel} was specificed in the ListeningOptions, it contains the results of the NLP Intent model classification on the last sentence. `NlpIntentResponse` will only run on complete sentences (`isFinalTranscription = true`).
    
    * @example
    * ```js
    * //@input Asset.VoiceMLModule vmlModule
    
    * var parseAdditionalParams = function(additionalParams) {
    *     if (additionalParams.length == 0) return "";
    *     var retParams = "Additional params";
    *     for (var i = 0; i < additionalParams.length; i++) {
    *         retParams += " Key: " + additionalParams[i].key + " Value: " + additionalParams[i].value;
    *     }
    *     return retParams;
    * }
    
    * var parseStatusCode = function(status) {
    *     var code = "";
    *     switch(status.code) {
    *         case VoiceMLModule.NlpResponsesStatusCodes.OK:
    *             code = "OK";
    *         break;
    *         case VoiceMLModule.NlpResponsesStatusCodes.ERROR:
    *             code = "ERROR";
    *         break;
    *     }
    *     return "\nStatus Code: " + code + " Description: " + status.description;
    * }
    
    * var onUpdateListeningEventHandler = function(eventArgs) {
    *     var intentResponses = eventArgs.getIntentResponses();
    *     var nlpResponseText = "";
    
    *     for (var iIterator = 0; iIterator < intentResponses.length; iIterator++) {
    *         intentResponse = intentResponses[iIterator];
    *         nlpResponseText += "Intent Response: " + intentResponse.modelName + "\n intent: " + intentResponse.intent;
    *         nlpResponseText += parseAdditionalParams(intentResponse.additionalParams);
    *         nlpResponseText += parseStatusCode(intentResponse.status);
    *         nlpResponseText += "\n\n";
    *     }
    * }
    
    * script.vmlModule.onListeningUpdate.add(onUpdateListeningEventHandler);
    * ```
    */
    class NlpIntentResponse extends VoiceML.BaseNlpResponse {
        
        /** @hidden */
        protected constructor()
        
        /**
        * The classification result of the NLP intent model for the sentence. For example for the sentence "show me the next item" with the model `VOICE_ENABLED_UI`, the result will be `next` for `VOICE_ENABLED_UI` the following intents are valid: `next`, `back`, `left`, `right`, `up`, `down`, `first`, `second`, `third`, `fourth`, `fifth`, `sixth`, `seventh`, `eighth`, `ninth`, `tenth`. In addition, the following values could be returned:
        
        * `#SNAP_ERROR_INCONCLUSIVE`: two or more intents detected.
        * `#SNAP_ERROR_INDECISIVE`: no intent detected.
        * `#SNAP_ERROR_NONVERBAL`: no transcribable human voice was detected.
        * `#SNAP_ERROR_SILENCE`: silence was detected.
        
        * Anything starting with `#SNAP_ERROR_`: Errors that are not currently defined in this documentation and should be ignored.
        
        * @readonly
        */
        intent: string
        
    }

}

declare namespace VoiceML {
    /**
    * Contains helper functions for NlpIntentModel.
    */
    class NlpIntentsModelOptions {
        
        /** @hidden */
        protected constructor()
        
        /**
        * Creates a new NlpIntentModel.
        */
        static create(intentModelName: string): VoiceML.NlpIntentModel
        
    }

}

declare namespace VoiceML {
    /**
    * Used to detect usage of certain keywords from the input audio.
    
    * Keyword detection (whose results will be returned in {@link VoiceML.NlpKeywordResponse}) in the {@link VoiceML.ListeningUpdateEventArgs} can happen in the mid input sentence (and in such case the the isFinalTranscription=`false`) or can happen at the end of the sentence (isFinalTranscription=`true`). Mid sentence detection have closer proximity to the time the word was spoken, but might be less accurate.
    
    * @see Returned By: {@link VoiceML.NlpKeywordModelOptions.create}
    
    * @example
    * ```js
    * var options = VoiceML.ListeningOptions.create();
    
    * var nlpKeywordModel = VoiceML.NlpKeywordModelOptions.create();
    * nlpKeywordModel.addKeywordGroup("fruit", ["orange", "apple"]);
    * nlpKeywordModel.addKeywordGroup("vegetable", ["carrot", "tomato"]);
    
    * options.nlpModels = [ nlpKeywordModel ];
    * ```
    */
    class NlpKeywordModel extends VoiceML.BaseNlpModel {
        
        /** @hidden */
        protected constructor()
        
        /**
        * Adds a keyword group to the keywords model, the group name will be returned in NlpKeywordResponse to indicate one of the keywords in the group has been detected. Groups are used to include synonyms, or context similar words.  `addKeywordGroup("walk", ["walk", "jog", "strolls"])`.
        */
        addKeywordGroup(name: string, keywords: string[]): void
        
        /**
        * Holds group of keywords to be used in the ML model.
        
        * @readonly
        */
        keywordGroups: VoiceML.KeywordModelGroup[]
        
    }

}

declare namespace VoiceML {
    /**
    * Contains helper functions for NlpKeywordModel.
    */
    class NlpKeywordModelOptions {
        
        /** @hidden */
        protected constructor()
        
        /**
        * Creates a new NlpKeywordModel.
        */
        static create(): VoiceML.NlpKeywordModel
        
    }

}

declare namespace VoiceML {
    /**
    * NlpKeywordResponse will be returned if KeywordModel has been supplied as an input model in the ListeningOptions.  The keyword model allows detection of keywords (or short phrases) in an input audio.
    
    * @example
    * ```js
    * //@input Asset.VoiceMLModule vmlModule
    
    * var onUpdateListeningEventHandler = function(eventArgs){
    *     var keywordResponses = eventArgs.getKeywordResponses();
    
    *     var nlpResponseText = "";
    *     for (var kIterator = 0; kIterator < keywordResponses.length; kIterator++){
    *         var keywordResponse = keywordResponses[kIterator];
    *         nlpResponseText += "Keyword Response: " + keywordResponse.modelName + "\n keywords: ";
    *         for (var keywordsIterator = 0; keywordsIterator < keywordResponse.keywords.length; keywordsIterator++){
    *             nlpResponseText += keywordResponse.keywords[keywordsIterator] + " ";
    *         }
    
    *         nlpResponseText += "\n\n";
    *     }
    * }
    
    * script.vmlModule.onListeningUpdate.add(onUpdateListeningEventHandler);
    * ```
    */
    class NlpKeywordResponse extends VoiceML.BaseNlpResponse {
        
        /** @hidden */
        protected constructor()
        
        /**
        * The keywords detected by the NLP keyword detection model.  In addition, the following values could be returned:
        
        * `#SNAP_ERROR_INCONCLUSIVE`: two or more intents detected.
        * `#SNAP_ERROR_INDECISIVE`: no intent detected.
        * `#SNAP_ERROR_NONVERBAL`: no transcribable human voice was detected.
        * `#SNAP_ERROR_SILENCE`: silence was detected.
        
        * Anything starting with `#SNAP_ERROR_`: Errors that are not currently defined in this documentation and should be ignored
        
        * @readonly
        */
        keywords: string[]
        
    }

}

declare namespace VoiceML {
    /**
    * The NLP Response Status indicates wether the NLP was successful in parsing the sentence.
    
    * @see Used By: {@link VoiceML.BaseNlpResponse#status}
    */
    class NlpResponseStatus extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * Nlp status code coming from the ML engine.
        
        * @readonly
        */
        code: number
        
        /**
        * Nlp status description coming from the ML engine.
        
        * @readonly
        */
        description: string
        
    }

}

declare namespace VoiceML {
    /**
    * The `PostProcessingAction` is the base class for `QnaAction` and other post processing actions that need to processed after the transcription phase.
    */
    class PostProcessingAction extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
    }

}

declare namespace VoiceML {
    /**
    * The `PostProcessingActionResponse` is the base class for `QnaActionRespose` and other post processing actions responses. It holds the id and status properties for all deriving objects.
    
    * @example
    * ```js
    * var answers = eventArgs.getQnaResponses();
    * for(var i = 0; i < answers.length; i++) {
    *       print("answer " + i + " id: " + answers[i].id);
    *       print("answer " + i + " status code: " + answers[i].status.code);
    *       print("answer " + i + " status description: " + answers[i].status.description);
    * }
    * ```
    */
    class PostProcessingActionResponse extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * The id of the `PostProcessingAction` so returning results can be tied to post processing actions
        
        * @readonly
        */
        id: number
        
        /**
        * The status holds the description and code for the post processing actions. In case of success the description will be empty and the code will be 0
        
        * @readonly
        */
        status: VoiceML.PostProcessingActionResponseStatus
        
    }

}

declare namespace VoiceML {
    /**
    
    
    * @see Used By: {@link VoiceML.PostProcessingActionResponse#status}
    */
    class PostProcessingActionResponseStatus extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * @readonly
        */
        code: number
        
        /**
        * @readonly
        */
        description: string
        
    }

}

declare namespace VoiceML {
    /**
    
    
    * @see Returned By: {@link VoiceML.QnaAction.create}
    
    * @example
    * ```js
    * var qa = VoiceML.QnaAction.create("The moon is 239 miles away");
    * options.postProcessingActions = [qa];
    * ```
    */
    class QnaAction extends VoiceML.PostProcessingAction {
        
        /** @hidden */
        protected constructor()
        
        /**
        * The context passed to the QnaAction. The QnaAction is passed to VoiceML.listeningOptions to use the DialogML within the VoiceML automatically
        */
        context: string
        
        /**
        * Parameter: string context. Creates a QnaAction object with a given context i.e. source text for the Dialog ML.
        */
        static create(context: string): VoiceML.QnaAction
        
    }

}

declare namespace VoiceML {
    /**
    * @example
    * ```js
    * var answers = eventArgs.getQnaResponses();
    * for(var i = 0; i < answers.length; i++) {
    *     print("answer option" + i + ": " + answers[i].answer);
    * }
    * ```
    */
    class QnaResponse extends VoiceML.PostProcessingActionResponse {
        
        /** @hidden */
        protected constructor()
        
        /**
        * The response string the module came up with in response to the eventArgs.transcript
        
        * @readonly
        */
        answer: string
        
        /**
        * The status of the QnaResponse
        
        * @readonly
        */
        answerStatusCode: number
        
    }

}

declare namespace VoiceML {
    /**
    * Speech context is used in cases where specific words are expected from the users, the transcription accuracy of these words can be improved, by straightening their likelihood in context. The strength is scaled 1-10 (10 being the strongest increase) the default value is 5.
    
    * @example
    * ```js
    * var options = VoiceML.ListeningOptions.create();
    * options.addSpeechContext(["run", "jog"], 2);
    * ```
    */
    class SpeechContext extends ScriptObject {
        
        /** @hidden */
        protected constructor()
        
        /**
        * The strength is scaled 1-10 (10 being the strongest increase) the default value is 5.
        
        * @readonly
        */
        boost: number
        
        /**
        * Array of keyword to enhance by the ML engine.
        
        * @readonly
        */
        phrases: string[]
        
    }

}

/**
* Allows the Lens to incorporate transcription, keyword detection, voice command detection and other NLP based features into Lenses.

* @see [VoiceML](https://developers.snap.com/lens-studio/features/voice-ml/speech-recognition) guide.

* @example
* ```js
* //@input Asset.VoiceMLModule vmlModule {"label": "Voice ML Module"}
* //@input Asset.AudioTrackAsset audioTrack

* var options = VoiceML.ListeningOptions.create();
* options.speechRecognizer = VoiceMLModule.SpeechRecognizer.Default;

* //General Option
* options.shouldReturnAsrTranscription = true;
* options.shouldReturnInterimAsrTranscription = true;

* //Speech Context
* var phrasesOne = ["carrot", "tomato"];
* var boostValueOne = 5;
* options.addSpeechContext(phrasesOne,boostValueOne);

* var phrasesTwo = ["orange", "apple"];
* var boostValueTwo = 6;
* options.addSpeechContext(phrasesTwo,boostValueTwo);


* //NLPKeywordModel
* var nlpKeywordModel = VoiceML.NlpKeywordModelOptions.create();
* nlpKeywordModel.addKeywordGroup("Vegetable", ["carrot", "tomato"]);
* nlpKeywordModel.addKeywordGroup("Fruit", ["orange", "apple"]);

* //Command
* var nlpIntentModel = VoiceML.NlpIntentsModelOptions.create("VOICE_ENABLED_UI");
* nlpIntentModel.possibleIntents =  ["next", "back", "left", "right", "up", "down", "first", "second", "third", "fourth", "fifth", "sixth", "seventh", "eighth", "ninth", "tenth"];
* options.nlpModels =[nlpKeywordModel, nlpIntentModel];

* var onListeningEnabledHandler = function() {
*     script.vmlModule.startListening(options);
* };
* var onListeningDisabledHandler = function() {
*     script.vmlModule.stopListening();
* };

* var getErrorMessage = function(response) {
*     var errorMessage = "";
*     switch (response) {
*         case "#SNAP_ERROR_INDECISIVE":
*             errorMessage = "indecisive";
*             break;
*         case "#SNAP_ERROR_INCONCLUSIVE":
*             errorMessage = "inconclusive";
*             break;
*         case "#SNAP_ERROR_NONVERBAL":
*             errorMessage = "non verbal";
*             break;
*         case "#SNAP_ERROR_SILENCE":
*             errorMessage = "too long silence";
*             break;
*         default:
*         if (response.includes("#SNAP_ERROR")) {
*             errorMessage = "general error";
*         } else {
*             errorMessage = "unknown error";
*         }
*     }
*     return errorMessage;
* };

* var parseKeywordResponses = function(keywordResponses) {
*     var keywords = [];
*     var code = "";
*     for (var kIterator = 0; kIterator < keywordResponses.length; kIterator++) {
*         var keywordResponse = keywordResponses[kIterator];

*         switch (keywordResponse.status.code) {
*             case VoiceMLModule.NlpResponsesStatusCodes.OK:
*                 code= "OK";
*                 for (var keywordsIterator = 0; keywordsIterator < keywordResponse.keywords.length; keywordsIterator++) {
*                     var keyword = keywordResponse.keywords[keywordsIterator];
*                     if (keyword.includes("#SNAP_ERROR")) {
*                         var errorMessage = getErrorMessage(keyword);
*                         print("Keyword Error: " + errorMessage);
*                         break;
*                     }
*                     keywords.push(keyword);
*                 }
*                 break;
*             case VoiceMLModule.NlpResponsesStatusCodes.ERROR:
*                 code = "ERROR";
*                 print("Status Code: "+code+ " Description: " + keywordResponse.status.code.description);
*                 break;
*             default:
*                 print("Status Code: No Status Code");
*         }
*     }
*     return keywords;
* };

* var parseCommandResponses = function(commandResponses) {
*     var commands = [];
*     var code = "";
*     for (var iIterator = 0; iIterator < commandResponses.length; iIterator++) {
*         var commandResponse = commandResponses[iIterator];
*         switch (commandResponse.status.code) {
*             case VoiceMLModule.NlpResponsesStatusCodes.OK:
*                 code= "OK";
*                 var command = commandResponse.intent;
*                 if (command.includes("#SNAP_ERROR")) {
*                     var errorMessage = getErrorMessage(command);
*                     print("Command Error: " + errorMessage);
*                     break;
*                 }
*                 commands.push(commandResponse.intent);
*                 break;
*             case VoiceMLModule.NlpResponsesStatusCodes.ERROR:
*                 code = "ERROR";
*                 print("Status Code: "+code+ " Description: " + commandResponse.status.code.description);
*                 break;
*             default:
*                 print("Status Code: No Status Code");
*         }
*     }
*     return commands;
* };

* var onUpdateListeningEventHandler = function(eventArgs) {
*     if (eventArgs.transcription.trim() == "") {
*         return;
*     }
*     print("Transcription: " + eventArgs.transcription);

*     if (!eventArgs.isFinalTranscription) {
*         return;
*     }
*     print("Final Transcription: " + eventArgs.transcription);

*     //Keyword Results
*     var keywordResponses = eventArgs.getKeywordResponses();
*     var keywords = parseKeywordResponses(keywordResponses);
*     if (keywords.length > 0) {
*         var keywordResponseText = "";
*         for (var kIterator=0;kIterator<keywords.length;kIterator++) {
*             keywordResponseText += keywords[kIterator] +"\n";
*         }
*         print("Keywords:" + keywordResponseText);
*     }

*     //Command Results
*     var commandResponses = eventArgs.getIntentResponses();
*     var commands = parseCommandResponses(commandResponses);

*     if (commands.length > 0) {
*         var commandResponseText = "";
*         for (var iIterator=0;iIterator<commands.length;iIterator++) {
*             commandResponseText += commands[iIterator]+"\n";
*         }
*         print("Commands: " + commandResponseText);
*     }

* };

* var onListeningErrorHandler = function(eventErrorArgs) {
*     print("Error: " + eventErrorArgs.error + " desc: "+ eventErrorArgs.description);
* };

* //VoiceML Callbacks
* script.vmlModule.onListeningUpdate.add(onUpdateListeningEventHandler);
* script.vmlModule.onListeningError.add(onListeningErrorHandler);
* script.vmlModule.onListeningEnabled.add(onListeningEnabledHandler);
* script.vmlModule.onListeningDisabled.add(onListeningDisabledHandler);

* ```
*/
declare class VoiceMLModule extends Asset {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Allows the user to provide voice commands for the VoiceML to execute on behalf of the users.  Current supported commands: "Take a Snap", "Start Recording", "Stop Recording". In case a command was detected, it will be automtically executed by the system and returned as part of the {@link VoiceML.NlpCommandResponse} in the `onListeningUpdate` callback. You can retrieve the command that was executed using the following snippet:
    
    * ```js
    * var onUpdateListeningEventHandler = function(eventArgs) {
    *     var commandResponses = eventArgs.getCommandResponses();
    *     var nlpResponseText = "";
    
    *     for (var i = 0; i < commandResponses.length; i++) {
    *         var commandResponse = commandResponses[i];
    *         nlpResponseText += "Command Response: " + commandResponse.modelName + "\n command: " + commandResponse.command;
    *     }
    * }
    * ```
    */
    enableSystemCommands(): void
    
    /**
    * Starts transcribing the user audio, the NLP model is connected and sends back results using an event, optionally could request for transcription and interim results.  Notice, you can only `startListening`, after microphone permissions have been granted. It is recommended to use `startListening` method only after `onListeningEnabled` callback was called.
    
    * @exposesUserData
    */
    startListening(options: VoiceML.ListeningOptions): void
    
    /**
    * Stops voice commands.
    */
    stopListening(): void
    
    /**
    * Registers a callback which will be called when microphone permissions are taken from the lens. `stopListening()` is implicitly called in such case.
    
    * @readonly
    */
    onListeningDisabled: event0<void>
    
    /**
    * Registers a callback which will be called when microphone permissions are granted to the Lens, the microphone is initialized, and is actively listening. The expected design pattern is to start the listening session once those permissions have been granted:
    
    * ```js
    * //@input Asset.VoiceMLModule vmlModule
    * var onListeningEnabledHandler = function(){
    *     script.vmlModule.startListening(options);
    * }
    * script.vmlModule.onListeningEnabled.add(onListeningEnabledHandler);
    * ```
    
    * @readonly
    */
    onListeningEnabled: event0<void>
    
    /**
    * Registers a callback, which will be called in case the VoiceML module can't process the inputs. Most errors are due to network connectivity, or misconfigured NLP inputs.
    
    * @readonly
    */
    onListeningError: event1<VoiceML.ListeningErrorEventArgs, void>
    
    /**
    * Registers a callback, which will be called with interim transcription or related NLP models.
    
    * @readonly
    */
    onListeningUpdate: event1<VoiceML.ListeningUpdateEventArgs, void>
    
}

declare namespace VoiceMLModule {
    /**
    * @example
    * ```js
    * var answers = eventArgs.getQnaResponses();
    * for(var i = 0; i < answers.length; i++) {
    *     if (answers[i].answerStatusCode == 2)
    *         print("Answer Unknown");
    * }
    * ```
    */
    enum AnswerStatusCodes {
        /**
        * Should never show up in javascript frontend. Default value.
        */
        UNSET,
        /**
        * Answer found
        */
        STATUS_OK,
        /**
        * User transcript was not a question
        */
        NOT_A_QUESTION,
        /**
        * There was a question, but we don't know the answer
        */
        NO_ANSWER_FOUND
    }

}

declare namespace VoiceMLModule {
    /**
    * Status Codes for NLP Responses.
    
    * @example
    * ```js
    * //@input Asset.VoiceMLModule vmlModule
    
    * var parseStatusCode = function(status){
    *     var code = "";
    *     switch(status.code) {
    *         case VoiceMLModule.NlpResponsesStatusCodes.OK:
    *             code = "OK";
    *         break;
    *         case VoiceMLModule.NlpResponsesStatusCodes.ERROR:
    *             code = "ERROR";
    *         break;
    *     }
    *     return "\nStatus Code: " + code + " Description: " + status.description;
    * }
    
    * var onUpdateListeningEventHandler = function(eventArgs){
    *     var keywordResponses = eventArgs.getKeywordResponses();
    
    *     var nlpResponseText = "";
    *     for (var kIterator = 0; kIterator < keywordResponses.length; kIterator++){
    *         var keywordResponse = keywordResponses[kIterator];
    *         nlpResponseText += "Keyword Response: " + keywordResponse.modelName + "\n keywords: ";
    *         for (var keywordsIterator = 0; keywordsIterator < keywordResponse.keywords.length; keywordsIterator++){
    *             nlpResponseText += keywordResponse.keywords[keywordsIterator] + " ";
    *         }
    
    *         nlpResponseText += parseStatusCode(keywordResponse.status);
    *         nlpResponseText += "\n\n";
    *     }
    * }
    
    * script.vmlModule.onListeningUpdate.add(onUpdateListeningEventHandler);
    * ```
    */
    enum NlpResponsesStatusCodes {
        /**
        * Ok Status Code for NLP Responses, this indicates a successful processing of the NLP model.
        */
        OK,
        /**
        * Error Status Code for NLP Responses, this indicates an unsuccessful processing of the NLP model.
        */
        ERROR
    }

}

declare namespace VoiceMLModule {
    class SpeechRecognizer {
        
        /** @hidden */
        protected constructor()
        
        static Default: string
        
    }

}

/**
* Types of weather returned by {@link UserContextSystem}'s callback.

* @see Used By: {@link UserContextSystem#requestWeatherCondition}

* @example
* ```js
* var userContextSystem = global.userContextSystem;

* userContextSystem.requestWeatherCondition(function(weather) {
*     if (weather == WeatherCondition.Sunny) {
*         print("It's a sunny day!");
*     }
* });
* ```
*/
declare enum WeatherCondition {
    /**
    * Unknown or unsupported weather condition
    */
    Unknown,
    /**
    * Lightning
    */
    Lightning,
    /**
    * Low Visibility
    */
    LowVisibility,
    /**
    * Partly Cloudy
    */
    PartlyCloudy,
    /**
    * Clear Night
    */
    ClearNight,
    /**
    * Cloudy
    */
    Cloudy,
    /**
    * Rainy
    */
    Rainy,
    /**
    * Hail
    */
    Hail,
    /**
    * Snow
    */
    Snow,
    /**
    * Windy
    */
    Windy,
    /**
    * Sunny
    */
    Sunny
}

/**
* WebPageTextureProvider is the associated texture control that can be accessed from the `texture.control` property. This allows you to call functions such as `loadUrl(https://snap.com)` or to pass input events to.

* _Note: Most functionality will require the `onReady` event to have been called. Any calls before this event may cause an exception._

* @see {@link RemoteServiceModule#createWebView}

* @experimental

* @wearableOnly

* @example
* ```js
* // Create the options
* var resolution = new vec2(512, 512);
* var options = RemoteServiceModule.createWebViewOptions(resolution);
* ```

* ```ts
* const resolution = new vec2(512, 512)
* const options = RemoteServiceModule.createWebViewOptions(resolution)

* this.remoteServiceModule.createWebView(
* 	options,
* 	(texture: Asset.Texutre) => {
* 		this.image.mainPass.baseTex = texture
* 		this.webViewControl = texture.control
* 		this.webViewControl.onReady.add(() => {
* 			print("onReady")
* 			this.webViewControl.loadUrl("https://snap.com")
* 		})
* 	},
* 	(msg: string) => {
* 		print(`Error: ${msg}`)
* 	}
* )
* ```
*/
declare class WebPageTextureProvider extends TextureProvider {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Get the current user-agent for the webview.
    
    * @experimental
    
    * @wearableOnly
    */
    getUserAgent(): string
    
    /**
    * Navigate back in the web history.
    
    * @experimental
    
    * @wearableOnly
    */
    goBack(): void
    
    /**
    * Navigate forward in the web history.
    
    * @experimental
    
    * @wearableOnly
    */
    goForward(): void
    
    /**
    * Request a url to be loaded.
    
    * @experimental
    
    * @wearableOnly
    */
    loadUrl(url: string): void
    
    /**
    * Request the current page to be reloaded.
    
    * @experimental
    
    * @wearableOnly
    */
    reload(): void
    
    /**
    * Set a custom user-agent for the webview.
    
    * _Note: By default all webviews will have the same user-agent set by the platform. This can be used to target Spectacles specific websites._
    
    * @experimental
    
    * @wearableOnly
    */
    setUserAgent(userAgent: string): void
    
    /**
    * Request the current page loading to stop.
    
    * _Note: Has no effect if the page is already loaded._
    
    * _Note: Does not stop scripts on the webpage._
    
    * @experimental
    
    * @wearableOnly
    */
    stop(): void
    
    /**
    * Touch will allow the lens creator to pass input events from the Lens to the WebView.
    
    * - **id:** of the touch (in the case of multi-finger, etc), for continuous interactions it is required they share the same id.
    * - **state:** What type of action is preformed.
    * - **x:** the x coordinate of the touch, in WebView space (2d).
    * - **y:** the Y coordinate of the touch, in WebView space (2d).
    
    * _Note: Using the WebView from the Asset Library will have already implemented this logic with SIK and Hand Tracking and Mobile Controller and it is not expected for creators to re-implement this handling unless they wish to provide further customization and options._
    
    * @experimental
    
    * @wearableOnly
    */
    touch(id: number, state: TouchState, x: number, y: number): void
    
    /**
    * Check if there is any back history on the web stack.
    
    * @readonly
    
    * @experimental
    
    * @wearableOnly
    */
    canGoBack: boolean
    
    /**
    * Check if there is any forward history on the web stack.
    
    * @readonly
    
    * @experimental
    
    * @wearableOnly
    */
    canGoForward: boolean
    
    /**
    * This event signals that the webview is ready for performing actions such as `loadUrl`, etc. This also is when the WebView should be visible on the {@link Texture} that was originally provided.
    
    * @readonly
    
    * @experimental
    
    * @wearableOnly
    */
    onReady: event0<void>
    
}

/**
* WebSocket provides an API for managing a WebSocket connection to a server, as well as for sending and receiving data on the connection.

* @see Returned By: {@link InternetModule#createWebSocket}, {@link RemoteServiceModule#createAPIWebSocket}

* @wearableOnly

* @CameraKit

* @example
* ```
* // Create WebSocket connection.
* let socket = script.remoteServiceModule.createWebSocket("wss://<some-url>");
* socket.binaryType = "blob";

* // Listen for the open event
* socket.onopen = (event) => {
*     // Socket has opened, send a message back to the server
*     socket.send("Message 1");

*     // Try sending a binary message
*     // (the bytes below spell 'Message 2')
*     const message = [77, 101, 115, 115, 97, 103, 101, 32, 50];
*     const bytes = new Uint8Array(message);
*     socket.send(bytes);
* };

* // Listen for messages
* socket.onmessage = async (event) => {
*     if (event.data instanceof Blob) {
*         // Binary frame, can be retrieved as either Uint8Array or string
*         let bytes = await event.data.bytes();
*         let text = await event.data.text();

*         print("Received binary message, printing as text: " + text);
*     } else {
*         // Text frame
*         let text = event.data;
*         print("Received text message: " + text);
*     }
* });

* socket.onclose = (event) => {
*     if (event.wasClean) {
*         print("Socket closed cleanly");
*     } else {
*         print("Socket closed with error, code: " + event.code);
*     }
* };

* socket.onerror = (event) => {
*     print("Socket error");
* };
* ```
*/
declare class WebSocket extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Adds a listener for a WebSocket event. Supported event types are:
    
    * `close`
    * Fired when a connection with a WebSocket is closed. Also available via the onclose property.
    
    * `error`
    * Fired when a connection with a WebSocket has been closed because of an error, such as when some data couldn't be sent. Also available via the onerror property.
    
    * `message`
    * Fired when data is received through a WebSocket. Also available via the onmessage property.
    
    * `open`
    * Fired when a connection with a WebSocket is opened. Also available via the onopen property.
    
    * @wearableOnly
    
    * @CameraKit
    */
    addEventListener(type: string, listener: (event: WebSocketEvent) => void): void
    
    /**
    * Closes the WebSocket connection or connection attempt, if any. If the connection is already CLOSED, this method does nothing.
    
    * @wearableOnly
    
    * @CameraKit
    */
    close(): void
    
    /**
    * Enqueues the given data to be transmitted to the server over the WebSocket connection. If the data can't be sent (for example, because it needs to be buffered but the buffer is full), the socket is closed automatically.
    
    * Text or binary data can be sent via this method. To send text data pass a `string` into the send function. To send binary, pass a `Uint8Array`.
    
    * Text example:
    * ```
    * webSocket.send("Hello World");
    * ```
    
    * Binary example:
    * ```
    * // This sequence is `Hello World` in Uint8
    * const uint8Array = new Uint8Array([
    * 72, 101, 108, 108, 111, 32, 87, 111, 114, 108, 100
    * ]);
    * webSocket.send(uint8Array);
    * ```
    
    * @wearableOnly
    
    * @CameraKit
    */
    send(data: (Uint8Array|string)): void
    
    /**
    * Controls how binary data is received over the connection. Currently only `blob` is supported. The `arraybuffer` type is not yet supported.
    
    * ```
    * // Create WebSocket connection.
    * webSocket = script.remoteServiceModule.createWebSocket("wss://<some-url>");
    * socket.binaryType = "blob";
    
    * // Listen for messages
    * socket.addEventListener("message", (event) => {
    *     if (event.data instanceof Blob) {
    *         // Binary frame, can be retrieved as either Uint8Array or string
    *         let bytes = await event.data.bytes();
    *         let text = await event.data.text();
    *     } else {
    *         // Text frame
    *         let text = event.data;
    *     }
    * });
    * ```
    
    * @wearableOnly
    
    * @CameraKit
    */
    binaryType: string
    
    /**
    * Set a listener for the `close` event. The event passed is {@link WebSocketCloseEvent}. Equivalent to `addEventListener("close", ...)`. This listener will be run in addition to any listeners added via `addEventListener`.
    
    * ```
    * websocket.onclose = (event) => {
    *   print("The connection has been closed.");
    * };
    * ```
    
    * @wearableOnly
    
    * @CameraKit
    */
    onclose: (event: WebSocketCloseEvent) => void
    
    /**
    * Set a listener for the `error` event. The event passed is {@link WebSocketErrorEvent}. Equivalent to `addEventListener("error", ...)`. This listener will be run in addition to any listeners added via `addEventListener`.
    
    * ```
    * websocket.onerror = (event) => {
    *   print("The connection has been closed due to an error: " + error);
    * };
    * ```
    
    * @wearableOnly
    
    * @CameraKit
    */
    onerror: (event: WebSocketEvent) => void
    
    /**
    * Set a listener for the `message` event. The event passed is {@link WebSocketMessageEvent}. Equivalent to `addEventListener("message", ...)`. This listener will be run in addition to any listeners added via `addEventListener`.
    
    * ```
    * websocket.onmessage = async (event) => {
    *     if (event.data instanceof Blob) {
    *         // Binary frame, can be retrieved as either Uint8Array or string
    *         let bytes = await event.data.bytes();
    *         let text = await event.data.text();
    
    *         print("Received binary message, printing as text: " + text);
    *     } else {
    *         // Text frame
    *         let text = event.data;
    *         print("Received text message: " + text);
    *     }
    * };
    * ```
    
    * @wearableOnly
    
    * @CameraKit
    */
    onmessage: (event: WebSocketMessageEvent) => void
    
    /**
    * Set a listener for the `open` event. The event passed is {@link WebSocketEvent}. Equivalent to `addEventListener("open", ...)`. This listener will be run in addition to any listeners added via `addEventListener`.
    
    * ```
    * websocket.onopen = (event) => {
    *   print("The connection has been opened successfully.");
    * };
    * ```
    
    * @wearableOnly
    
    * @CameraKit
    */
    onopen: (event: WebSocketEvent) => void
    
    /**
    * Returns the current state of the WebSocket connection.
    
    * `CONNECTING (0)`
    * Socket has been created. The connection is not yet open.
    
    * `OPEN (1)`
    * The connection is open and ready to communicate.
    
    * `CLOSING (2)`
    * The connection is in the process of closing.
    
    * `CLOSED (3)`
    * The connection is closed or couldn't be opened.
    
    * @readonly
    
    * @wearableOnly
    
    * @CameraKit
    */
    readyState: number
    
    /**
    * Returns the url to which the WebSocket is connecting/connected.
    
    * @readonly
    
    * @wearableOnly
    
    * @CameraKit
    */
    url: string
    
}

/**
* Event type for WebSocket close events. This event indicates when the WebSocket connection has been closed. Listen for this event by using `addEventListener` with `close`, or by setting the `onclose` property.

* @see Used By: {@link WebSocket#onclose}

* @wearableOnly

* @CameraKit

* @example
* ```
* websocket.onclose = (event) => {
*   print("The connection has been closed.");
* };
* ```
*/
declare class WebSocketCloseEvent extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns an `unsigned short` indicating the close code sent by the server.
    
    * | Status Code | Meaning                          | Description                                                                                                                                                       |
    * |-------------|----------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|
    * | 0999       |                                  | Not used.                                                                                                                                                         |
    * | 1000        | Normal Closure                   | The connection successfully completed the purpose for which it was created.                                                                                       |
    * | 1001        | Going Away                       | The endpoint is going away, either because of a server failure or because the browser is navigating away from the page that opened the connection.                |
    * | 1002        | Protocol error                   | The endpoint is terminating the connection due to a protocol error.                                                                                               |
    * | 1003        | Unsupported Data                 | The connection is being terminated because the endpoint received data of a type it cannot accept. (For example, a text-only endpoint received binary data.)       |
    * | 1004        | Reserved                         | Reserved. A meaning might be defined in the future.                                                                                                               |
    * | 1005        | No Status Rcvd                   | Reserved. Indicates that no status code was provided even though one was expected.                                                                                |
    * | 1006        | Abnormal Closure                 | Reserved. Indicates that a connection was closed abnormally (that is, with no close frame being sent) when a status code is expected.                             |
    * | 1007        | Invalid frame payload data       | The endpoint is terminating the connection because a message was received that contained inconsistent data (e.g., non-UTF-8 data within a text message).          |
    * | 1008        | Policy Violation                 | The endpoint is terminating the connection because it received a message that violates its policy. This is a generic status code, used when codes 1003 and 1009 are not suitable. |
    * | 1009        | Message Too Big                  | The endpoint is terminating the connection because a data frame was received that is too large.                                                                   |
    * | 1010        | Mandatory Ext.                   | The client is terminating the connection because it expected the server to negotiate one or more extensions, but the server didn't.                               |
    * | 1011        | Internal Error                   | The server is terminating the connection because it encountered an unexpected condition that prevented it from fulfilling the request.                            |
    * | 1012        | Service Restart                  | The server is terminating the connection because it is restarting.                                                                                                |
    * | 1013        | Try Again Later                  | The server is terminating the connection due to a temporary condition, e.g., it is overloaded and is casting off some of its clients.                             |
    * | 1014        | Bad Gateway                      | The server was acting as a gateway or proxy and received an invalid response from the upstream server. This is similar to 502 HTTP Status Code.                   |
    * | 1015        | TLS handshake                    | Reserved. Indicates that the connection was closed due to a failure to perform a TLS handshake (e.g., the server certificate can't be verified).                   |
    * | 10162999   |                                  | For definition by future revisions of the WebSocket Protocol specification, and for definition by extension specifications.                                       |
    * | 30003999   |                                  | For use by libraries, frameworks, and applications. These status codes are registered directly with IANA. The interpretation of these codes is undefined by the WebSocket protocol. |
    * | 40004999   |                                  | For private use, and thus can't be registered. |
    
    * @readonly
    
    * @wearableOnly
    
    * @CameraKit
    */
    code: number
    
    /**
    * The reason the WebSocket connection was closed.
    
    * @readonly
    
    * @wearableOnly
    
    * @CameraKit
    */
    reason: string
    
    /**
    * True if the socket connection was closed without error.
    
    * @readonly
    
    * @wearableOnly
    
    * @CameraKit
    */
    wasClean: boolean
    
}

/**
* Generic event type for WebSocket.

* @see Used By: {@link WebSocket#addEventListener}, {@link WebSocket#onerror}, {@link WebSocket#onopen}

* @wearableOnly

* @CameraKit

* @example
* ```
* websocket.onopen = (event) => {
*   print("The connection has been opened successfully");
* };
* ```
*/
declare class WebSocketEvent extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Event type for WebSocket message events. This event fires when a message has been received from the server. Listen for this event by using `addEventListener` with `message`, or by setting the `onmessage` property.

* @see Used By: {@link WebSocket#onmessage}

* @wearableOnly

* @CameraKit

* @example
* ```
* websocket.onmessage = async (event) => {
*     if (event.data instanceof Blob) {
*         // Binary frame, can be retrieved as either Uint8Array or string
*         let bytes = await event.data.bytes();
*         let text = await event.data.text();

*         print("Received binary message, printing as text: " + text);
*     } else {
*         // Text frame
*         let text = event.data;
*         print("Received text message: " + text);
*     }
* };
* ```
*/
declare class WebSocketMessageEvent extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * The data received from the server. For binary messages, this is of type {@link Blob}. For text messages, this is a string.
    
    * @readonly
    
    * @wearableOnly
    
    * @CameraKit
    */
    data: (Blob|string)
    
    /**
    * The type of the data received, `text` or `binary`, returned as a string.
    
    * @readonly
    
    * @wearableOnly
    
    * @CameraKit
    */
    type: string
    
}

/**
* WebViewOptions allow you to specify various aspects of the WebView that will be created. These are only used at creation time.

* @see {@link InternetModule#createWebViewOptions}

* @see Used By: {@link InternetModule#createWebView}
* @see Returned By: {@link InternetModule.createWebViewOptions}

* @experimental

* @example
* ```js
* // Create the options
* var resolution = new vec2(512, 512);
* var options = InternetModule.createWebViewOptions(resolution);
* options.policy.allow = [ "snap.com", "snapchat.com" ];
* options.policy.block = [ "fakesnap.com" ];
* ```

* ```js
* const resolution = new vec2(512,512);
* const options = InternetModule.createWebViewOptions(resolution);
* ```
*/
declare class WebViewOptions extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Allows the ability to set various web request policies.
    
    * @readonly
    
    * @experimental
    */
    requestPolicy: WebViewPolicy
    
    /**
    * The resolution of the WebView that was requested. Units are in pixels.
    
    * _Note: This resolution is unrelated to the device resolution._
    * _Note: This does not change the scale or dimensions of the RenderMeshVisual that is used to draw the WebView._
    
    * __Tip:__
    * For best results choose a resolution that best matches your expected aspect ratio and desired responsive size of web content. This can not be changed later without creating a new WebView instance.
    
    * @experimental
    */
    resolution: vec2
    
}

/**
* WebViewPolicy allows creators to control what websites are allowed to be loaded in the WebView. This can be helpful to prevent users from navigating away from your desired webpage through external links.

* _Note:_
* Partial matches are now currently supported but subdomains are.
* `*snap.com` is not supported.
* `*.snap.com` is supported.

* _Note:_
* `*` is not supported for complex patterns but a single `*` on its own can be used to represent "all".

* _Note:_
* `snap.com` would be the same as `*.snap.com`.

* @see Used By: {@link WebViewOptions#requestPolicy}

* @experimental

* @example
* ```js
* // Create the options
* var resolution = new vec2(512, 512);
* var options = RemoteServiceModule.createWebViewOptions(resolution);
* optionsp.allow = [ "snap.com", "snapchat.com" ];
* options.block = [ "*" ];
* ```

* ```ts
* const resolution = new vec2(512,512);
* const options = RemoteServiceModule.createWebViewOptions(resolution);
* ```
*/
declare class WebViewPolicy extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Set an allow list of websites that can be loaded.
    
    * _Note: Default is to allow any website._
    
    * _Note: The platform may block websites._
    
    * _Note: A platform blocked website may not be overridden using this property._
    
    * @experimental
    */
    allow: string[]
    
    /**
    * Set a block list of websites that will not be loaded.
    
    * _Note: Default is to not block any website._
    
    * _Note: A platform allowed website can be blocked by this property._
    
    * @experimental
    */
    block: string[]
    
}

/**


* @see Used By: {@link AnimationKeyFrame#weightedMode}
*/
declare enum WeightedMode {
    None,
    Left,
    Right,
    Both
}

/**
* Groups {@link Physics} objects in its subtree into an independent world simulation.

* @remarks
* All simulation occurs within a physics world, each with its own configurable settings (e.g. gravity). When a physics object is placed under a {@link WorldComponent}, it belongs to that world and will only interact with other objects in that world. By default, there exists a root-level world for the scene, but multiple worlds may be created to run independent simulations.

* @see [World Component](https://developers.snap.com/lens-studio/features/physics/physics-component#physics-world)

* @example
* ```js
* // Change gravity in a given World Component

* // @input Physics.WorldComponent worldComponent
* script.worldComponent.worldSettings.gravity = new vec3(0.0, -300.0, 0.0);
* ```
*/
declare class WorldComponent extends Component {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Create an intersection probe for a specific world.
    */
    createProbe(): Probe
    
    /**
    * Update order of this world relative to other worlds (lower values are earlier). The implicit root world order is 0, so worlds with negative order will update before the root world, and worlds with non-negative order update after.
    */
    updateOrder: number
    
    /**
    * Reference to world settings. If not set, use the default world settings for the project.
    */
    worldSettings: Physics.WorldSettingsAsset
    
}

/**
* Provides depth information of the video feed that the Lens is being applied to when available.
*/
declare class WorldDepthTextureProvider extends TextureProvider {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns the depth at the passed in point (values between 0 and 1, where (0, 0) represents the top-left corner) of the physical camera. The depth returned samples raw depth captured by the camera--that is: the depth of the real world in cm. The depth will be provided as a positive number.
    
    * @exposesUserData
    */
    sampleDepthAtPoint(point: vec2): number
    
}

/**
* Holds settings for world mesh tracking in DeviceTracking component. Accessible through DeviceTracking.worldOptions.

* @see Used By: {@link DeviceTracking#worldOptions}
*/
declare class WorldOptions extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Enables or disables world mesh classification gathering.
    */
    enableWorldMeshesClassificationTracking: boolean
    
    /**
    * Enables or disables the generation of world meshes.
    */
    enableWorldMeshesTracking: boolean
    
    nativePlaneTrackingType: NativePlaneTrackingType
    
    pointCloudEnabled: boolean
    
}

/**
* The result of the hitTest method call. This includes the world position of the hit, the world normal of the hit. Returns `null` if no intersection with environment was detected.

* @see Used By: {@link HitTestSession#hitTest}

* @wearableOnly

* @example
* ```
* // Set object's position and rotation if WorldQuery ray hits something.

* onHitTestResult(result) {
*     if (result === null) {
*         // no hit detected
*     } else {
*         // get hit information
*         const hitPosition = results.position;
*         const hitNormal = results.normal;
*         //identifying the direction the object should look at based on the normal of the hit location.

*         var lookDirection;
*         if (1 - Math.abs(hitNormal.normalize().dot(vec3.up())) < 0.01) {
*             lookDirection = vec3.forward();
*         } else {
*             lookDirection = hitNormal.cross(vec3.up());
*         }
*         const toRotation = quat.lookAt(lookDirection, hitNormal);

*         //set position and rotation

*         this.targetObject.getTransform().setWorldPosition(hitPosition);
*         this.targetObject.getTransform().setWorldRotation(toRotation);
*     }
* }
* ```
*/
declare class WorldQueryHitTestResult extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * A normal of the surface at the position where the ray intersects with the environment.
    
    * @readonly
    
    * @wearableOnly
    */
    normal: vec3
    
    /**
    * A position where ray intersects with the environment.
    
    * @readonly
    
    * @wearableOnly
    */
    position: vec3
    
}

/**
* Provides access to various APIs which can perform hit test for real surfaces to sample the depth and normal at a certain location.

* @see [World Query Module](https://developers.snap.com/spectacles/about-spectacles-features/apis/world-query) guide.

* @wearableOnly

* @example
* ```ts
* // import required modules
* const WorldQueryModule = require("LensStudio:WorldQueryModule")
* const EPSILON = 0.01;
* const RAY_LENGTH = 1000;

* @component
* export class NewScript extends BaseScriptComponent {

*     private hitTestSession;
*     private transform: Transform;
*     private cameraTransform: Transform;

*     @input
*     targetObject: SceneObject;

*     @input
*     camera: Camera;

*     @input
*     filterEnabled: boolean;

*     onAwake() {
*         // create new hit session
*         this.hitTestSession = this.createHitTestSession(this.filterEnabled);

*         this.transform = this.targetObject.getTransform();
*         this.cameraTransform = this.camera.getSceneObject().getTransform();
*         // disable target object when surface is not detected
*         this.targetObject.enabled = false;
*         // create update event
*         this.createEvent("UpdateEvent").bind(this.onUpdate.bind(this));
*     }


*     createHitTestSession(filterEnabled) {
*         // create hit test session with options
*         let options = HitTestSessionOptions.create();
*         options.filter = filterEnabled;


*         let session = WorldQueryModule.createHitTestSessionWithOptions(options);
*         return session;
*     }


*     onHitTestResult(results) {
*         if (results === null) {
*             this.targetObject.enabled = false;
*         } else {
*             this.targetObject.enabled = true;
*             // get hit information
*             const hitPosition = results.position;
*             const hitNormal = results.normal;


*             // identifying the direction the object should look at based on the normal of the hit location.

*             let lookDirection;
*             if (1 - Math.abs(hitNormal.normalize().dot(vec3.up())) < EPSILON) {
*                 lookDirection = vec3.forward();
*             } else {
*                 lookDirection = hitNormal.cross(vec3.up());
*             }


*             const toRotation = quat.lookAt(lookDirection, hitNormal);
*             // set position and rotation
*             this.targetObject.getTransform().setWorldPosition(hitPosition);
*             this.targetObject.getTransform().setWorldRotation(toRotation);
*         }
*     }


*     onUpdate() {
*         let rayStart = this.cameraTransform.getWorldPosition();
*         let rayEnd = rayStart.add(this.cameraTransform.forward.uniformScale(RAY_LENGTH));
*         this.hitTestSession.hitTest(rayStart, rayEnd, this.onHitTestResult.bind(this));
*     }
* }
* //
*/
declare class WorldQueryModule extends Asset {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Create a HitTestSession with default options.
    
    * @wearableOnly
    */
    createHitTestSession(): HitTestSession
    
    /**
    * Create a new HitTestSession with options.
    
    * @wearableOnly
    */
    createHitTestSessionWithOptions(options: HitTestSessionOptions): HitTestSession
    
}

/**
* Provider for RenderMesh data representing the estimated shape of real world objects generated from depth information. Only available when world mesh tracking is supported and enabled.

* @example
* ```javascript
* //@input Component.RenderMeshVisual worldMesh

* var provider = script.worldMesh.mesh.control;
* provider.meshClassificationFormat = MeshClassificationFormat.PerVertexFast;
* provider.useNormals = true;
* ```
*/
declare class WorldRenderObjectProvider extends MeshRenderObjectProvider {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Enable or disable world mesh tracking.
    */
    enableWorldMeshesTracking: boolean
    
    /**
    * Returns the number of faces of the mesh.
    
    * @readonly
    */
    faceCount: number
    
    /**
    * Mesh classification format being used.
    */
    meshClassificationFormat: MeshClassificationFormat
    
    /**
    * Enable or disable normal direction usage.
    */
    useNormals: boolean
    
    /**
    * Returns the number of vertices of the mesh.
    
    * @readonly
    */
    vertexCount: number
    
}

/**
* Provides information about whether certain world tracking features are supported by the device.

* @see Used By: {@link DeviceTracking#worldTrackingCapabilities}

* @example
* ```javascript
* //@input Component.DeviceTracking deviceTracking

* if( script.deviceTracking.worldTrackingCapabilities.sceneReconstructionSupported )
* {
*     //   ...
* }
* ```
*/
declare class WorldTrackingCapabilities extends ScriptObject {
    
    /** @hidden */
    protected constructor()
    
    /**
    * @readonly
    */
    planesTrackingSupported: boolean
    
    /**
    * Returns true if the device supports scene reconstruction.
    
    * @readonly
    */
    sceneReconstructionSupported: boolean
    
    /**
    * @readonly
    */
    trackedWorldPointsSupported: boolean
    
}

/**
* Triggered when new world tracking meshes are detected. Only available when a Device Tracking component is in the scene, and world mesh tracking is supported and enabled.

* @example
* ```javascript
* script.createEvent("WorldTrackingMeshesAddedEvent").bind(onMeshesAdded);

* function onMeshesAdded(eventData) {
*     var trackedMeshes = eventData.getMeshes();
*     print(trackedMeshes.length + " meshes were added");

*     for (var i = 0; i < trackedMeshes.length; i++) {
*         print("TrackedMesh is valid " + trackedMeshes[i].isValid);
*         print("Transformation matrix " + trackedMeshes[i].transform);
*     }
* }
* ```
*/
declare class WorldTrackingMeshesAddedEvent extends SceneEvent {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns an array of newly added Tracked Meshes.
    */
    getMeshes(): TrackedMesh[]
    
}

/**
* Triggered when some world tracking meshes are no longer detected. Only available when a Device Tracking component is in the scene, and world mesh tracking is supported and enabled.

* @example
* ```javascript
* script.createEvent("WorldTrackingMeshesRemovedEvent").bind(onMeshesRemoved);

* function onMeshesRemoved(eventData){
*     var removedMeshes = eventData.getMeshes();
*     print(trackedMeshes.length + " meshes were removed");
* }
* ```
*/
declare class WorldTrackingMeshesRemovedEvent extends SceneEvent {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns an array of TrackedMeshes that are no longer detected.
    */
    getMeshes(): TrackedMesh[]
    
}

/**
* Triggered when world tracking meshes are updated. Only available when a Device Tracking component is in the scene, and world mesh tracking is supported and enabled.

* @example
* ```javascript
* script.createEvent("WorldTrackingMeshesUpdatedEvent").bind(onMeshesUpdated);

* function onMeshesUpdated(eventData) {
*     var trackedMeshes = eventData.getMeshes();
*     print(trackedMeshes.length + " meshes were updated");

*     for (var i = 0; i < trackedMeshes.length; i++) {
*         print("TrackedMesh is valid " + trackedMeshes[i].isValid);
*         print("Transformation matrix " + trackedMeshes[i].transform);
*     }
* }
* ```
*/
declare class WorldTrackingMeshesUpdatedEvent extends SceneEvent {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns an array of TrackedMeshes that were updated.
    */
    getMeshes(): TrackedMesh[]
    
}

/**
* Triggered when plane(s) are newly detected. The `worldOptions.nativePlaneTrackingType` must be set to anything other than `NativePlaneTrackingType.None`. In addition, {@link DeviceTracking} component must be set to `World` mode to orient the Camera relative to the planes correctly.

* @example
* ```
* //@input Component.DeviceTracking deviceTracking
* script.deviceTracking.worldOptions.nativePlaneTrackingType = NativePlaneTrackingType.Both;

* script.createEvent("WorldTrackingPlanesAddedEvent").bind(function(eventData){

*    var eventPlanes = eventData.getPlanes();

* });
* ```
*/
declare class WorldTrackingPlanesAddedEvent extends SceneEvent {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns an array of newly added {@link TrackedPlane}.
    */
    getPlanes(): TrackedPlane[]
    
}

/**
* Triggered when plane(s) are no longer detected.  This usually happens when two planes merge into one.  Planes persist when no longer seen by camera(s) and when previously detected objects move (e.g. a door is opened) to create a static scene. {@link DeviceTracking} component must be set to `World` mode to orient the Camera relative to the planes correctly.

* @example
* ```
* //@input Component.DeviceTracking deviceTracking
* script.deviceTracking.worldOptions.nativePlaneTrackingType = NativePlaneTrackingType.Both;

* script.createEvent("WorldTrackingPlanesRemovedEvent").bind(function(eventData){

*    var eventPlanes = eventData.getPlanes();

* });
* ```
*/
declare class WorldTrackingPlanesRemovedEvent extends SceneEvent {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns an array of {@link TrackedPlane} that are no longer detected.
    */
    getPlanes(): TrackedPlane[]
    
}

/**
* Triggered when currently detected plane(s) are updated.  This usually happens when a plane grows in size. The `worldOptions.nativePlaneTrackingType` must be set to anything other than `NativePlaneTrackingType.None`. In addition, {@link DeviceTracking} component must be set to `World` mode to orient the Camera relative to the planes correctly.

* @example
* ```
* //@input Component.DeviceTracking deviceTracking
* script.deviceTracking.worldOptions.nativePlaneTrackingType = NativePlaneTrackingType.Both;

* script.createEvent("WorldTrackingPlanesUpdatedEvent").bind(function(eventData){

*    var eventPlanes = eventData.getPlanes();

* });
* ```
*/
declare class WorldTrackingPlanesUpdatedEvent extends SceneEvent {
    
    /** @hidden */
    protected constructor()
    
    /**
    * Returns an array of {@link TrackedPlane} that were updated.
    */
    getPlanes(): TrackedPlane[]
    
}

/**
* Declares permissions for your Lens project.

* @see [Permissions Overview](https://developers.snap.com/spectacles/permission-privacy/overview#list-of-permissions-types).
*/
declare class WorldUnderstandingModule extends Asset {
    
    /** @hidden */
    protected constructor()
    
}

/**
* Describes how a texture should be sampled when using coordinates outside of the normal range.

* @see Used By: {@link SamplerBuilder#setWrapMode}, {@link SamplerBuilder#setWrapUMode}, {@link SamplerBuilder#setWrapVMode}, {@link SamplerBuilder#setWrapWMode}, {@link SamplerWrapper#wrap}, {@link SamplerWrapper#wrapU}, {@link SamplerWrapper#wrapV}, {@link SamplerWrapper#wrapW}
*/
declare enum WrapMode {
    /**
    * Take the average of the border color and stretch it out.
    */
    ClampToEdge,
    /**
    * Repeats the current texture at the edge again.
    */
    Repeat,
    /**
    * Repeats the current texture at the edge again, but flipping the orientation.
    */
    MirroredRepeat,
    /**
    * Take the border color at the edge of a texture and stretch it out.
    */
    ClampToBorder
}

/**


* @see Used By: {@link SnapchatUser#zodiac}
*/
declare enum Zodiac {
    Aquarius,
    Aries,
    Cancer,
    Capricorn,
    Gemini,
    Leo,
    Libra,
    Pisces,
    Sagittarius,
    Scorpio,
    Taurus,
    Virgo
}



declare var script : ScriptComponent;

interface event0<R> {
    add(callback: () => R) : EventRegistration
    remove(eventRegistration: EventRegistration) : void
}
interface event1<T0,R> {
    add(callback: (arg0:T0) => R) : EventRegistration
    remove(eventRegistration: EventRegistration) : void
}
interface event2<T0,T1,R> {
    add(callback: (arg0:T0, arg1:T1) => R) : EventRegistration
    remove(eventRegistration: EventRegistration) : void
}
interface event3<T0,T1,T2,R> {
    add(callback: (arg0:T0, arg1:T1, arg2:T2) => R) : EventRegistration
    remove(eventRegistration: EventRegistration) : void
}
interface event4<T0,T1,T2,T3,R> {
    add(callback: (arg0:T0, arg1:T1, arg2:T2, arg3:T3) => R) : EventRegistration
    remove(eventRegistration: EventRegistration) : void
}
interface event5<T0,T1,T2,T3,T4,R> {
    add(callback: (arg0:T0, arg1:T1, arg2:T2, arg3:T3, arg4:T4) => R) : EventRegistration
    remove(eventRegistration: EventRegistration) : void
}
interface event6<T0,T1,T2,T3,T4,T5,R> {
    add(callback: (arg0:T0, arg1:T1, arg2:T2, arg3:T3, arg4:T4, arg5:T5) => R) : EventRegistration
    remove(eventRegistration: EventRegistration) : void
}
interface event7<T0,T1,T2,T3,T4,T5,T6,R> {
    add(callback: (arg0:T0, arg1:T1, arg2:T2, arg3:T3, arg4:T4, arg5:T5, arg6:T6) => R) : EventRegistration
    remove(eventRegistration: EventRegistration) : void
}
type Filter = Physics.Filter
